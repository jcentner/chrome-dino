{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2f52b8-c51f-42a3-8871-4efd37d3aea9",
   "metadata": {},
   "source": [
    "# Chrome Dino re-write 2023\n",
    "\n",
    "Use Pytorch, stable-baselines3, etc. to build a model to play chrome://dino with reinforcement learning.\n",
    "Works best with \"Start Slow\" disabled\n",
    "\n",
    "## dependencies (todo: make these a requirements.txt)\n",
    "- CUDA-enabled Pytorch (Pytorch 2.00; CUDA 1.18)\n",
    "- Stable-Baselines3 (with extras like OpenCV): https://stable-baselines3.readthedocs.io/en/master/\n",
    "- Protobuf (a training dependency) (3.20.*)\n",
    "- pytesseract (interface to Google Tesseract)\n",
    "- Google Tesseract-OCR ((5.3.1.20230401)\n",
    "- Gym (gym v0.21 since this is used by Stable-Baselines3; RL environment library): https://gymnasium.farama.org/\n",
    "- MSS (crossplatform screenshots)\n",
    "- openCV (2)\n",
    "- selenium (chrome test driver) (ChromeDriver 112.0.5615.49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02089c1-d995-4aee-9be7-904bedcb0f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e79f30-c80d-45da-8183-39dc343a2991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# needed to support old version of gym (0.21) used with Stable-Baselines3\n",
    "# gym 0.21 has installation issue; gym moved to Gymnasium; SB3 still does not support Gymnasium \n",
    "!pip install setuptools==66 Cmake git+https://github.com/openai/gym.git@9180d12e1b66e7e2a1a622614f787a6ec147ac40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f422a8-8eb2-492d-9ab1-b276d95bcf73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install stable-baselines3[extra] protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8364f7c-3701-48c3-95cb-0488f281d699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install mss pydirectinput selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c072b6-553b-4120-801a-0e4de60df5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73768984-360c-4107-8122-6024473d995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from mss import mss\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import pytesseract\n",
    "import pydirectinput\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import WebDriverException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c6147-2f1e-4fbe-b2cd-449b9cac725c",
   "metadata": {},
   "source": [
    "## set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e774aea-382f-47ec-bbec-ccfee874c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a base Gym environment for managing state\n",
    "# Env must implement step and reset; optionally, render \n",
    "# https://www.gymlibrary.dev/api/core/\n",
    "class ChromeDinoRL(Env):\n",
    "    \n",
    "    # initialize environment spaces\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # observation parameters\n",
    "        self.capture = mss()\n",
    "        self.game_window = {'top':140, 'left':20, 'width':540, 'height':300}\n",
    "        self.game_over_window = {'top':260, 'left':240, 'width':320, 'height':60}\n",
    "        \n",
    "        self.obs_width = int(self.game_window[\"width\"] / 4)\n",
    "        self.obs_height = int(self.game_window[\"height\"] / 4)\n",
    "        \n",
    "        # set up observations and actions\n",
    "        # multidimensional array as return output from observation (an image)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, self.obs_height, self.obs_width), dtype=np.uint8)\n",
    "        # two actions: jump or not\n",
    "        self.action_space = Discrete(2)\n",
    "        \n",
    "    # run one timestep in the environment\n",
    "    def step(self, action):\n",
    "        # 0 -> spacebar, 1 -> noop\n",
    "        action_map = {\n",
    "            0: 'space',\n",
    "            1: 'no_op'\n",
    "        }\n",
    "        \n",
    "        if action == 0:\n",
    "            pydirectinput.press(action_map[action])\n",
    "        \n",
    "        # update state each timestep\n",
    "        game_over = self.game_over()[0]\n",
    "        obs = self.observe_env()\n",
    "        reward = 1 # stay alive\n",
    "        # experiment with different reward mechanisms\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        return obs, reward, game_over, info\n",
    "        \n",
    "    # reset to initial state, return initial observation\n",
    "    def reset(self):\n",
    "        time.sleep(1)\n",
    "        pydirectinput.click(x=200, y=200)\n",
    "        pydirectinput.press('space')\n",
    "        return self.observe_env()\n",
    "        \n",
    "    # capture screen and render with cv2\n",
    "    # close everything with \"q\" \n",
    "    def render(self):\n",
    "        cv2.imshow('Chrome Dino', np.array(self.capture.grab(self.game_window)))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "    \n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    # do an observation\n",
    "    def observe_env(self):        \n",
    "        #obs = self.capture.grab(self.game_window)\n",
    "        obs = np.array(self.capture.grab(self.game_window)).astype(np.uint8)\n",
    "        \n",
    "        # preprocessing -> grayscale, shrink, then reshape for Stable Baselines\n",
    "        gray = cv2.cvtColor(obs, cv2.COLOR_BGR2GRAY)\n",
    "        shrunk = cv2.resize(gray, (self.obs_width, self.obs_height))\n",
    "        channel = np.reshape(shrunk, (1, self.obs_height, self.obs_width))\n",
    "        \n",
    "        return channel\n",
    "        \n",
    "    # identify game over\n",
    "    def game_over(self):\n",
    "        game_over = np.array(self.capture.grab(self.game_over_window)).astype(np.uint8)\n",
    "        check_words = ['GAME', 'OVER']\n",
    "        \n",
    "        # preprocess for tesseract-OCR\n",
    "        \n",
    "        # grayscale, gaussian blur, Otsu's threshold\n",
    "        # reference: https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html#otsus-binarization\n",
    "        gray = cv2.cvtColor(game_over, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "        thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "        # morphological ops to remove remaining noise and invert image to black on white\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1) # erosion + dilation \n",
    "        invert = 255 - opening\n",
    "        \n",
    "        OCR = pytesseract.image_to_string(game_over)\n",
    "        \n",
    "        words = OCR.split()\n",
    "        \n",
    "        if all(word in words for word in check_words):\n",
    "            return True, OCR, invert, words\n",
    "        else:\n",
    "            return False, OCR, invert, words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9bb3e-970d-421f-91c7-30971b8a0edc",
   "metadata": {},
   "source": [
    "## set up driver\n",
    "\n",
    "automated window setup for consistent environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6119728-cab5-414a-b8d2-10b899af13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up webdriver for selenium on chrome\n",
    "# requires chromedriver present in path specified\n",
    "class ChromeDinoDriver():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_driver = None\n",
    "        self.chrome_path = r\"chromedriver\\chromedriver.exe\" \n",
    "        self.startpage = \"chrome://dino/\"\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--window-size=640,480\")\n",
    "        \n",
    "    # set up and run driver\n",
    "    # options \n",
    "    def run(self):\n",
    "        #self.chrome_driver = webdriver.Chrome(executable_path=self.chrome_path, options=self.chrome_options)\n",
    "        self.chrome_driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=self.chrome_options)\n",
    "        self.chrome_driver.set_window_position(0, 0, windowHandle='current')\n",
    "        try:\n",
    "            self.chrome_driver.get(\"chrome://dino\")\n",
    "        except WebDriverException:\n",
    "            pass # ignore selenium complaining about offline\n",
    "\n",
    "    def end(self): # close driver\n",
    "        # duplicates are automatically closed by new selenium service\n",
    "        self.chrome_driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2852c-5473-413a-82b1-17acc0111918",
   "metadata": {},
   "source": [
    "### misc testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18acd610-91ac-440e-ab6c-3c256c443c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test environment\n",
    "env = ChromeDinoRL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7aa8c-ecd9-485b-9326-cd17f9a5eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random action; observation space empty\n",
    "print(env.action_space.sample())\n",
    "plt.imshow(env.observation_space.sample()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba83fc-928f-48d3-b2de-0ef11c610c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try an observation\n",
    "obs = env.observe_env()\n",
    "plt.imshow(cv2.cvtColor(obs[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f8804-33b1-474e-98e5-5ffb75fa0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test driver\n",
    "driver = ChromeDinoDriver()\n",
    "driver.run()\n",
    "time.sleep(2)\n",
    "driver.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6118a74-972b-40b3-a727-10334f2a27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start driver and try an observation\n",
    "driver = ChromeDinoDriver()\n",
    "driver.run()\n",
    "plt.imshow(cv2.cvtColor(env.observe_env()[0], cv2.COLOR_BGR2RGB))\n",
    "driver.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e18b6c-8ab4-4cdc-9429-9f2a0bc94320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start driver for testing\n",
    "driver = ChromeDinoDriver()\n",
    "driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b11ad1-ea88-4384-b5bf-991beef94eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render() # close with env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d6027-6fda-4363-b459-05cb483b0024",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112bdbf-d39c-40c8-90a1-038234ea7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26aefb5-b60f-4a9c-8c8e-8c2fc3cc06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(env.observe_env()[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b754f-5b55-4d6c-a9d7-050fbf1308bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "go = env.game_over()\n",
    "print(\"Game Over? : \" + str(go[0]))\n",
    "print(go[1])\n",
    "print(go[3])\n",
    "plt.imshow(cv2.cvtColor(go[2], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63b16f-a887-4570-baa4-e54e3ec83345",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a76c43-4cf2-4436-8958-fd9a9b70b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "\n",
    "env = ChromeDinoRL()\n",
    "driver = ChromeDinoDriver()\n",
    "driver.run()\n",
    "\n",
    "for ep in range(4):\n",
    "    obs = env.reset()\n",
    "    game_over = False \n",
    "    score = 0\n",
    "    \n",
    "    while not game_over:\n",
    "        obs, reward, game_over, info = env.step(env.action_space.sample())\n",
    "        score += reward\n",
    "    \n",
    "    print('Score for ep {} = {}.'.format(ep, score))\n",
    "    \n",
    "driver.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba51ccf5-21fe-4c0f-aca6-127bab0bf229",
   "metadata": {
    "tags": []
   },
   "source": [
    "## train model\n",
    "\n",
    "Train the DQN and save it using a Stable-Baselines3 callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c36edc-5817-4ba1-95b5-85ab068220b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training imports\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f802d15e-a1bd-43ed-84a5-df159bcbdb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stable baselines callback for saving model and logs during training\n",
    "    \n",
    "CHECKPOINT_DIR = './train/'\n",
    "MODELS_DIR = './models/'\n",
    "LOG_DIR = './logs/'\n",
    "\n",
    "class TrainAndLogCallback(BaseCallback):\n",
    "    \n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLogCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq # number of steps between checkpoints\n",
    "        self.save_path = save_path\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.log_dir = LOG_DIR\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # save model every check_freq steps\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "            # get training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # show mean training reward over the last check_freq episodes\n",
    "                mean_reward = np.mean(y[(-1*self.check_freq):])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                    print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_reward\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe6c95d6-4baf-4c6c-a724-3a92888fae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLogCallback(check_freq=360, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8887e88-804f-4553-9df1-087d04e5c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# create DQN\n",
    "# reference: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "\n",
    "env = ChromeDinoRL()\n",
    "env = Monitor(env, LOG_DIR) # monitor progress using tensorboard logs\n",
    "\n",
    "# check environment - no output means no issues\n",
    "#env_checker.check_env(env)\n",
    "\n",
    "model = DQN(\n",
    "    'CnnPolicy', # passing image observation\n",
    "    env, # registered in Gym \n",
    "    tensorboard_log=LOG_DIR,\n",
    "    verbose=1,\n",
    "    buffer_size=600000, # size of replay buffer; 600k = 12GB-ish RAM\n",
    "    learning_starts=100) # how many steps to collect transitions for before learning starts (100 maybe?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70132370-d27b-482d-a264-c642a1881c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/DQN_7\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.998    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 63       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.5     |\n",
      "|    ep_rew_mean      | 12.5     |\n",
      "|    exploration_rate | 0.997    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 100      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.5     |\n",
      "|    ep_rew_mean      | 12.5     |\n",
      "|    exploration_rate | 0.996    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 150      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 12       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | 12.8     |\n",
      "|    exploration_rate | 0.995    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 204      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0347   |\n",
      "|    n_updates        | 25       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | 13       |\n",
      "|    exploration_rate | 0.993    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 260      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0109   |\n",
      "|    n_updates        | 39       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | 13.3     |\n",
      "|    exploration_rate | 0.992    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 155      |\n",
      "|    total_timesteps  | 319      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00217  |\n",
      "|    n_updates        | 54       |\n",
      "----------------------------------\n",
      "Num timesteps: 353\n",
      "Best mean reward: -inf - Last mean reward per episode: 13.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.6     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.99     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 183      |\n",
      "|    total_timesteps  | 382      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000516 |\n",
      "|    n_updates        | 70       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.9     |\n",
      "|    ep_rew_mean      | 13.9     |\n",
      "|    exploration_rate | 0.988    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 210      |\n",
      "|    total_timesteps  | 446      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.51e-05 |\n",
      "|    n_updates        | 86       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14       |\n",
      "|    ep_rew_mean      | 14       |\n",
      "|    exploration_rate | 0.987    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 237      |\n",
      "|    total_timesteps  | 503      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000103 |\n",
      "|    n_updates        | 100      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.985    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 264      |\n",
      "|    total_timesteps  | 572      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.26e-06 |\n",
      "|    n_updates        | 117      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.983    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 293      |\n",
      "|    total_timesteps  | 633      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.53e-06 |\n",
      "|    n_updates        | 133      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.982    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 319      |\n",
      "|    total_timesteps  | 692      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.13e-05 |\n",
      "|    n_updates        | 147      |\n",
      "----------------------------------\n",
      "Num timesteps: 713\n",
      "Best mean reward: 13.50 - Last mean reward per episode: 14.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.98     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 345      |\n",
      "|    total_timesteps  | 751      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.41e-06 |\n",
      "|    n_updates        | 162      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.979    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 371      |\n",
      "|    total_timesteps  | 809      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.69e-05 |\n",
      "|    n_updates        | 177      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.977    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 400      |\n",
      "|    total_timesteps  | 877      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.85e-05 |\n",
      "|    n_updates        | 194      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.975    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 428      |\n",
      "|    total_timesteps  | 937      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.02e-06 |\n",
      "|    n_updates        | 209      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.974    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 453      |\n",
      "|    total_timesteps  | 988      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.62e-05 |\n",
      "|    n_updates        | 221      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.972    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 481      |\n",
      "|    total_timesteps  | 1045     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.46e-05 |\n",
      "|    n_updates        | 236      |\n",
      "----------------------------------\n",
      "Num timesteps: 1073\n",
      "Best mean reward: 14.43 - Last mean reward per episode: 14.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.971    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 508      |\n",
      "|    total_timesteps  | 1105     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.17e-06 |\n",
      "|    n_updates        | 251      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.969    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 534      |\n",
      "|    total_timesteps  | 1162     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.55e-05 |\n",
      "|    n_updates        | 265      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.968    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 561      |\n",
      "|    total_timesteps  | 1226     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.34e-05 |\n",
      "|    n_updates        | 281      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.966    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 588      |\n",
      "|    total_timesteps  | 1291     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.95e-06 |\n",
      "|    n_updates        | 297      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.965    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 613      |\n",
      "|    total_timesteps  | 1339     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.03e-05 |\n",
      "|    n_updates        | 309      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.963    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 639      |\n",
      "|    total_timesteps  | 1390     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.49e-05 |\n",
      "|    n_updates        | 322      |\n",
      "----------------------------------\n",
      "Num timesteps: 1433\n",
      "Best mean reward: 14.51 - Last mean reward per episode: 14.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.962    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 666      |\n",
      "|    total_timesteps  | 1452     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.67e-05 |\n",
      "|    n_updates        | 337      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.96     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 692      |\n",
      "|    total_timesteps  | 1511     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.63e-05 |\n",
      "|    n_updates        | 352      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.958    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 720      |\n",
      "|    total_timesteps  | 1574     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.2e-05  |\n",
      "|    n_updates        | 368      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.957    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 749      |\n",
      "|    total_timesteps  | 1634     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.41e-05 |\n",
      "|    n_updates        | 383      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.955    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 774      |\n",
      "|    total_timesteps  | 1689     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.23e-05 |\n",
      "|    n_updates        | 397      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.953    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 809      |\n",
      "|    total_timesteps  | 1768     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.12e-05 |\n",
      "|    n_updates        | 416      |\n",
      "----------------------------------\n",
      "Num timesteps: 1793\n",
      "Best mean reward: 14.51 - Last mean reward per episode: 14.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.952    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 835      |\n",
      "|    total_timesteps  | 1830     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.46e-05 |\n",
      "|    n_updates        | 432      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.95     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 860      |\n",
      "|    total_timesteps  | 1889     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.86e-05 |\n",
      "|    n_updates        | 447      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.948    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 892      |\n",
      "|    total_timesteps  | 1959     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.81e-05 |\n",
      "|    n_updates        | 464      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.947    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 917      |\n",
      "|    total_timesteps  | 2017     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.38e-05 |\n",
      "|    n_updates        | 479      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.945    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 943      |\n",
      "|    total_timesteps  | 2071     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.13e-05 |\n",
      "|    n_updates        | 492      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.944    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 971      |\n",
      "|    total_timesteps  | 2132     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.22e-05 |\n",
      "|    n_updates        | 507      |\n",
      "----------------------------------\n",
      "Num timesteps: 2153\n",
      "Best mean reward: 14.73 - Last mean reward per episode: 14.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.942    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 998      |\n",
      "|    total_timesteps  | 2195     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.5e-05  |\n",
      "|    n_updates        | 523      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.94     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1026     |\n",
      "|    total_timesteps  | 2257     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.39e-05 |\n",
      "|    n_updates        | 539      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.939    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1051     |\n",
      "|    total_timesteps  | 2316     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.87e-05 |\n",
      "|    n_updates        | 553      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.937    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1080     |\n",
      "|    total_timesteps  | 2380     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.87e-06 |\n",
      "|    n_updates        | 569      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.936    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1105     |\n",
      "|    total_timesteps  | 2432     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.66e-06 |\n",
      "|    n_updates        | 582      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.934    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1135     |\n",
      "|    total_timesteps  | 2490     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.1e-06  |\n",
      "|    n_updates        | 597      |\n",
      "----------------------------------\n",
      "Num timesteps: 2513\n",
      "Best mean reward: 14.81 - Last mean reward per episode: 14.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.932    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1165     |\n",
      "|    total_timesteps  | 2559     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.11e-05 |\n",
      "|    n_updates        | 614      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.931    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1194     |\n",
      "|    total_timesteps  | 2624     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.34e-05 |\n",
      "|    n_updates        | 630      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.929    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1220     |\n",
      "|    total_timesteps  | 2680     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.69e-06 |\n",
      "|    n_updates        | 644      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.928    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1250     |\n",
      "|    total_timesteps  | 2742     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.23e-06 |\n",
      "|    n_updates        | 660      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.926    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1276     |\n",
      "|    total_timesteps  | 2797     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.53e-05 |\n",
      "|    n_updates        | 674      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.925    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1302     |\n",
      "|    total_timesteps  | 2848     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.71e-06 |\n",
      "|    n_updates        | 686      |\n",
      "----------------------------------\n",
      "Num timesteps: 2873\n",
      "Best mean reward: 14.85 - Last mean reward per episode: 14.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.923    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1327     |\n",
      "|    total_timesteps  | 2905     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.19e-06 |\n",
      "|    n_updates        | 701      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.922    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1356     |\n",
      "|    total_timesteps  | 2972     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.07e-05 |\n",
      "|    n_updates        | 717      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.92     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1382     |\n",
      "|    total_timesteps  | 3022     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.57e-06 |\n",
      "|    n_updates        | 730      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.919    |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1408     |\n",
      "|    total_timesteps  | 3081     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.2e-06  |\n",
      "|    n_updates        | 745      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.917    |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1435     |\n",
      "|    total_timesteps  | 3144     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.77e-06 |\n",
      "|    n_updates        | 760      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.915    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1464     |\n",
      "|    total_timesteps  | 3208     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.28e-05 |\n",
      "|    n_updates        | 776      |\n",
      "----------------------------------\n",
      "Num timesteps: 3233\n",
      "Best mean reward: 14.85 - Last mean reward per episode: 14.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.914    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1490     |\n",
      "|    total_timesteps  | 3258     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.73e-06 |\n",
      "|    n_updates        | 789      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.913    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1515     |\n",
      "|    total_timesteps  | 3312     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.51e-05 |\n",
      "|    n_updates        | 802      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.911    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1541     |\n",
      "|    total_timesteps  | 3368     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.32e-06 |\n",
      "|    n_updates        | 816      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.91     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1567     |\n",
      "|    total_timesteps  | 3425     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.6e-05  |\n",
      "|    n_updates        | 831      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.908    |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1595     |\n",
      "|    total_timesteps  | 3495     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.4e-06  |\n",
      "|    n_updates        | 848      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.906    |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1625     |\n",
      "|    total_timesteps  | 3569     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.22e-06 |\n",
      "|    n_updates        | 867      |\n",
      "----------------------------------\n",
      "Num timesteps: 3593\n",
      "Best mean reward: 14.85 - Last mean reward per episode: 14.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.904    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1652     |\n",
      "|    total_timesteps  | 3625     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.36e-06 |\n",
      "|    n_updates        | 881      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.903    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1678     |\n",
      "|    total_timesteps  | 3680     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.25e-06 |\n",
      "|    n_updates        | 894      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.901    |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1705     |\n",
      "|    total_timesteps  | 3748     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.52e-06 |\n",
      "|    n_updates        | 911      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.899    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1732     |\n",
      "|    total_timesteps  | 3813     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.67e-05 |\n",
      "|    n_updates        | 928      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.898    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1760     |\n",
      "|    total_timesteps  | 3875     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.28e-05 |\n",
      "|    n_updates        | 943      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.896    |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1785     |\n",
      "|    total_timesteps  | 3933     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.25e-07 |\n",
      "|    n_updates        | 958      |\n",
      "----------------------------------\n",
      "Num timesteps: 3953\n",
      "Best mean reward: 14.86 - Last mean reward per episode: 14.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.895    |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1812     |\n",
      "|    total_timesteps  | 3986     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.73e-06 |\n",
      "|    n_updates        | 971      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.893    |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1839     |\n",
      "|    total_timesteps  | 4046     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.5e-07  |\n",
      "|    n_updates        | 986      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.891    |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1868     |\n",
      "|    total_timesteps  | 4114     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.61e-07 |\n",
      "|    n_updates        | 1003     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.89     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1893     |\n",
      "|    total_timesteps  | 4169     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.57e-06 |\n",
      "|    n_updates        | 1017     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.888    |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1921     |\n",
      "|    total_timesteps  | 4232     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.98e-06 |\n",
      "|    n_updates        | 1032     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.887    |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1950     |\n",
      "|    total_timesteps  | 4297     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.31e-05 |\n",
      "|    n_updates        | 1049     |\n",
      "----------------------------------\n",
      "Num timesteps: 4313\n",
      "Best mean reward: 14.90 - Last mean reward per episode: 14.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.885    |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 1977     |\n",
      "|    total_timesteps  | 4358     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.36e-06 |\n",
      "|    n_updates        | 1064     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.883    |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2005     |\n",
      "|    total_timesteps  | 4422     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.68e-06 |\n",
      "|    n_updates        | 1080     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.882    |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2032     |\n",
      "|    total_timesteps  | 4480     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.61e-07 |\n",
      "|    n_updates        | 1094     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.88     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2059     |\n",
      "|    total_timesteps  | 4545     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.46e-05 |\n",
      "|    n_updates        | 1111     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.878    |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2086     |\n",
      "|    total_timesteps  | 4609     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.93e-06 |\n",
      "|    n_updates        | 1127     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.877    |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2114     |\n",
      "|    total_timesteps  | 4670     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.43e-05 |\n",
      "|    n_updates        | 1142     |\n",
      "----------------------------------\n",
      "Num timesteps: 4673\n",
      "Best mean reward: 14.92 - Last mean reward per episode: 14.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.875    |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2141     |\n",
      "|    total_timesteps  | 4731     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.25e-05 |\n",
      "|    n_updates        | 1157     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.874    |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2166     |\n",
      "|    total_timesteps  | 4788     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.06e-06 |\n",
      "|    n_updates        | 1171     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.872    |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2192     |\n",
      "|    total_timesteps  | 4843     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.5e-06  |\n",
      "|    n_updates        | 1185     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.87     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2222     |\n",
      "|    total_timesteps  | 4913     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.16e-07 |\n",
      "|    n_updates        | 1203     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.869    |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2250     |\n",
      "|    total_timesteps  | 4969     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.42e-05 |\n",
      "|    n_updates        | 1217     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.867    |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2275     |\n",
      "|    total_timesteps  | 5023     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.13e-05 |\n",
      "|    n_updates        | 1230     |\n",
      "----------------------------------\n",
      "Num timesteps: 5033\n",
      "Best mean reward: 14.97 - Last mean reward per episode: 14.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.866    |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2307     |\n",
      "|    total_timesteps  | 5096     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.19e-06 |\n",
      "|    n_updates        | 1248     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.864    |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2335     |\n",
      "|    total_timesteps  | 5161     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.18e-06 |\n",
      "|    n_updates        | 1265     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.862    |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2360     |\n",
      "|    total_timesteps  | 5218     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.15e-05 |\n",
      "|    n_updates        | 1279     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.86     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2390     |\n",
      "|    total_timesteps  | 5289     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.12e-05 |\n",
      "|    n_updates        | 1297     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.859    |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2416     |\n",
      "|    total_timesteps  | 5344     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.99e-06 |\n",
      "|    n_updates        | 1310     |\n",
      "----------------------------------\n",
      "Num timesteps: 5393\n",
      "Best mean reward: 14.97 - Last mean reward per episode: 15.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.857    |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2443     |\n",
      "|    total_timesteps  | 5407     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.51e-06 |\n",
      "|    n_updates        | 1326     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.856    |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2469     |\n",
      "|    total_timesteps  | 5465     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.05e-05 |\n",
      "|    n_updates        | 1341     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.853    |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2510     |\n",
      "|    total_timesteps  | 5562     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.62e-06 |\n",
      "|    n_updates        | 1365     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.852    |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2537     |\n",
      "|    total_timesteps  | 5615     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.69e-06 |\n",
      "|    n_updates        | 1378     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.85     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2565     |\n",
      "|    total_timesteps  | 5681     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.45e-06 |\n",
      "|    n_updates        | 1395     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2595     |\n",
      "|    total_timesteps  | 5747     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.65e-06 |\n",
      "|    n_updates        | 1411     |\n",
      "----------------------------------\n",
      "Num timesteps: 5753\n",
      "Best mean reward: 15.01 - Last mean reward per episode: 15.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.847    |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2622     |\n",
      "|    total_timesteps  | 5807     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.15e-06 |\n",
      "|    n_updates        | 1426     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.845    |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2651     |\n",
      "|    total_timesteps  | 5873     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.22e-05 |\n",
      "|    n_updates        | 1443     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.843    |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2679     |\n",
      "|    total_timesteps  | 5938     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.8e-06  |\n",
      "|    n_updates        | 1459     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.842    |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2706     |\n",
      "|    total_timesteps  | 5996     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.16e-06 |\n",
      "|    n_updates        | 1473     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.84     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2734     |\n",
      "|    total_timesteps  | 6058     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.66e-06 |\n",
      "|    n_updates        | 1489     |\n",
      "----------------------------------\n",
      "Num timesteps: 6113\n",
      "Best mean reward: 15.24 - Last mean reward per episode: 15.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.839    |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2759     |\n",
      "|    total_timesteps  | 6115     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.21e-06 |\n",
      "|    n_updates        | 1503     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.837    |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2786     |\n",
      "|    total_timesteps  | 6174     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.58e-06 |\n",
      "|    n_updates        | 1518     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.835    |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2814     |\n",
      "|    total_timesteps  | 6239     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.75e-06 |\n",
      "|    n_updates        | 1534     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.834    |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2842     |\n",
      "|    total_timesteps  | 6298     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.41e-05 |\n",
      "|    n_updates        | 1549     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.832    |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2868     |\n",
      "|    total_timesteps  | 6352     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.19e-06 |\n",
      "|    n_updates        | 1562     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.831    |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2897     |\n",
      "|    total_timesteps  | 6416     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.92e-05 |\n",
      "|    n_updates        | 1578     |\n",
      "----------------------------------\n",
      "Num timesteps: 6473\n",
      "Best mean reward: 15.24 - Last mean reward per episode: 15.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.829    |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2923     |\n",
      "|    total_timesteps  | 6483     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.83e-05 |\n",
      "|    n_updates        | 1595     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.827    |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2950     |\n",
      "|    total_timesteps  | 6538     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.52e-06 |\n",
      "|    n_updates        | 1609     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.826    |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 2976     |\n",
      "|    total_timesteps  | 6600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.23e-06 |\n",
      "|    n_updates        | 1624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.824    |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3010     |\n",
      "|    total_timesteps  | 6674     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.35e-05 |\n",
      "|    n_updates        | 1643     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.822    |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3036     |\n",
      "|    total_timesteps  | 6733     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.97e-06 |\n",
      "|    n_updates        | 1658     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.821    |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3063     |\n",
      "|    total_timesteps  | 6792     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.33e-05 |\n",
      "|    n_updates        | 1672     |\n",
      "----------------------------------\n",
      "Num timesteps: 6833\n",
      "Best mean reward: 15.26 - Last mean reward per episode: 15.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.819    |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3091     |\n",
      "|    total_timesteps  | 6854     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.08e-06 |\n",
      "|    n_updates        | 1688     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.817    |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3119     |\n",
      "|    total_timesteps  | 6918     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.42e-05 |\n",
      "|    n_updates        | 1704     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.816    |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3149     |\n",
      "|    total_timesteps  | 6986     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.79e-06 |\n",
      "|    n_updates        | 1721     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.814    |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3175     |\n",
      "|    total_timesteps  | 7043     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.7e-06  |\n",
      "|    n_updates        | 1735     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.812    |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3203     |\n",
      "|    total_timesteps  | 7108     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.82e-05 |\n",
      "|    n_updates        | 1751     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.811    |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3231     |\n",
      "|    total_timesteps  | 7176     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.23e-05 |\n",
      "|    n_updates        | 1768     |\n",
      "----------------------------------\n",
      "Num timesteps: 7193\n",
      "Best mean reward: 15.30 - Last mean reward per episode: 15.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.809    |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3263     |\n",
      "|    total_timesteps  | 7246     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.34e-05 |\n",
      "|    n_updates        | 1786     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.807    |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3295     |\n",
      "|    total_timesteps  | 7310     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.77e-06 |\n",
      "|    n_updates        | 1802     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.805    |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3325     |\n",
      "|    total_timesteps  | 7383     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.37e-05 |\n",
      "|    n_updates        | 1820     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.804    |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3350     |\n",
      "|    total_timesteps  | 7439     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.51e-06 |\n",
      "|    n_updates        | 1834     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.802    |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3381     |\n",
      "|    total_timesteps  | 7510     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.54e-06 |\n",
      "|    n_updates        | 1852     |\n",
      "----------------------------------\n",
      "Num timesteps: 7553\n",
      "Best mean reward: 15.39 - Last mean reward per episode: 15.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.8      |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3408     |\n",
      "|    total_timesteps  | 7575     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.35e-05 |\n",
      "|    n_updates        | 1868     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.798    |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3435     |\n",
      "|    total_timesteps  | 7636     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.57e-06 |\n",
      "|    n_updates        | 1883     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.797    |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3465     |\n",
      "|    total_timesteps  | 7704     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.7e-06  |\n",
      "|    n_updates        | 1900     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16       |\n",
      "|    ep_rew_mean      | 16       |\n",
      "|    exploration_rate | 0.795    |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3494     |\n",
      "|    total_timesteps  | 7774     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.87e-05 |\n",
      "|    n_updates        | 1918     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.793    |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3521     |\n",
      "|    total_timesteps  | 7832     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.41e-06 |\n",
      "|    n_updates        | 1932     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.792    |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3547     |\n",
      "|    total_timesteps  | 7890     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.82e-06 |\n",
      "|    n_updates        | 1947     |\n",
      "----------------------------------\n",
      "Num timesteps: 7913\n",
      "Best mean reward: 15.44 - Last mean reward per episode: 15.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16       |\n",
      "|    ep_rew_mean      | 16       |\n",
      "|    exploration_rate | 0.79     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3574     |\n",
      "|    total_timesteps  | 7949     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.38e-06 |\n",
      "|    n_updates        | 1962     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16       |\n",
      "|    ep_rew_mean      | 16       |\n",
      "|    exploration_rate | 0.789    |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3602     |\n",
      "|    total_timesteps  | 8013     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.19e-06 |\n",
      "|    n_updates        | 1978     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.787    |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3627     |\n",
      "|    total_timesteps  | 8066     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.25e-06 |\n",
      "|    n_updates        | 1991     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.785    |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3656     |\n",
      "|    total_timesteps  | 8133     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.91e-06 |\n",
      "|    n_updates        | 2008     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.784    |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3684     |\n",
      "|    total_timesteps  | 8193     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.81e-06 |\n",
      "|    n_updates        | 2023     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.782    |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3711     |\n",
      "|    total_timesteps  | 8250     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1e-05    |\n",
      "|    n_updates        | 2037     |\n",
      "----------------------------------\n",
      "Num timesteps: 8273\n",
      "Best mean reward: 15.47 - Last mean reward per episode: 15.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.781    |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3737     |\n",
      "|    total_timesteps  | 8303     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.45e-06 |\n",
      "|    n_updates        | 2050     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.779    |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3764     |\n",
      "|    total_timesteps  | 8363     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.13e-05 |\n",
      "|    n_updates        | 2065     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.778    |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3790     |\n",
      "|    total_timesteps  | 8422     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.96e-05 |\n",
      "|    n_updates        | 2080     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.776    |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3818     |\n",
      "|    total_timesteps  | 8486     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.44e-06 |\n",
      "|    n_updates        | 2096     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.775    |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3843     |\n",
      "|    total_timesteps  | 8537     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.51e-06 |\n",
      "|    n_updates        | 2109     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.773    |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3870     |\n",
      "|    total_timesteps  | 8596     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.16e-07 |\n",
      "|    n_updates        | 2123     |\n",
      "----------------------------------\n",
      "Num timesteps: 8633\n",
      "Best mean reward: 15.47 - Last mean reward per episode: 15.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.772    |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3899     |\n",
      "|    total_timesteps  | 8657     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.02e-06 |\n",
      "|    n_updates        | 2139     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.77     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3926     |\n",
      "|    total_timesteps  | 8715     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.13e-06 |\n",
      "|    n_updates        | 2153     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.768    |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3951     |\n",
      "|    total_timesteps  | 8773     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.91e-05 |\n",
      "|    n_updates        | 2168     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.767    |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 3976     |\n",
      "|    total_timesteps  | 8835     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.32e-05 |\n",
      "|    n_updates        | 2183     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.765    |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4005     |\n",
      "|    total_timesteps  | 8898     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.69e-06 |\n",
      "|    n_updates        | 2199     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.764    |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4032     |\n",
      "|    total_timesteps  | 8953     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.75e-06 |\n",
      "|    n_updates        | 2213     |\n",
      "----------------------------------\n",
      "Num timesteps: 8993\n",
      "Best mean reward: 15.49 - Last mean reward per episode: 15.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.762    |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4065     |\n",
      "|    total_timesteps  | 9028     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4e-06    |\n",
      "|    n_updates        | 2231     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.76     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4094     |\n",
      "|    total_timesteps  | 9090     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.21e-06 |\n",
      "|    n_updates        | 2247     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.758    |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4123     |\n",
      "|    total_timesteps  | 9161     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.62e-06 |\n",
      "|    n_updates        | 2265     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.757    |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4149     |\n",
      "|    total_timesteps  | 9222     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.11e-05 |\n",
      "|    n_updates        | 2280     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.755    |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4176     |\n",
      "|    total_timesteps  | 9284     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.02e-05 |\n",
      "|    n_updates        | 2295     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.753    |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4205     |\n",
      "|    total_timesteps  | 9349     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.91e-05 |\n",
      "|    n_updates        | 2312     |\n",
      "----------------------------------\n",
      "Num timesteps: 9353\n",
      "Best mean reward: 15.51 - Last mean reward per episode: 15.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.752    |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4230     |\n",
      "|    total_timesteps  | 9401     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.41e-05 |\n",
      "|    n_updates        | 2325     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.75     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4264     |\n",
      "|    total_timesteps  | 9470     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.96e-06 |\n",
      "|    n_updates        | 2342     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.748    |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4290     |\n",
      "|    total_timesteps  | 9533     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.91e-06 |\n",
      "|    n_updates        | 2358     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.747    |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4321     |\n",
      "|    total_timesteps  | 9603     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.01e-06 |\n",
      "|    n_updates        | 2375     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.745    |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4351     |\n",
      "|    total_timesteps  | 9674     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.44e-06 |\n",
      "|    n_updates        | 2393     |\n",
      "----------------------------------\n",
      "Num timesteps: 9713\n",
      "Best mean reward: 15.56 - Last mean reward per episode: 15.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.743    |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4380     |\n",
      "|    total_timesteps  | 9743     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.91e-05 |\n",
      "|    n_updates        | 2410     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.742    |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4406     |\n",
      "|    total_timesteps  | 9793     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.37e-06 |\n",
      "|    n_updates        | 2423     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.74     |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4432     |\n",
      "|    total_timesteps  | 9848     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.48e-05 |\n",
      "|    n_updates        | 2436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.738    |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4462     |\n",
      "|    total_timesteps  | 9911     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.55e-06 |\n",
      "|    n_updates        | 2452     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.737    |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4492     |\n",
      "|    total_timesteps  | 9977     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.35e-06 |\n",
      "|    n_updates        | 2469     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.735    |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4518     |\n",
      "|    total_timesteps  | 10036    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.116    |\n",
      "|    n_updates        | 2483     |\n",
      "----------------------------------\n",
      "Num timesteps: 10073\n",
      "Best mean reward: 15.64 - Last mean reward per episode: 15.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.733    |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4548     |\n",
      "|    total_timesteps  | 10104    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0684   |\n",
      "|    n_updates        | 2500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.732    |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4574     |\n",
      "|    total_timesteps  | 10162    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00879  |\n",
      "|    n_updates        | 2515     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.73     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4603     |\n",
      "|    total_timesteps  | 10227    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0622   |\n",
      "|    n_updates        | 2531     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.728    |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4630     |\n",
      "|    total_timesteps  | 10296    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0136   |\n",
      "|    n_updates        | 2548     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.727    |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4657     |\n",
      "|    total_timesteps  | 10362    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000613 |\n",
      "|    n_updates        | 2565     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.725    |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4684     |\n",
      "|    total_timesteps  | 10420    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00268  |\n",
      "|    n_updates        | 2579     |\n",
      "----------------------------------\n",
      "Num timesteps: 10433\n",
      "Best mean reward: 15.64 - Last mean reward per episode: 15.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.724    |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4711     |\n",
      "|    total_timesteps  | 10476    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0152   |\n",
      "|    n_updates        | 2593     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.722    |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4737     |\n",
      "|    total_timesteps  | 10531    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0127   |\n",
      "|    n_updates        | 2607     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.72     |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4769     |\n",
      "|    total_timesteps  | 10615    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 2628     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.718    |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4794     |\n",
      "|    total_timesteps  | 10671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0285   |\n",
      "|    n_updates        | 2642     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.717    |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4822     |\n",
      "|    total_timesteps  | 10735    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00297  |\n",
      "|    n_updates        | 2658     |\n",
      "----------------------------------\n",
      "Num timesteps: 10793\n",
      "Best mean reward: 15.64 - Last mean reward per episode: 15.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4850     |\n",
      "|    total_timesteps  | 10799    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.023    |\n",
      "|    n_updates        | 2674     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.713    |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4876     |\n",
      "|    total_timesteps  | 10859    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0196   |\n",
      "|    n_updates        | 2689     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.712    |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4902     |\n",
      "|    total_timesteps  | 10927    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0248   |\n",
      "|    n_updates        | 2706     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.71     |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4928     |\n",
      "|    total_timesteps  | 10990    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0166   |\n",
      "|    n_updates        | 2722     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.708    |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4955     |\n",
      "|    total_timesteps  | 11052    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.012    |\n",
      "|    n_updates        | 2737     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.707    |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 4983     |\n",
      "|    total_timesteps  | 11110    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0012   |\n",
      "|    n_updates        | 2752     |\n",
      "----------------------------------\n",
      "Num timesteps: 11153\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.57\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.705    |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5008     |\n",
      "|    total_timesteps  | 11161    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00437  |\n",
      "|    n_updates        | 2765     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.704    |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5034     |\n",
      "|    total_timesteps  | 11219    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0162   |\n",
      "|    n_updates        | 2779     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.702    |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5059     |\n",
      "|    total_timesteps  | 11274    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00403  |\n",
      "|    n_updates        | 2793     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.701    |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5086     |\n",
      "|    total_timesteps  | 11338    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00986  |\n",
      "|    n_updates        | 2809     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.699    |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5112     |\n",
      "|    total_timesteps  | 11398    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00517  |\n",
      "|    n_updates        | 2824     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.697    |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5138     |\n",
      "|    total_timesteps  | 11466    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0109   |\n",
      "|    n_updates        | 2841     |\n",
      "----------------------------------\n",
      "Num timesteps: 11513\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5164     |\n",
      "|    total_timesteps  | 11522    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 2855     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.694    |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5196     |\n",
      "|    total_timesteps  | 11608    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00923  |\n",
      "|    n_updates        | 2876     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.692    |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5222     |\n",
      "|    total_timesteps  | 11663    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00965  |\n",
      "|    n_updates        | 2890     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.691    |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5249     |\n",
      "|    total_timesteps  | 11725    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0102   |\n",
      "|    n_updates        | 2906     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.689    |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5274     |\n",
      "|    total_timesteps  | 11780    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00152  |\n",
      "|    n_updates        | 2919     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.688    |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5299     |\n",
      "|    total_timesteps  | 11833    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00966  |\n",
      "|    n_updates        | 2933     |\n",
      "----------------------------------\n",
      "Num timesteps: 11873\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.686    |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5326     |\n",
      "|    total_timesteps  | 11895    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00845  |\n",
      "|    n_updates        | 2948     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.685    |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5353     |\n",
      "|    total_timesteps  | 11954    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00865  |\n",
      "|    n_updates        | 2963     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.683    |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5378     |\n",
      "|    total_timesteps  | 12007    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0142   |\n",
      "|    n_updates        | 2976     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.681    |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5405     |\n",
      "|    total_timesteps  | 12073    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0175   |\n",
      "|    n_updates        | 2993     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.68     |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5431     |\n",
      "|    total_timesteps  | 12134    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0117   |\n",
      "|    n_updates        | 3008     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.678    |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5457     |\n",
      "|    total_timesteps  | 12196    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0116   |\n",
      "|    n_updates        | 3023     |\n",
      "----------------------------------\n",
      "Num timesteps: 12233\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.677    |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5483     |\n",
      "|    total_timesteps  | 12253    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00196  |\n",
      "|    n_updates        | 3038     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.675    |\n",
      "| time/               |          |\n",
      "|    episodes         | 804      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5508     |\n",
      "|    total_timesteps  | 12306    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00227  |\n",
      "|    n_updates        | 3051     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.673    |\n",
      "| time/               |          |\n",
      "|    episodes         | 808      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5536     |\n",
      "|    total_timesteps  | 12373    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00217  |\n",
      "|    n_updates        | 3068     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.672    |\n",
      "| time/               |          |\n",
      "|    episodes         | 812      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5563     |\n",
      "|    total_timesteps  | 12434    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00606  |\n",
      "|    n_updates        | 3083     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.67     |\n",
      "| time/               |          |\n",
      "|    episodes         | 816      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5588     |\n",
      "|    total_timesteps  | 12489    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.001    |\n",
      "|    n_updates        | 3097     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.669    |\n",
      "| time/               |          |\n",
      "|    episodes         | 820      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5613     |\n",
      "|    total_timesteps  | 12551    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00255  |\n",
      "|    n_updates        | 3112     |\n",
      "----------------------------------\n",
      "Num timesteps: 12593\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.667    |\n",
      "| time/               |          |\n",
      "|    episodes         | 824      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5638     |\n",
      "|    total_timesteps  | 12601    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0288   |\n",
      "|    n_updates        | 3125     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.666    |\n",
      "| time/               |          |\n",
      "|    episodes         | 828      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5665     |\n",
      "|    total_timesteps  | 12672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00489  |\n",
      "|    n_updates        | 3142     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.664    |\n",
      "| time/               |          |\n",
      "|    episodes         | 832      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5693     |\n",
      "|    total_timesteps  | 12735    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00975  |\n",
      "|    n_updates        | 3158     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.662    |\n",
      "| time/               |          |\n",
      "|    episodes         | 836      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5718     |\n",
      "|    total_timesteps  | 12795    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00321  |\n",
      "|    n_updates        | 3173     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.661    |\n",
      "| time/               |          |\n",
      "|    episodes         | 840      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5744     |\n",
      "|    total_timesteps  | 12852    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00861  |\n",
      "|    n_updates        | 3187     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.659    |\n",
      "| time/               |          |\n",
      "|    episodes         | 844      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5770     |\n",
      "|    total_timesteps  | 12905    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00972  |\n",
      "|    n_updates        | 3201     |\n",
      "----------------------------------\n",
      "Num timesteps: 12953\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.658    |\n",
      "| time/               |          |\n",
      "|    episodes         | 848      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5799     |\n",
      "|    total_timesteps  | 12972    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0091   |\n",
      "|    n_updates        | 3217     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.656    |\n",
      "| time/               |          |\n",
      "|    episodes         | 852      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5827     |\n",
      "|    total_timesteps  | 13036    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00616  |\n",
      "|    n_updates        | 3233     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.654    |\n",
      "| time/               |          |\n",
      "|    episodes         | 856      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5853     |\n",
      "|    total_timesteps  | 13098    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00658  |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.653    |\n",
      "| time/               |          |\n",
      "|    episodes         | 860      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5878     |\n",
      "|    total_timesteps  | 13155    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000867 |\n",
      "|    n_updates        | 3263     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.651    |\n",
      "| time/               |          |\n",
      "|    episodes         | 864      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5908     |\n",
      "|    total_timesteps  | 13225    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0383   |\n",
      "|    n_updates        | 3281     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.649    |\n",
      "| time/               |          |\n",
      "|    episodes         | 868      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5935     |\n",
      "|    total_timesteps  | 13287    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00215  |\n",
      "|    n_updates        | 3296     |\n",
      "----------------------------------\n",
      "Num timesteps: 13313\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.648    |\n",
      "| time/               |          |\n",
      "|    episodes         | 872      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5961     |\n",
      "|    total_timesteps  | 13349    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00796  |\n",
      "|    n_updates        | 3312     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.646    |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 5987     |\n",
      "|    total_timesteps  | 13407    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00127  |\n",
      "|    n_updates        | 3326     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.645    |\n",
      "| time/               |          |\n",
      "|    episodes         | 880      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6013     |\n",
      "|    total_timesteps  | 13467    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00552  |\n",
      "|    n_updates        | 3341     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.643    |\n",
      "| time/               |          |\n",
      "|    episodes         | 884      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6039     |\n",
      "|    total_timesteps  | 13532    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00262  |\n",
      "|    n_updates        | 3357     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.641    |\n",
      "| time/               |          |\n",
      "|    episodes         | 888      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6066     |\n",
      "|    total_timesteps  | 13596    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 3373     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.64     |\n",
      "| time/               |          |\n",
      "|    episodes         | 892      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6093     |\n",
      "|    total_timesteps  | 13659    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00987  |\n",
      "|    n_updates        | 3389     |\n",
      "----------------------------------\n",
      "Num timesteps: 13673\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.638    |\n",
      "| time/               |          |\n",
      "|    episodes         | 896      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6119     |\n",
      "|    total_timesteps  | 13724    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0125   |\n",
      "|    n_updates        | 3405     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.636    |\n",
      "| time/               |          |\n",
      "|    episodes         | 900      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6145     |\n",
      "|    total_timesteps  | 13784    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00452  |\n",
      "|    n_updates        | 3420     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.635    |\n",
      "| time/               |          |\n",
      "|    episodes         | 904      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6170     |\n",
      "|    total_timesteps  | 13845    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00328  |\n",
      "|    n_updates        | 3436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.633    |\n",
      "| time/               |          |\n",
      "|    episodes         | 908      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6199     |\n",
      "|    total_timesteps  | 13904    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00244  |\n",
      "|    n_updates        | 3450     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.632    |\n",
      "| time/               |          |\n",
      "|    episodes         | 912      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6225     |\n",
      "|    total_timesteps  | 13959    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00228  |\n",
      "|    n_updates        | 3464     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.63     |\n",
      "| time/               |          |\n",
      "|    episodes         | 916      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6251     |\n",
      "|    total_timesteps  | 14018    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00902  |\n",
      "|    n_updates        | 3479     |\n",
      "----------------------------------\n",
      "Num timesteps: 14033\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.628    |\n",
      "| time/               |          |\n",
      "|    episodes         | 920      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6278     |\n",
      "|    total_timesteps  | 14078    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000697 |\n",
      "|    n_updates        | 3494     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.627    |\n",
      "| time/               |          |\n",
      "|    episodes         | 924      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6303     |\n",
      "|    total_timesteps  | 14135    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0222   |\n",
      "|    n_updates        | 3508     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.625    |\n",
      "| time/               |          |\n",
      "|    episodes         | 928      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6330     |\n",
      "|    total_timesteps  | 14197    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00715  |\n",
      "|    n_updates        | 3524     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.624    |\n",
      "| time/               |          |\n",
      "|    episodes         | 932      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6355     |\n",
      "|    total_timesteps  | 14248    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00379  |\n",
      "|    n_updates        | 3536     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.622    |\n",
      "| time/               |          |\n",
      "|    episodes         | 936      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6383     |\n",
      "|    total_timesteps  | 14313    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00536  |\n",
      "|    n_updates        | 3553     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.621    |\n",
      "| time/               |          |\n",
      "|    episodes         | 940      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6408     |\n",
      "|    total_timesteps  | 14368    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0158   |\n",
      "|    n_updates        | 3566     |\n",
      "----------------------------------\n",
      "Num timesteps: 14393\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.619    |\n",
      "| time/               |          |\n",
      "|    episodes         | 944      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6434     |\n",
      "|    total_timesteps  | 14431    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00419  |\n",
      "|    n_updates        | 3582     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.617    |\n",
      "| time/               |          |\n",
      "|    episodes         | 948      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6464     |\n",
      "|    total_timesteps  | 14497    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0135   |\n",
      "|    n_updates        | 3599     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.616    |\n",
      "| time/               |          |\n",
      "|    episodes         | 952      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6489     |\n",
      "|    total_timesteps  | 14556    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00453  |\n",
      "|    n_updates        | 3613     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.614    |\n",
      "| time/               |          |\n",
      "|    episodes         | 956      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6519     |\n",
      "|    total_timesteps  | 14633    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00231  |\n",
      "|    n_updates        | 3633     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.613    |\n",
      "| time/               |          |\n",
      "|    episodes         | 960      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6544     |\n",
      "|    total_timesteps  | 14681    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0214   |\n",
      "|    n_updates        | 3645     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.611    |\n",
      "| time/               |          |\n",
      "|    episodes         | 964      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6572     |\n",
      "|    total_timesteps  | 14748    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0251   |\n",
      "|    n_updates        | 3661     |\n",
      "----------------------------------\n",
      "Num timesteps: 14753\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.609    |\n",
      "| time/               |          |\n",
      "|    episodes         | 968      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6597     |\n",
      "|    total_timesteps  | 14800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00405  |\n",
      "|    n_updates        | 3674     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.608    |\n",
      "| time/               |          |\n",
      "|    episodes         | 972      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6623     |\n",
      "|    total_timesteps  | 14863    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0156   |\n",
      "|    n_updates        | 3690     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.606    |\n",
      "| time/               |          |\n",
      "|    episodes         | 976      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6653     |\n",
      "|    total_timesteps  | 14932    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0234   |\n",
      "|    n_updates        | 3707     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.604    |\n",
      "| time/               |          |\n",
      "|    episodes         | 980      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6682     |\n",
      "|    total_timesteps  | 14990    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00247  |\n",
      "|    n_updates        | 3722     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.603    |\n",
      "| time/               |          |\n",
      "|    episodes         | 984      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6709     |\n",
      "|    total_timesteps  | 15054    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00205  |\n",
      "|    n_updates        | 3738     |\n",
      "----------------------------------\n",
      "Num timesteps: 15113\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.601    |\n",
      "| time/               |          |\n",
      "|    episodes         | 988      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6735     |\n",
      "|    total_timesteps  | 15115    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00211  |\n",
      "|    n_updates        | 3753     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.6      |\n",
      "| time/               |          |\n",
      "|    episodes         | 992      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6761     |\n",
      "|    total_timesteps  | 15169    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00777  |\n",
      "|    n_updates        | 3767     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.598    |\n",
      "| time/               |          |\n",
      "|    episodes         | 996      |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6788     |\n",
      "|    total_timesteps  | 15227    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00351  |\n",
      "|    n_updates        | 3781     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.597    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6812     |\n",
      "|    total_timesteps  | 15281    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00808  |\n",
      "|    n_updates        | 3795     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.595    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1004     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6838     |\n",
      "|    total_timesteps  | 15338    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0189   |\n",
      "|    n_updates        | 3809     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.594    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1008     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6866     |\n",
      "|    total_timesteps  | 15402    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00277  |\n",
      "|    n_updates        | 3825     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.592    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1012     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6891     |\n",
      "|    total_timesteps  | 15453    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00967  |\n",
      "|    n_updates        | 3838     |\n",
      "----------------------------------\n",
      "Num timesteps: 15473\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.591    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1016     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6917     |\n",
      "|    total_timesteps  | 15515    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00221  |\n",
      "|    n_updates        | 3853     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.589    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1020     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6944     |\n",
      "|    total_timesteps  | 15581    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 3870     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.587    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1024     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 6971     |\n",
      "|    total_timesteps  | 15639    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0134   |\n",
      "|    n_updates        | 3884     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.586    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1028     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7002     |\n",
      "|    total_timesteps  | 15707    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00125  |\n",
      "|    n_updates        | 3901     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.584    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1032     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7028     |\n",
      "|    total_timesteps  | 15765    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00029  |\n",
      "|    n_updates        | 3916     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.583    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1036     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7054     |\n",
      "|    total_timesteps  | 15819    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00126  |\n",
      "|    n_updates        | 3929     |\n",
      "----------------------------------\n",
      "Num timesteps: 15833\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.581    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1040     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7079     |\n",
      "|    total_timesteps  | 15871    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000605 |\n",
      "|    n_updates        | 3942     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.58     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1044     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7104     |\n",
      "|    total_timesteps  | 15924    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00816  |\n",
      "|    n_updates        | 3955     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.578    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1048     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7130     |\n",
      "|    total_timesteps  | 15985    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000997 |\n",
      "|    n_updates        | 3971     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.577    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1052     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7156     |\n",
      "|    total_timesteps  | 16046    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00356  |\n",
      "|    n_updates        | 3986     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.575    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1056     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7182     |\n",
      "|    total_timesteps  | 16111    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00833  |\n",
      "|    n_updates        | 4002     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.574    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1060     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7208     |\n",
      "|    total_timesteps  | 16161    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00722  |\n",
      "|    n_updates        | 4015     |\n",
      "----------------------------------\n",
      "Num timesteps: 16193\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.572    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1064     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7234     |\n",
      "|    total_timesteps  | 16212    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0149   |\n",
      "|    n_updates        | 4027     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.571    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1068     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7260     |\n",
      "|    total_timesteps  | 16275    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00413  |\n",
      "|    n_updates        | 4043     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.569    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1072     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7285     |\n",
      "|    total_timesteps  | 16338    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00385  |\n",
      "|    n_updates        | 4059     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.567    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1076     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7310     |\n",
      "|    total_timesteps  | 16392    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00995  |\n",
      "|    n_updates        | 4072     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.566    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1080     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7338     |\n",
      "|    total_timesteps  | 16458    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0057   |\n",
      "|    n_updates        | 4089     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.564    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1084     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7366     |\n",
      "|    total_timesteps  | 16525    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00167  |\n",
      "|    n_updates        | 4106     |\n",
      "----------------------------------\n",
      "Num timesteps: 16553\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.563    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1088     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7393     |\n",
      "|    total_timesteps  | 16578    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00126  |\n",
      "|    n_updates        | 4119     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.561    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1092     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7421     |\n",
      "|    total_timesteps  | 16641    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000607 |\n",
      "|    n_updates        | 4135     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.559    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1096     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7447     |\n",
      "|    total_timesteps  | 16696    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00892  |\n",
      "|    n_updates        | 4148     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.558    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1100     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7475     |\n",
      "|    total_timesteps  | 16764    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0019   |\n",
      "|    n_updates        | 4165     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.556    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1104     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7500     |\n",
      "|    total_timesteps  | 16818    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00112  |\n",
      "|    n_updates        | 4179     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.555    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1108     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7526     |\n",
      "|    total_timesteps  | 16874    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00153  |\n",
      "|    n_updates        | 4193     |\n",
      "----------------------------------\n",
      "Num timesteps: 16913\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.553    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1112     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7553     |\n",
      "|    total_timesteps  | 16934    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00292  |\n",
      "|    n_updates        | 4208     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.552    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1116     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7579     |\n",
      "|    total_timesteps  | 16994    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000197 |\n",
      "|    n_updates        | 4223     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.55     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1120     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7604     |\n",
      "|    total_timesteps  | 17051    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.014    |\n",
      "|    n_updates        | 4237     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.549    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1124     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7629     |\n",
      "|    total_timesteps  | 17108    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00118  |\n",
      "|    n_updates        | 4251     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.547    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1128     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7653     |\n",
      "|    total_timesteps  | 17164    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 4265     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.545    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1132     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7685     |\n",
      "|    total_timesteps  | 17234    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00797  |\n",
      "|    n_updates        | 4283     |\n",
      "----------------------------------\n",
      "Num timesteps: 17273\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1136     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7712     |\n",
      "|    total_timesteps  | 17298    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 4299     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.542    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1140     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7741     |\n",
      "|    total_timesteps  | 17371    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00917  |\n",
      "|    n_updates        | 4317     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.54     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1144     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7767     |\n",
      "|    total_timesteps  | 17429    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00186  |\n",
      "|    n_updates        | 4332     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.539    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1148     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7793     |\n",
      "|    total_timesteps  | 17486    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00209  |\n",
      "|    n_updates        | 4346     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.537    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1152     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7819     |\n",
      "|    total_timesteps  | 17549    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0018   |\n",
      "|    n_updates        | 4362     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.535    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1156     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7844     |\n",
      "|    total_timesteps  | 17609    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 4377     |\n",
      "----------------------------------\n",
      "Num timesteps: 17633\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.534    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1160     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7870     |\n",
      "|    total_timesteps  | 17655    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000655 |\n",
      "|    n_updates        | 4388     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.533    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1164     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7896     |\n",
      "|    total_timesteps  | 17711    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00442  |\n",
      "|    n_updates        | 4402     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.531    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1168     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7921     |\n",
      "|    total_timesteps  | 17764    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00877  |\n",
      "|    n_updates        | 4415     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.529    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1172     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7954     |\n",
      "|    total_timesteps  | 17845    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00347  |\n",
      "|    n_updates        | 4436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.527    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1176     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 7980     |\n",
      "|    total_timesteps  | 17908    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00334  |\n",
      "|    n_updates        | 4451     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.526    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1180     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8007     |\n",
      "|    total_timesteps  | 17967    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00334  |\n",
      "|    n_updates        | 4466     |\n",
      "----------------------------------\n",
      "Num timesteps: 17993\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.524    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1184     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8033     |\n",
      "|    total_timesteps  | 18024    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 4480     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.523    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1188     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8059     |\n",
      "|    total_timesteps  | 18081    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00559  |\n",
      "|    n_updates        | 4495     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.521    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1192     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8086     |\n",
      "|    total_timesteps  | 18145    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.019    |\n",
      "|    n_updates        | 4511     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.519    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1196     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8113     |\n",
      "|    total_timesteps  | 18211    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00121  |\n",
      "|    n_updates        | 4527     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.518    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1200     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8138     |\n",
      "|    total_timesteps  | 18270    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00196  |\n",
      "|    n_updates        | 4542     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.516    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1204     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8165     |\n",
      "|    total_timesteps  | 18329    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00424  |\n",
      "|    n_updates        | 4557     |\n",
      "----------------------------------\n",
      "Num timesteps: 18353\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.515    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1208     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8191     |\n",
      "|    total_timesteps  | 18391    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00273  |\n",
      "|    n_updates        | 4572     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.513    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1212     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8217     |\n",
      "|    total_timesteps  | 18447    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00257  |\n",
      "|    n_updates        | 4586     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.512    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1216     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8242     |\n",
      "|    total_timesteps  | 18497    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00252  |\n",
      "|    n_updates        | 4599     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.51     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1220     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8267     |\n",
      "|    total_timesteps  | 18552    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00957  |\n",
      "|    n_updates        | 4612     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.509    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1224     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8295     |\n",
      "|    total_timesteps  | 18621    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 4630     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.507    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1228     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8321     |\n",
      "|    total_timesteps  | 18681    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00191  |\n",
      "|    n_updates        | 4645     |\n",
      "----------------------------------\n",
      "Num timesteps: 18713\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 14.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.505    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1232     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8348     |\n",
      "|    total_timesteps  | 18740    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00945  |\n",
      "|    n_updates        | 4659     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.504    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1236     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8375     |\n",
      "|    total_timesteps  | 18803    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00287  |\n",
      "|    n_updates        | 4675     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.502    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1240     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8400     |\n",
      "|    total_timesteps  | 18861    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0168   |\n",
      "|    n_updates        | 4690     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.501    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1244     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8426     |\n",
      "|    total_timesteps  | 18927    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00444  |\n",
      "|    n_updates        | 4706     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.499    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1248     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8451     |\n",
      "|    total_timesteps  | 18986    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000124 |\n",
      "|    n_updates        | 4721     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.497    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1252     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8478     |\n",
      "|    total_timesteps  | 19046    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000835 |\n",
      "|    n_updates        | 4736     |\n",
      "----------------------------------\n",
      "Num timesteps: 19073\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 14.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.496    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1256     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8503     |\n",
      "|    total_timesteps  | 19108    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00188  |\n",
      "|    n_updates        | 4751     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.494    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1260     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8531     |\n",
      "|    total_timesteps  | 19174    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00535  |\n",
      "|    n_updates        | 4768     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.492    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1264     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8556     |\n",
      "|    total_timesteps  | 19237    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00583  |\n",
      "|    n_updates        | 4784     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.491    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1268     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8582     |\n",
      "|    total_timesteps  | 19295    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00525  |\n",
      "|    n_updates        | 4798     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.489    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1272     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8608     |\n",
      "|    total_timesteps  | 19362    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00578  |\n",
      "|    n_updates        | 4815     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.488    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1276     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8635     |\n",
      "|    total_timesteps  | 19419    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00399  |\n",
      "|    n_updates        | 4829     |\n",
      "----------------------------------\n",
      "Num timesteps: 19433\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.486    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1280     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8661     |\n",
      "|    total_timesteps  | 19480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0104   |\n",
      "|    n_updates        | 4844     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.485    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1284     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8687     |\n",
      "|    total_timesteps  | 19534    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00107  |\n",
      "|    n_updates        | 4858     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.483    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1288     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8713     |\n",
      "|    total_timesteps  | 19594    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00534  |\n",
      "|    n_updates        | 4873     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.481    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1292     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8739     |\n",
      "|    total_timesteps  | 19655    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00143  |\n",
      "|    n_updates        | 4888     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.48     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1296     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8766     |\n",
      "|    total_timesteps  | 19720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00534  |\n",
      "|    n_updates        | 4904     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.478    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1300     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8791     |\n",
      "|    total_timesteps  | 19775    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00032  |\n",
      "|    n_updates        | 4918     |\n",
      "----------------------------------\n",
      "Num timesteps: 19793\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.477    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1304     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8817     |\n",
      "|    total_timesteps  | 19829    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00372  |\n",
      "|    n_updates        | 4932     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.475    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1308     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8842     |\n",
      "|    total_timesteps  | 19880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00251  |\n",
      "|    n_updates        | 4944     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.474    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1312     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8869     |\n",
      "|    total_timesteps  | 19946    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000465 |\n",
      "|    n_updates        | 4961     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.472    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1316     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8894     |\n",
      "|    total_timesteps  | 20007    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0475   |\n",
      "|    n_updates        | 4976     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.47     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1320     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8924     |\n",
      "|    total_timesteps  | 20073    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0663   |\n",
      "|    n_updates        | 4993     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.469    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1324     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8950     |\n",
      "|    total_timesteps  | 20126    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0456   |\n",
      "|    n_updates        | 5006     |\n",
      "----------------------------------\n",
      "Num timesteps: 20153\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 14.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.467    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1328     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 8976     |\n",
      "|    total_timesteps  | 20188    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.025    |\n",
      "|    n_updates        | 5021     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.466    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1332     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9004     |\n",
      "|    total_timesteps  | 20254    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00589  |\n",
      "|    n_updates        | 5038     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.464    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1336     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9030     |\n",
      "|    total_timesteps  | 20319    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 5054     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.462    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1340     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9057     |\n",
      "|    total_timesteps  | 20392    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0326   |\n",
      "|    n_updates        | 5072     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.46     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1344     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9084     |\n",
      "|    total_timesteps  | 20460    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0189   |\n",
      "|    n_updates        | 5089     |\n",
      "----------------------------------\n",
      "Num timesteps: 20513\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.458    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1348     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9112     |\n",
      "|    total_timesteps  | 20526    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0441   |\n",
      "|    n_updates        | 5106     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.457    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1352     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9137     |\n",
      "|    total_timesteps  | 20581    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 5120     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.455    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1356     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9170     |\n",
      "|    total_timesteps  | 20666    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0154   |\n",
      "|    n_updates        | 5141     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.452    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1360     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9204     |\n",
      "|    total_timesteps  | 20751    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00125  |\n",
      "|    n_updates        | 5162     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.1     |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.45     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1364     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9238     |\n",
      "|    total_timesteps  | 20852    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00959  |\n",
      "|    n_updates        | 5187     |\n",
      "----------------------------------\n",
      "Num timesteps: 20873\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.5     |\n",
      "|    ep_rew_mean      | 16.5     |\n",
      "|    exploration_rate | 0.447    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1368     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9268     |\n",
      "|    total_timesteps  | 20948    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0136   |\n",
      "|    n_updates        | 5211     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | 16.6     |\n",
      "|    exploration_rate | 0.445    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1372     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9296     |\n",
      "|    total_timesteps  | 21026    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0369   |\n",
      "|    n_updates        | 5231     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.443    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1376     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9323     |\n",
      "|    total_timesteps  | 21104    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0075   |\n",
      "|    n_updates        | 5250     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17       |\n",
      "|    ep_rew_mean      | 17       |\n",
      "|    exploration_rate | 0.441    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1380     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9349     |\n",
      "|    total_timesteps  | 21181    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0157   |\n",
      "|    n_updates        | 5270     |\n",
      "----------------------------------\n",
      "Num timesteps: 21233\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.57\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.439    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1384     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9378     |\n",
      "|    total_timesteps  | 21251    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 5287     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.438    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1388     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9403     |\n",
      "|    total_timesteps  | 21313    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0341   |\n",
      "|    n_updates        | 5303     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.3     |\n",
      "|    ep_rew_mean      | 17.3     |\n",
      "|    exploration_rate | 0.436    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1392     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9432     |\n",
      "|    total_timesteps  | 21382    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 5320     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.434    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1396     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9458     |\n",
      "|    total_timesteps  | 21445    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0556   |\n",
      "|    n_updates        | 5336     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.8     |\n",
      "|    ep_rew_mean      | 17.8     |\n",
      "|    exploration_rate | 0.431    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1400     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9494     |\n",
      "|    total_timesteps  | 21550    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0096   |\n",
      "|    n_updates        | 5362     |\n",
      "----------------------------------\n",
      "Num timesteps: 21593\n",
      "Best mean reward: 15.67 - Last mean reward per episode: 15.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 18.1     |\n",
      "|    exploration_rate | 0.429    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1404     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9525     |\n",
      "|    total_timesteps  | 21641    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0218   |\n",
      "|    n_updates        | 5385     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.427    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1408     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9555     |\n",
      "|    total_timesteps  | 21722    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 5405     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.425    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1412     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9584     |\n",
      "|    total_timesteps  | 21792    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0088   |\n",
      "|    n_updates        | 5422     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.423    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1416     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9610     |\n",
      "|    total_timesteps  | 21855    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0036   |\n",
      "|    n_updates        | 5438     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.421    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1420     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9640     |\n",
      "|    total_timesteps  | 21935    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0757   |\n",
      "|    n_updates        | 5458     |\n",
      "----------------------------------\n",
      "Num timesteps: 21953\n",
      "Best mean reward: 15.82 - Last mean reward per episode: 16.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.419    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1424     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9669     |\n",
      "|    total_timesteps  | 22010    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 5477     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19       |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.417    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1428     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9699     |\n",
      "|    total_timesteps  | 22088    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0045   |\n",
      "|    n_updates        | 5496     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.415    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1432     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9728     |\n",
      "|    total_timesteps  | 22170    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00997  |\n",
      "|    n_updates        | 5517     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.413    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1436     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9761     |\n",
      "|    total_timesteps  | 22259    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0054   |\n",
      "|    n_updates        | 5539     |\n",
      "----------------------------------\n",
      "Num timesteps: 22313\n",
      "Best mean reward: 16.04 - Last mean reward per episode: 16.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | 19.5     |\n",
      "|    exploration_rate | 0.41     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1440     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9791     |\n",
      "|    total_timesteps  | 22342    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0241   |\n",
      "|    n_updates        | 5560     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | 19.6     |\n",
      "|    exploration_rate | 0.408    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1444     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9820     |\n",
      "|    total_timesteps  | 22416    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00202  |\n",
      "|    n_updates        | 5578     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.406    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1448     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9851     |\n",
      "|    total_timesteps  | 22498    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00945  |\n",
      "|    n_updates        | 5599     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.404    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1452     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9880     |\n",
      "|    total_timesteps  | 22575    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0241   |\n",
      "|    n_updates        | 5618     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.402    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1456     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9906     |\n",
      "|    total_timesteps  | 22644    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 5635     |\n",
      "----------------------------------\n",
      "Num timesteps: 22673\n",
      "Best mean reward: 16.36 - Last mean reward per episode: 16.53\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | 19.6     |\n",
      "|    exploration_rate | 0.401    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1460     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9932     |\n",
      "|    total_timesteps  | 22713    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0654   |\n",
      "|    n_updates        | 5653     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.399    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1464     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 9961     |\n",
      "|    total_timesteps  | 22788    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00588  |\n",
      "|    n_updates        | 5671     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.396    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1468     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10001    |\n",
      "|    total_timesteps  | 22893    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00215  |\n",
      "|    n_updates        | 5698     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | 19.3     |\n",
      "|    exploration_rate | 0.394    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1472     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10027    |\n",
      "|    total_timesteps  | 22960    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00142  |\n",
      "|    n_updates        | 5714     |\n",
      "----------------------------------\n",
      "Num timesteps: 23033\n",
      "Best mean reward: 16.53 - Last mean reward per episode: 16.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.391    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1476     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10067    |\n",
      "|    total_timesteps  | 23077    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0245   |\n",
      "|    n_updates        | 5744     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.389    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1480     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10096    |\n",
      "|    total_timesteps  | 23152    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00932  |\n",
      "|    n_updates        | 5762     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.387    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1484     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10126    |\n",
      "|    total_timesteps  | 23228    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0247   |\n",
      "|    n_updates        | 5781     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.385    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1488     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10156    |\n",
      "|    total_timesteps  | 23313    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00699  |\n",
      "|    n_updates        | 5803     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.383    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1492     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10184    |\n",
      "|    total_timesteps  | 23383    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00848  |\n",
      "|    n_updates        | 5820     |\n",
      "----------------------------------\n",
      "Num timesteps: 23393\n",
      "Best mean reward: 16.84 - Last mean reward per episode: 17.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.381    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1496     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10214    |\n",
      "|    total_timesteps  | 23467    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00413  |\n",
      "|    n_updates        | 5841     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.378    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1500     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10252    |\n",
      "|    total_timesteps  | 23569    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0278   |\n",
      "|    n_updates        | 5867     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.376    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1504     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10281    |\n",
      "|    total_timesteps  | 23633    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 5883     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.374    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1508     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10310    |\n",
      "|    total_timesteps  | 23710    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 5902     |\n",
      "----------------------------------\n",
      "Num timesteps: 23753\n",
      "Best mean reward: 17.08 - Last mean reward per episode: 17.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.372    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1512     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10340    |\n",
      "|    total_timesteps  | 23807    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0096   |\n",
      "|    n_updates        | 5926     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.37     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1516     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10368    |\n",
      "|    total_timesteps  | 23871    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0085   |\n",
      "|    n_updates        | 5942     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.368    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1520     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10403    |\n",
      "|    total_timesteps  | 23968    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0645   |\n",
      "|    n_updates        | 5966     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.365    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1524     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10433    |\n",
      "|    total_timesteps  | 24045    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 5986     |\n",
      "----------------------------------\n",
      "Num timesteps: 24113\n",
      "Best mean reward: 17.32 - Last mean reward per episode: 17.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 20.5     |\n",
      "|    exploration_rate | 0.363    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1528     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10465    |\n",
      "|    total_timesteps  | 24134    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0201   |\n",
      "|    n_updates        | 6008     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 20.5     |\n",
      "|    exploration_rate | 0.361    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1532     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10495    |\n",
      "|    total_timesteps  | 24217    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00203  |\n",
      "|    n_updates        | 6029     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.359    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1536     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10521    |\n",
      "|    total_timesteps  | 24296    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00278  |\n",
      "|    n_updates        | 6048     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.356    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1540     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10559    |\n",
      "|    total_timesteps  | 24404    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00293  |\n",
      "|    n_updates        | 6075     |\n",
      "----------------------------------\n",
      "Num timesteps: 24473\n",
      "Best mean reward: 17.67 - Last mean reward per episode: 17.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.354    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1544     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10589    |\n",
      "|    total_timesteps  | 24492    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0102   |\n",
      "|    n_updates        | 6097     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.351    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1548     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10618    |\n",
      "|    total_timesteps  | 24586    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0182   |\n",
      "|    n_updates        | 6121     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 21       |\n",
      "|    exploration_rate | 0.349    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1552     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10652    |\n",
      "|    total_timesteps  | 24677    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00866  |\n",
      "|    n_updates        | 6144     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 21       |\n",
      "|    exploration_rate | 0.347    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1556     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10680    |\n",
      "|    total_timesteps  | 24746    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00747  |\n",
      "|    n_updates        | 6161     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.345    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1560     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10708    |\n",
      "|    total_timesteps  | 24823    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0206   |\n",
      "|    n_updates        | 6180     |\n",
      "----------------------------------\n",
      "Num timesteps: 24833\n",
      "Best mean reward: 17.95 - Last mean reward per episode: 18.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.343    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1564     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10738    |\n",
      "|    total_timesteps  | 24913    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00379  |\n",
      "|    n_updates        | 6203     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 21       |\n",
      "|    exploration_rate | 0.341    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1568     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10764    |\n",
      "|    total_timesteps  | 24989    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0295   |\n",
      "|    n_updates        | 6222     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.338    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1572     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10801    |\n",
      "|    total_timesteps  | 25096    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00934  |\n",
      "|    n_updates        | 6248     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 21       |\n",
      "|    exploration_rate | 0.336    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1576     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10831    |\n",
      "|    total_timesteps  | 25174    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00131  |\n",
      "|    n_updates        | 6268     |\n",
      "----------------------------------\n",
      "Num timesteps: 25193\n",
      "Best mean reward: 18.20 - Last mean reward per episode: 18.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.333    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1580     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10863    |\n",
      "|    total_timesteps  | 25259    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00291  |\n",
      "|    n_updates        | 6289     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.331    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1584     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10897    |\n",
      "|    total_timesteps  | 25364    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00412  |\n",
      "|    n_updates        | 6315     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.329    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1588     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10923    |\n",
      "|    total_timesteps  | 25443    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0344   |\n",
      "|    n_updates        | 6335     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.326    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1592     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10958    |\n",
      "|    total_timesteps  | 25532    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0139   |\n",
      "|    n_updates        | 6357     |\n",
      "----------------------------------\n",
      "Num timesteps: 25553\n",
      "Best mean reward: 18.55 - Last mean reward per episode: 18.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.324    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1596     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 10998    |\n",
      "|    total_timesteps  | 25631    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00501  |\n",
      "|    n_updates        | 6382     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.321    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1600     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11026    |\n",
      "|    total_timesteps  | 25715    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00141  |\n",
      "|    n_updates        | 6403     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.318    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1604     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11065    |\n",
      "|    total_timesteps  | 25833    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0295   |\n",
      "|    n_updates        | 6433     |\n",
      "----------------------------------\n",
      "Num timesteps: 25913\n",
      "Best mean reward: 18.85 - Last mean reward per episode: 19.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.315    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1608     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11103    |\n",
      "|    total_timesteps  | 25956    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0291   |\n",
      "|    n_updates        | 6463     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.312    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1612     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11140    |\n",
      "|    total_timesteps  | 26069    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00579  |\n",
      "|    n_updates        | 6492     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.31     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1616     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11172    |\n",
      "|    total_timesteps  | 26156    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00848  |\n",
      "|    n_updates        | 6513     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.307    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1620     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11208    |\n",
      "|    total_timesteps  | 26254    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00719  |\n",
      "|    n_updates        | 6538     |\n",
      "----------------------------------\n",
      "Num timesteps: 26273\n",
      "Best mean reward: 19.26 - Last mean reward per episode: 19.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.305    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1624     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11236    |\n",
      "|    total_timesteps  | 26322    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00343  |\n",
      "|    n_updates        | 6555     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.303    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1628     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11267    |\n",
      "|    total_timesteps  | 26415    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0017   |\n",
      "|    n_updates        | 6578     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.3      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1632     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11298    |\n",
      "|    total_timesteps  | 26508    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00199  |\n",
      "|    n_updates        | 6601     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.298    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1636     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11326    |\n",
      "|    total_timesteps  | 26587    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0162   |\n",
      "|    n_updates        | 6621     |\n",
      "----------------------------------\n",
      "Num timesteps: 26633\n",
      "Best mean reward: 19.67 - Last mean reward per episode: 19.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.296    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1640     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11354    |\n",
      "|    total_timesteps  | 26671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000922 |\n",
      "|    n_updates        | 6642     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.293    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1644     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11393    |\n",
      "|    total_timesteps  | 26799    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00969  |\n",
      "|    n_updates        | 6674     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.29     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1648     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11428    |\n",
      "|    total_timesteps  | 26913    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0484   |\n",
      "|    n_updates        | 6703     |\n",
      "----------------------------------\n",
      "Num timesteps: 26993\n",
      "Best mean reward: 19.95 - Last mean reward per episode: 20.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.286    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1652     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11472    |\n",
      "|    total_timesteps  | 27044    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00157  |\n",
      "|    n_updates        | 6735     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.284    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1656     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11504    |\n",
      "|    total_timesteps  | 27137    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0174   |\n",
      "|    n_updates        | 6759     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24       |\n",
      "|    ep_rew_mean      | 24       |\n",
      "|    exploration_rate | 0.282    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1660     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11536    |\n",
      "|    total_timesteps  | 27221    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0276   |\n",
      "|    n_updates        | 6780     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24       |\n",
      "|    ep_rew_mean      | 24       |\n",
      "|    exploration_rate | 0.279    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1664     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11567    |\n",
      "|    total_timesteps  | 27313    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00251  |\n",
      "|    n_updates        | 6803     |\n",
      "----------------------------------\n",
      "Num timesteps: 27353\n",
      "Best mean reward: 20.43 - Last mean reward per episode: 20.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.277    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1668     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11592    |\n",
      "|    total_timesteps  | 27397    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00226  |\n",
      "|    n_updates        | 6824     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.275    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1672     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11621    |\n",
      "|    total_timesteps  | 27482    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00961  |\n",
      "|    n_updates        | 6845     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | 24.4     |\n",
      "|    exploration_rate | 0.271    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1676     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11664    |\n",
      "|    total_timesteps  | 27614    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00903  |\n",
      "|    n_updates        | 6878     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | 24.4     |\n",
      "|    exploration_rate | 0.269    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1680     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11693    |\n",
      "|    total_timesteps  | 27701    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00519  |\n",
      "|    n_updates        | 6900     |\n",
      "----------------------------------\n",
      "Num timesteps: 27713\n",
      "Best mean reward: 20.81 - Last mean reward per episode: 21.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 24.3     |\n",
      "|    exploration_rate | 0.267    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1684     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11725    |\n",
      "|    total_timesteps  | 27795    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00445  |\n",
      "|    n_updates        | 6923     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 24.3     |\n",
      "|    exploration_rate | 0.264    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1688     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11750    |\n",
      "|    total_timesteps  | 27872    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0329   |\n",
      "|    n_updates        | 6942     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 24.2     |\n",
      "|    exploration_rate | 0.262    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1692     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11779    |\n",
      "|    total_timesteps  | 27952    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00164  |\n",
      "|    n_updates        | 6962     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 24.2     |\n",
      "|    exploration_rate | 0.26     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1696     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11810    |\n",
      "|    total_timesteps  | 28052    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00212  |\n",
      "|    n_updates        | 6987     |\n",
      "----------------------------------\n",
      "Num timesteps: 28073\n",
      "Best mean reward: 21.19 - Last mean reward per episode: 21.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | 24.4     |\n",
      "|    exploration_rate | 0.257    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1700     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11844    |\n",
      "|    total_timesteps  | 28158    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00539  |\n",
      "|    n_updates        | 7014     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | 24.4     |\n",
      "|    exploration_rate | 0.254    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1704     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11880    |\n",
      "|    total_timesteps  | 28272    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00556  |\n",
      "|    n_updates        | 7042     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | 24.4     |\n",
      "|    exploration_rate | 0.251    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1708     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11919    |\n",
      "|    total_timesteps  | 28401    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000779 |\n",
      "|    n_updates        | 7075     |\n",
      "----------------------------------\n",
      "Num timesteps: 28433\n",
      "Best mean reward: 21.48 - Last mean reward per episode: 21.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 24.2     |\n",
      "|    exploration_rate | 0.248    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1712     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11950    |\n",
      "|    total_timesteps  | 28494    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 7098     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 24.5     |\n",
      "|    exploration_rate | 0.245    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1716     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 11985    |\n",
      "|    total_timesteps  | 28609    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000695 |\n",
      "|    n_updates        | 7127     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 24.9     |\n",
      "|    exploration_rate | 0.241    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1720     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12026    |\n",
      "|    total_timesteps  | 28748    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00515  |\n",
      "|    n_updates        | 7161     |\n",
      "----------------------------------\n",
      "Num timesteps: 28793\n",
      "Best mean reward: 21.91 - Last mean reward per episode: 22.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.239    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1724     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12055    |\n",
      "|    total_timesteps  | 28826    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0021   |\n",
      "|    n_updates        | 7181     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.2     |\n",
      "|    ep_rew_mean      | 25.2     |\n",
      "|    exploration_rate | 0.236    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1728     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12089    |\n",
      "|    total_timesteps  | 28934    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00593  |\n",
      "|    n_updates        | 7208     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.8     |\n",
      "|    ep_rew_mean      | 25.8     |\n",
      "|    exploration_rate | 0.232    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1732     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12138    |\n",
      "|    total_timesteps  | 29090    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0144   |\n",
      "|    n_updates        | 7247     |\n",
      "----------------------------------\n",
      "Num timesteps: 29153\n",
      "Best mean reward: 22.16 - Last mean reward per episode: 22.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.4     |\n",
      "|    ep_rew_mean      | 26.4     |\n",
      "|    exploration_rate | 0.229    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1736     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12184    |\n",
      "|    total_timesteps  | 29231    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00247  |\n",
      "|    n_updates        | 7282     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27       |\n",
      "|    ep_rew_mean      | 27       |\n",
      "|    exploration_rate | 0.225    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1740     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12227    |\n",
      "|    total_timesteps  | 29368    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000889 |\n",
      "|    n_updates        | 7316     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.8     |\n",
      "|    ep_rew_mean      | 26.8     |\n",
      "|    exploration_rate | 0.222    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1744     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12265    |\n",
      "|    total_timesteps  | 29481    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0182   |\n",
      "|    n_updates        | 7345     |\n",
      "----------------------------------\n",
      "Num timesteps: 29513\n",
      "Best mean reward: 22.40 - Last mean reward per episode: 22.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.1     |\n",
      "|    ep_rew_mean      | 27.1     |\n",
      "|    exploration_rate | 0.218    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1748     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12313    |\n",
      "|    total_timesteps  | 29626    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00336  |\n",
      "|    n_updates        | 7381     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.8     |\n",
      "|    ep_rew_mean      | 26.8     |\n",
      "|    exploration_rate | 0.216    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1752     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12341    |\n",
      "|    total_timesteps  | 29720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000744 |\n",
      "|    n_updates        | 7404     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.1     |\n",
      "|    ep_rew_mean      | 27.1     |\n",
      "|    exploration_rate | 0.212    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1756     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12379    |\n",
      "|    total_timesteps  | 29845    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.041    |\n",
      "|    n_updates        | 7436     |\n",
      "----------------------------------\n",
      "Num timesteps: 29873\n",
      "Best mean reward: 22.86 - Last mean reward per episode: 23.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.3     |\n",
      "|    ep_rew_mean      | 27.3     |\n",
      "|    exploration_rate | 0.21     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1760     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12415    |\n",
      "|    total_timesteps  | 29951    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00925  |\n",
      "|    n_updates        | 7462     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.5     |\n",
      "|    ep_rew_mean      | 27.5     |\n",
      "|    exploration_rate | 0.207    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1764     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12453    |\n",
      "|    total_timesteps  | 30060    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0556   |\n",
      "|    n_updates        | 7489     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28       |\n",
      "|    ep_rew_mean      | 28       |\n",
      "|    exploration_rate | 0.203    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1768     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12501    |\n",
      "|    total_timesteps  | 30200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00931  |\n",
      "|    n_updates        | 7524     |\n",
      "----------------------------------\n",
      "Num timesteps: 30233\n",
      "Best mean reward: 23.32 - Last mean reward per episode: 23.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.3     |\n",
      "|    ep_rew_mean      | 28.3     |\n",
      "|    exploration_rate | 0.2      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1772     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12545    |\n",
      "|    total_timesteps  | 30316    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00547  |\n",
      "|    n_updates        | 7553     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.2     |\n",
      "|    ep_rew_mean      | 28.2     |\n",
      "|    exploration_rate | 0.197    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1776     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12583    |\n",
      "|    total_timesteps  | 30433    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0368   |\n",
      "|    n_updates        | 7583     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.1     |\n",
      "|    ep_rew_mean      | 28.1     |\n",
      "|    exploration_rate | 0.195    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1780     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12616    |\n",
      "|    total_timesteps  | 30516    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00977  |\n",
      "|    n_updates        | 7603     |\n",
      "----------------------------------\n",
      "Num timesteps: 30593\n",
      "Best mean reward: 23.56 - Last mean reward per episode: 23.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.4     |\n",
      "|    ep_rew_mean      | 28.4     |\n",
      "|    exploration_rate | 0.192    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1784     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12658    |\n",
      "|    total_timesteps  | 30632    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00389  |\n",
      "|    n_updates        | 7632     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.5     |\n",
      "|    ep_rew_mean      | 28.5     |\n",
      "|    exploration_rate | 0.189    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1788     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12691    |\n",
      "|    total_timesteps  | 30721    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00208  |\n",
      "|    n_updates        | 7655     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.6     |\n",
      "|    ep_rew_mean      | 28.6     |\n",
      "|    exploration_rate | 0.187    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1792     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12726    |\n",
      "|    total_timesteps  | 30810    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0319   |\n",
      "|    n_updates        | 7677     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.4     |\n",
      "|    ep_rew_mean      | 28.4     |\n",
      "|    exploration_rate | 0.185    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1796     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12758    |\n",
      "|    total_timesteps  | 30889    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00898  |\n",
      "|    n_updates        | 7697     |\n",
      "----------------------------------\n",
      "Num timesteps: 30953\n",
      "Best mean reward: 23.88 - Last mean reward per episode: 24.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.7     |\n",
      "|    ep_rew_mean      | 28.7     |\n",
      "|    exploration_rate | 0.181    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1800     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12807    |\n",
      "|    total_timesteps  | 31027    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0483   |\n",
      "|    n_updates        | 7731     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.1     |\n",
      "|    ep_rew_mean      | 28.1     |\n",
      "|    exploration_rate | 0.18     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1804     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12835    |\n",
      "|    total_timesteps  | 31085    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0223   |\n",
      "|    n_updates        | 7746     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28       |\n",
      "|    ep_rew_mean      | 28       |\n",
      "|    exploration_rate | 0.177    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1808     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12880    |\n",
      "|    total_timesteps  | 31204    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00499  |\n",
      "|    n_updates        | 7775     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.1     |\n",
      "|    ep_rew_mean      | 28.1     |\n",
      "|    exploration_rate | 0.174    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1812     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12919    |\n",
      "|    total_timesteps  | 31307    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00635  |\n",
      "|    n_updates        | 7801     |\n",
      "----------------------------------\n",
      "Num timesteps: 31313\n",
      "Best mean reward: 24.00 - Last mean reward per episode: 24.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.4     |\n",
      "|    ep_rew_mean      | 28.4     |\n",
      "|    exploration_rate | 0.17     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1816     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 12964    |\n",
      "|    total_timesteps  | 31448    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0391   |\n",
      "|    n_updates        | 7836     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.2     |\n",
      "|    ep_rew_mean      | 28.2     |\n",
      "|    exploration_rate | 0.167    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1820     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13006    |\n",
      "|    total_timesteps  | 31571    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 7867     |\n",
      "----------------------------------\n",
      "Num timesteps: 31673\n",
      "Best mean reward: 24.26 - Last mean reward per episode: 24.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29.1     |\n",
      "|    ep_rew_mean      | 29.1     |\n",
      "|    exploration_rate | 0.162    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1824     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13068    |\n",
      "|    total_timesteps  | 31741    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0639   |\n",
      "|    n_updates        | 7910     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29.3     |\n",
      "|    ep_rew_mean      | 29.3     |\n",
      "|    exploration_rate | 0.159    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1828     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13112    |\n",
      "|    total_timesteps  | 31865    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000679 |\n",
      "|    n_updates        | 7941     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.9     |\n",
      "|    ep_rew_mean      | 28.9     |\n",
      "|    exploration_rate | 0.156    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1832     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13153    |\n",
      "|    total_timesteps  | 31977    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00671  |\n",
      "|    n_updates        | 7969     |\n",
      "----------------------------------\n",
      "Num timesteps: 32033\n",
      "Best mean reward: 24.71 - Last mean reward per episode: 24.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.6     |\n",
      "|    ep_rew_mean      | 28.6     |\n",
      "|    exploration_rate | 0.153    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1836     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13194    |\n",
      "|    total_timesteps  | 32091    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00386  |\n",
      "|    n_updates        | 7997     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.2     |\n",
      "|    ep_rew_mean      | 28.2     |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1840     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13232    |\n",
      "|    total_timesteps  | 32192    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0202   |\n",
      "|    n_updates        | 8022     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.6     |\n",
      "|    ep_rew_mean      | 28.6     |\n",
      "|    exploration_rate | 0.147    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1844     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13283    |\n",
      "|    total_timesteps  | 32341    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0204   |\n",
      "|    n_updates        | 8060     |\n",
      "----------------------------------\n",
      "Num timesteps: 32393\n",
      "Best mean reward: 24.97 - Last mean reward per episode: 25.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.5     |\n",
      "|    ep_rew_mean      | 28.5     |\n",
      "|    exploration_rate | 0.143    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1848     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13335    |\n",
      "|    total_timesteps  | 32480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00824  |\n",
      "|    n_updates        | 8094     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.8     |\n",
      "|    ep_rew_mean      | 28.8     |\n",
      "|    exploration_rate | 0.14     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1852     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13380    |\n",
      "|    total_timesteps  | 32600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 8124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.9     |\n",
      "|    ep_rew_mean      | 28.9     |\n",
      "|    exploration_rate | 0.136    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1856     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13427    |\n",
      "|    total_timesteps  | 32737    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0371   |\n",
      "|    n_updates        | 8159     |\n",
      "----------------------------------\n",
      "Num timesteps: 32753\n",
      "Best mean reward: 25.29 - Last mean reward per episode: 25.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29.7     |\n",
      "|    ep_rew_mean      | 29.7     |\n",
      "|    exploration_rate | 0.131    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1860     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13489    |\n",
      "|    total_timesteps  | 32917    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00926  |\n",
      "|    n_updates        | 8204     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 30       |\n",
      "|    ep_rew_mean      | 30       |\n",
      "|    exploration_rate | 0.128    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1864     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13537    |\n",
      "|    total_timesteps  | 33059    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0413   |\n",
      "|    n_updates        | 8239     |\n",
      "----------------------------------\n",
      "Num timesteps: 33113\n",
      "Best mean reward: 25.75 - Last mean reward per episode: 26.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 30.3     |\n",
      "|    ep_rew_mean      | 30.3     |\n",
      "|    exploration_rate | 0.123    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1868     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13594    |\n",
      "|    total_timesteps  | 33231    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00615  |\n",
      "|    n_updates        | 8282     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 31.1     |\n",
      "|    ep_rew_mean      | 31.1     |\n",
      "|    exploration_rate | 0.118    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1872     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13659    |\n",
      "|    total_timesteps  | 33430    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0139   |\n",
      "|    n_updates        | 8332     |\n",
      "----------------------------------\n",
      "Num timesteps: 33473\n",
      "Best mean reward: 26.18 - Last mean reward per episode: 26.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 31.2     |\n",
      "|    ep_rew_mean      | 31.2     |\n",
      "|    exploration_rate | 0.115    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1876     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13706    |\n",
      "|    total_timesteps  | 33549    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00464  |\n",
      "|    n_updates        | 8362     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 31.4     |\n",
      "|    ep_rew_mean      | 31.4     |\n",
      "|    exploration_rate | 0.112    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1880     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13738    |\n",
      "|    total_timesteps  | 33655    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00461  |\n",
      "|    n_updates        | 8388     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 31.1     |\n",
      "|    ep_rew_mean      | 31.1     |\n",
      "|    exploration_rate | 0.11     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1884     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13774    |\n",
      "|    total_timesteps  | 33745    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00969  |\n",
      "|    n_updates        | 8411     |\n",
      "----------------------------------\n",
      "Num timesteps: 33833\n",
      "Best mean reward: 26.79 - Last mean reward per episode: 27.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 32       |\n",
      "|    ep_rew_mean      | 32       |\n",
      "|    exploration_rate | 0.105    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1888     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13838    |\n",
      "|    total_timesteps  | 33925    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00507  |\n",
      "|    n_updates        | 8456     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 32.7     |\n",
      "|    ep_rew_mean      | 32.7     |\n",
      "|    exploration_rate | 0.101    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1892     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13891    |\n",
      "|    total_timesteps  | 34078    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000833 |\n",
      "|    n_updates        | 8494     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 32.9     |\n",
      "|    ep_rew_mean      | 32.9     |\n",
      "|    exploration_rate | 0.0981   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1896     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13929    |\n",
      "|    total_timesteps  | 34179    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00862  |\n",
      "|    n_updates        | 8519     |\n",
      "----------------------------------\n",
      "Num timesteps: 34193\n",
      "Best mean reward: 27.04 - Last mean reward per episode: 27.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 32.5     |\n",
      "|    ep_rew_mean      | 32.5     |\n",
      "|    exploration_rate | 0.0955   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1900     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 13965    |\n",
      "|    total_timesteps  | 34276    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00301  |\n",
      "|    n_updates        | 8543     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 33.2     |\n",
      "|    ep_rew_mean      | 33.2     |\n",
      "|    exploration_rate | 0.0922   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1904     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14006    |\n",
      "|    total_timesteps  | 34401    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00671  |\n",
      "|    n_updates        | 8575     |\n",
      "----------------------------------\n",
      "Num timesteps: 34553\n",
      "Best mean reward: 27.45 - Last mean reward per episode: 27.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 33.7     |\n",
      "|    ep_rew_mean      | 33.7     |\n",
      "|    exploration_rate | 0.0876   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1908     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14067    |\n",
      "|    total_timesteps  | 34574    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 8618     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 33.7     |\n",
      "|    ep_rew_mean      | 33.7     |\n",
      "|    exploration_rate | 0.0848   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1912     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14110    |\n",
      "|    total_timesteps  | 34681    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00447  |\n",
      "|    n_updates        | 8645     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 34.6     |\n",
      "|    ep_rew_mean      | 34.6     |\n",
      "|    exploration_rate | 0.0789   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1916     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14181    |\n",
      "|    total_timesteps  | 34904    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00561  |\n",
      "|    n_updates        | 8700     |\n",
      "----------------------------------\n",
      "Num timesteps: 34913\n",
      "Best mean reward: 27.72 - Last mean reward per episode: 28.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 36.1     |\n",
      "|    ep_rew_mean      | 36.1     |\n",
      "|    exploration_rate | 0.0715   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1920     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14266    |\n",
      "|    total_timesteps  | 35186    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0203   |\n",
      "|    n_updates        | 8771     |\n",
      "----------------------------------\n",
      "Num timesteps: 35273\n",
      "Best mean reward: 28.22 - Last mean reward per episode: 28.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 36       |\n",
      "|    ep_rew_mean      | 36       |\n",
      "|    exploration_rate | 0.0673   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1924     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14321    |\n",
      "|    total_timesteps  | 35344    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00508  |\n",
      "|    n_updates        | 8810     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 36.8     |\n",
      "|    ep_rew_mean      | 36.8     |\n",
      "|    exploration_rate | 0.0621   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1928     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14387    |\n",
      "|    total_timesteps  | 35541    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00302  |\n",
      "|    n_updates        | 8860     |\n",
      "----------------------------------\n",
      "Num timesteps: 35633\n",
      "Best mean reward: 28.82 - Last mean reward per episode: 29.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 37.6     |\n",
      "|    ep_rew_mean      | 37.6     |\n",
      "|    exploration_rate | 0.057    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1932     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14454    |\n",
      "|    total_timesteps  | 35733    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00385  |\n",
      "|    n_updates        | 8908     |\n",
      "----------------------------------\n",
      "Num timesteps: 35993\n",
      "Best mean reward: 29.46 - Last mean reward per episode: 29.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 39.5     |\n",
      "|    ep_rew_mean      | 39.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1936     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14551    |\n",
      "|    total_timesteps  | 36040    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.022    |\n",
      "|    n_updates        | 8984     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.6     |\n",
      "|    ep_rew_mean      | 40.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1940     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14616    |\n",
      "|    total_timesteps  | 36248    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00569  |\n",
      "|    n_updates        | 9036     |\n",
      "----------------------------------\n",
      "Num timesteps: 36353\n",
      "Best mean reward: 29.94 - Last mean reward per episode: 30.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.3     |\n",
      "|    ep_rew_mean      | 41.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1944     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14692    |\n",
      "|    total_timesteps  | 36475    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0268   |\n",
      "|    n_updates        | 9093     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41       |\n",
      "|    ep_rew_mean      | 41       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1948     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14734    |\n",
      "|    total_timesteps  | 36585    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00757  |\n",
      "|    n_updates        | 9121     |\n",
      "----------------------------------\n",
      "Num timesteps: 36713\n",
      "Best mean reward: 30.58 - Last mean reward per episode: 31.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.7     |\n",
      "|    ep_rew_mean      | 41.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1952     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14792    |\n",
      "|    total_timesteps  | 36770    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0058   |\n",
      "|    n_updates        | 9167     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.2     |\n",
      "|    ep_rew_mean      | 42.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1956     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14852    |\n",
      "|    total_timesteps  | 36954    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00909  |\n",
      "|    n_updates        | 9213     |\n",
      "----------------------------------\n",
      "Num timesteps: 37073\n",
      "Best mean reward: 31.10 - Last mean reward per episode: 31.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.8     |\n",
      "|    ep_rew_mean      | 42.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1960     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 14937    |\n",
      "|    total_timesteps  | 37197    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00362  |\n",
      "|    n_updates        | 9274     |\n",
      "----------------------------------\n",
      "Num timesteps: 37433\n",
      "Best mean reward: 31.66 - Last mean reward per episode: 32.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.8     |\n",
      "|    ep_rew_mean      | 43.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1964     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15012    |\n",
      "|    total_timesteps  | 37439    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000803 |\n",
      "|    n_updates        | 9334     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.7     |\n",
      "|    ep_rew_mean      | 43.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1968     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15068    |\n",
      "|    total_timesteps  | 37597    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0159   |\n",
      "|    n_updates        | 9374     |\n",
      "----------------------------------\n",
      "Num timesteps: 37793\n",
      "Best mean reward: 32.12 - Last mean reward per episode: 32.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1972     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15136    |\n",
      "|    total_timesteps  | 37795    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0775   |\n",
      "|    n_updates        | 9423     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1976     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15194    |\n",
      "|    total_timesteps  | 37950    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0158   |\n",
      "|    n_updates        | 9462     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.2     |\n",
      "|    ep_rew_mean      | 44.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1980     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15243    |\n",
      "|    total_timesteps  | 38077    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0277   |\n",
      "|    n_updates        | 9494     |\n",
      "----------------------------------\n",
      "Num timesteps: 38153\n",
      "Best mean reward: 32.50 - Last mean reward per episode: 32.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45       |\n",
      "|    ep_rew_mean      | 45       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1984     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15299    |\n",
      "|    total_timesteps  | 38244    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00586  |\n",
      "|    n_updates        | 9535     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.5     |\n",
      "|    ep_rew_mean      | 45.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1988     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15378    |\n",
      "|    total_timesteps  | 38473    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0184   |\n",
      "|    n_updates        | 9593     |\n",
      "----------------------------------\n",
      "Num timesteps: 38513\n",
      "Best mean reward: 32.91 - Last mean reward per episode: 33.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46       |\n",
      "|    ep_rew_mean      | 46       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1992     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15445    |\n",
      "|    total_timesteps  | 38679    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00231  |\n",
      "|    n_updates        | 9644     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.8     |\n",
      "|    ep_rew_mean      | 46.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1996     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15500    |\n",
      "|    total_timesteps  | 38854    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0671   |\n",
      "|    n_updates        | 9688     |\n",
      "----------------------------------\n",
      "Num timesteps: 38873\n",
      "Best mean reward: 33.51 - Last mean reward per episode: 34.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2000     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15560    |\n",
      "|    total_timesteps  | 39018    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00945  |\n",
      "|    n_updates        | 9729     |\n",
      "----------------------------------\n",
      "Num timesteps: 39233\n",
      "Best mean reward: 34.08 - Last mean reward per episode: 34.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49       |\n",
      "|    ep_rew_mean      | 49       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2004     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15647    |\n",
      "|    total_timesteps  | 39302    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0195   |\n",
      "|    n_updates        | 9800     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.6     |\n",
      "|    ep_rew_mean      | 49.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2008     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15730    |\n",
      "|    total_timesteps  | 39536    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00935  |\n",
      "|    n_updates        | 9858     |\n",
      "----------------------------------\n",
      "Num timesteps: 39593\n",
      "Best mean reward: 34.51 - Last mean reward per episode: 35.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50       |\n",
      "|    ep_rew_mean      | 50       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2012     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15780    |\n",
      "|    total_timesteps  | 39682    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00295  |\n",
      "|    n_updates        | 9895     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.2     |\n",
      "|    ep_rew_mean      | 49.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2016     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15832    |\n",
      "|    total_timesteps  | 39829    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0045   |\n",
      "|    n_updates        | 9932     |\n",
      "----------------------------------\n",
      "Num timesteps: 39953\n",
      "Best mean reward: 35.12 - Last mean reward per episode: 35.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.9     |\n",
      "|    ep_rew_mean      | 48.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2020     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15912    |\n",
      "|    total_timesteps  | 40075    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0326   |\n",
      "|    n_updates        | 9993     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.5     |\n",
      "|    ep_rew_mean      | 49.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2024     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 15982    |\n",
      "|    total_timesteps  | 40296    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0426   |\n",
      "|    n_updates        | 10048    |\n",
      "----------------------------------\n",
      "Num timesteps: 40313\n",
      "Best mean reward: 35.45 - Last mean reward per episode: 36.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.4     |\n",
      "|    ep_rew_mean      | 49.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2028     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16050    |\n",
      "|    total_timesteps  | 40485    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0366   |\n",
      "|    n_updates        | 10096    |\n",
      "----------------------------------\n",
      "Num timesteps: 40673\n",
      "Best mean reward: 36.04 - Last mean reward per episode: 36.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.2     |\n",
      "|    ep_rew_mean      | 50.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2032     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16135    |\n",
      "|    total_timesteps  | 40753    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0315   |\n",
      "|    n_updates        | 10163    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.8     |\n",
      "|    ep_rew_mean      | 48.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2036     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16191    |\n",
      "|    total_timesteps  | 40915    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0458   |\n",
      "|    n_updates        | 10203    |\n",
      "----------------------------------\n",
      "Num timesteps: 41033\n",
      "Best mean reward: 36.60 - Last mean reward per episode: 37.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.9     |\n",
      "|    ep_rew_mean      | 48.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2040     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16266    |\n",
      "|    total_timesteps  | 41141    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00503  |\n",
      "|    n_updates        | 10260    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.6     |\n",
      "|    ep_rew_mean      | 48.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2044     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16332    |\n",
      "|    total_timesteps  | 41340    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0139   |\n",
      "|    n_updates        | 10309    |\n",
      "----------------------------------\n",
      "Num timesteps: 41393\n",
      "Best mean reward: 37.08 - Last mean reward per episode: 37.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.7     |\n",
      "|    ep_rew_mean      | 48.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2048     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16374    |\n",
      "|    total_timesteps  | 41456    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00476  |\n",
      "|    n_updates        | 10338    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.9     |\n",
      "|    ep_rew_mean      | 48.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2052     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16442    |\n",
      "|    total_timesteps  | 41655    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0195   |\n",
      "|    n_updates        | 10388    |\n",
      "----------------------------------\n",
      "Num timesteps: 41753\n",
      "Best mean reward: 37.68 - Last mean reward per episode: 38.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.9     |\n",
      "|    ep_rew_mean      | 48.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2056     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16505    |\n",
      "|    total_timesteps  | 41840    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0332   |\n",
      "|    n_updates        | 10434    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.9     |\n",
      "|    ep_rew_mean      | 47.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2060     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16556    |\n",
      "|    total_timesteps  | 41982    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00768  |\n",
      "|    n_updates        | 10470    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.7     |\n",
      "|    ep_rew_mean      | 46.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2064     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16603    |\n",
      "|    total_timesteps  | 42107    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 10501    |\n",
      "----------------------------------\n",
      "Num timesteps: 42113\n",
      "Best mean reward: 38.08 - Last mean reward per episode: 38.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2068     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16675    |\n",
      "|    total_timesteps  | 42340    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00429  |\n",
      "|    n_updates        | 10559    |\n",
      "----------------------------------\n",
      "Num timesteps: 42473\n",
      "Best mean reward: 38.43 - Last mean reward per episode: 38.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.8     |\n",
      "|    ep_rew_mean      | 46.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2072     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16719    |\n",
      "|    total_timesteps  | 42479    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00939  |\n",
      "|    n_updates        | 10594    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2076     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16790    |\n",
      "|    total_timesteps  | 42690    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00415  |\n",
      "|    n_updates        | 10647    |\n",
      "----------------------------------\n",
      "Num timesteps: 42833\n",
      "Best mean reward: 38.85 - Last mean reward per episode: 39.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.5     |\n",
      "|    ep_rew_mean      | 48.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2080     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16870    |\n",
      "|    total_timesteps  | 42929    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.022    |\n",
      "|    n_updates        | 10707    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.7     |\n",
      "|    ep_rew_mean      | 48.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2084     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16930    |\n",
      "|    total_timesteps  | 43111    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00822  |\n",
      "|    n_updates        | 10752    |\n",
      "----------------------------------\n",
      "Num timesteps: 43193\n",
      "Best mean reward: 39.24 - Last mean reward per episode: 39.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48       |\n",
      "|    ep_rew_mean      | 48       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2088     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 16983    |\n",
      "|    total_timesteps  | 43273    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0169   |\n",
      "|    n_updates        | 10793    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.2     |\n",
      "|    ep_rew_mean      | 47.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2092     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17028    |\n",
      "|    total_timesteps  | 43395    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00911  |\n",
      "|    n_updates        | 10823    |\n",
      "----------------------------------\n",
      "Num timesteps: 43553\n",
      "Best mean reward: 39.69 - Last mean reward per episode: 39.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.8     |\n",
      "|    ep_rew_mean      | 47.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2096     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17108    |\n",
      "|    total_timesteps  | 43630    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0236   |\n",
      "|    n_updates        | 10882    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48       |\n",
      "|    ep_rew_mean      | 48       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2100     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17169    |\n",
      "|    total_timesteps  | 43817    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0712   |\n",
      "|    n_updates        | 10929    |\n",
      "----------------------------------\n",
      "Num timesteps: 43913\n",
      "Best mean reward: 39.82 - Last mean reward per episode: 40.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.9     |\n",
      "|    ep_rew_mean      | 46.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2104     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17230    |\n",
      "|    total_timesteps  | 43991    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.012    |\n",
      "|    n_updates        | 10972    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.6     |\n",
      "|    ep_rew_mean      | 46.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2108     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17296    |\n",
      "|    total_timesteps  | 44201    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0403   |\n",
      "|    n_updates        | 11025    |\n",
      "----------------------------------\n",
      "Num timesteps: 44273\n",
      "Best mean reward: 40.15 - Last mean reward per episode: 40.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48       |\n",
      "|    ep_rew_mean      | 48       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2112     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17389    |\n",
      "|    total_timesteps  | 44478    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0235   |\n",
      "|    n_updates        | 11094    |\n",
      "----------------------------------\n",
      "Num timesteps: 44633\n",
      "Best mean reward: 40.49 - Last mean reward per episode: 41.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.8     |\n",
      "|    ep_rew_mean      | 48.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2116     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17462    |\n",
      "|    total_timesteps  | 44713    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00393  |\n",
      "|    n_updates        | 11153    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.7     |\n",
      "|    ep_rew_mean      | 47.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2120     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17514    |\n",
      "|    total_timesteps  | 44848    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00829  |\n",
      "|    n_updates        | 11186    |\n",
      "----------------------------------\n",
      "Num timesteps: 44993\n",
      "Best mean reward: 41.16 - Last mean reward per episode: 41.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.5     |\n",
      "|    ep_rew_mean      | 47.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2124     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17584    |\n",
      "|    total_timesteps  | 45048    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00421  |\n",
      "|    n_updates        | 11236    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.9     |\n",
      "|    ep_rew_mean      | 46.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2128     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17624    |\n",
      "|    total_timesteps  | 45171    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00263  |\n",
      "|    n_updates        | 11267    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.8     |\n",
      "|    ep_rew_mean      | 45.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2132     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17678    |\n",
      "|    total_timesteps  | 45335    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0241   |\n",
      "|    n_updates        | 11308    |\n",
      "----------------------------------\n",
      "Num timesteps: 45353\n",
      "Best mean reward: 41.60 - Last mean reward per episode: 41.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.6     |\n",
      "|    ep_rew_mean      | 45.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2136     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17727    |\n",
      "|    total_timesteps  | 45471    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0196   |\n",
      "|    n_updates        | 11342    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.5     |\n",
      "|    ep_rew_mean      | 45.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2140     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17801    |\n",
      "|    total_timesteps  | 45690    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00463  |\n",
      "|    n_updates        | 11397    |\n",
      "----------------------------------\n",
      "Num timesteps: 45713\n",
      "Best mean reward: 41.72 - Last mean reward per episode: 42.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.6     |\n",
      "|    ep_rew_mean      | 45.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2144     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17871    |\n",
      "|    total_timesteps  | 45899    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0039   |\n",
      "|    n_updates        | 11449    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.4     |\n",
      "|    ep_rew_mean      | 45.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2148     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17908    |\n",
      "|    total_timesteps  | 45999    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00107  |\n",
      "|    n_updates        | 11474    |\n",
      "----------------------------------\n",
      "Num timesteps: 46073\n",
      "Best mean reward: 42.16 - Last mean reward per episode: 42.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.5     |\n",
      "|    ep_rew_mean      | 44.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2152     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 17947    |\n",
      "|    total_timesteps  | 46110    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 11502    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.4     |\n",
      "|    ep_rew_mean      | 44.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2156     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18006    |\n",
      "|    total_timesteps  | 46276    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00314  |\n",
      "|    n_updates        | 11543    |\n",
      "----------------------------------\n",
      "Num timesteps: 46433\n",
      "Best mean reward: 42.43 - Last mean reward per episode: 42.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45       |\n",
      "|    ep_rew_mean      | 45       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2160     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18075    |\n",
      "|    total_timesteps  | 46477    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0332   |\n",
      "|    n_updates        | 11594    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46       |\n",
      "|    ep_rew_mean      | 46       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2164     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18149    |\n",
      "|    total_timesteps  | 46711    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0214   |\n",
      "|    n_updates        | 11652    |\n",
      "----------------------------------\n",
      "Num timesteps: 46793\n",
      "Best mean reward: 42.80 - Last mean reward per episode: 43.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.2     |\n",
      "|    ep_rew_mean      | 45.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2168     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18199    |\n",
      "|    total_timesteps  | 46858    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0022   |\n",
      "|    n_updates        | 11689    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.5     |\n",
      "|    ep_rew_mean      | 45.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2172     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18256    |\n",
      "|    total_timesteps  | 47026    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0206   |\n",
      "|    n_updates        | 11731    |\n",
      "----------------------------------\n",
      "Num timesteps: 47153\n",
      "Best mean reward: 43.40 - Last mean reward per episode: 43.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46       |\n",
      "|    ep_rew_mean      | 46       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2176     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18341    |\n",
      "|    total_timesteps  | 47288    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00209  |\n",
      "|    n_updates        | 11796    |\n",
      "----------------------------------\n",
      "Num timesteps: 47513\n",
      "Best mean reward: 43.69 - Last mean reward per episode: 44.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46       |\n",
      "|    ep_rew_mean      | 46       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2180     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18421    |\n",
      "|    total_timesteps  | 47534    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00906  |\n",
      "|    n_updates        | 11858    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.4     |\n",
      "|    ep_rew_mean      | 46.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2184     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18499    |\n",
      "|    total_timesteps  | 47749    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.133    |\n",
      "|    n_updates        | 11912    |\n",
      "----------------------------------\n",
      "Num timesteps: 47873\n",
      "Best mean reward: 44.18 - Last mean reward per episode: 44.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.5     |\n",
      "|    ep_rew_mean      | 46.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2188     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18552    |\n",
      "|    total_timesteps  | 47923    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0176   |\n",
      "|    n_updates        | 11955    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.2     |\n",
      "|    ep_rew_mean      | 47.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2192     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18619    |\n",
      "|    total_timesteps  | 48119    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00526  |\n",
      "|    n_updates        | 12004    |\n",
      "----------------------------------\n",
      "Num timesteps: 48233\n",
      "Best mean reward: 44.50 - Last mean reward per episode: 44.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.9     |\n",
      "|    ep_rew_mean      | 46.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2196     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18688    |\n",
      "|    total_timesteps  | 48320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 12054    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.9     |\n",
      "|    ep_rew_mean      | 46.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2200     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18748    |\n",
      "|    total_timesteps  | 48503    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0626   |\n",
      "|    n_updates        | 12100    |\n",
      "----------------------------------\n",
      "Num timesteps: 48593\n",
      "Best mean reward: 44.95 - Last mean reward per episode: 45.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.5     |\n",
      "|    ep_rew_mean      | 46.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2204     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18799    |\n",
      "|    total_timesteps  | 48637    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 12134    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.1     |\n",
      "|    ep_rew_mean      | 45.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2208     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18828    |\n",
      "|    total_timesteps  | 48708    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00613  |\n",
      "|    n_updates        | 12151    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.9     |\n",
      "|    ep_rew_mean      | 43.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2212     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18880    |\n",
      "|    total_timesteps  | 48866    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0464   |\n",
      "|    n_updates        | 12191    |\n",
      "----------------------------------\n",
      "Num timesteps: 48953\n",
      "Best mean reward: 45.32 - Last mean reward per episode: 45.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.2     |\n",
      "|    ep_rew_mean      | 43.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2216     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18941    |\n",
      "|    total_timesteps  | 49029    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0237   |\n",
      "|    n_updates        | 12232    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43       |\n",
      "|    ep_rew_mean      | 43       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2220     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 18982    |\n",
      "|    total_timesteps  | 49143    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00762  |\n",
      "|    n_updates        | 12260    |\n",
      "----------------------------------\n",
      "Num timesteps: 49313\n",
      "Best mean reward: 45.32 - Last mean reward per episode: 45.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.7     |\n",
      "|    ep_rew_mean      | 42.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2224     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19040    |\n",
      "|    total_timesteps  | 49314    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00305  |\n",
      "|    n_updates        | 12303    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.1     |\n",
      "|    ep_rew_mean      | 43.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2228     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19096    |\n",
      "|    total_timesteps  | 49479    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00799  |\n",
      "|    n_updates        | 12344    |\n",
      "----------------------------------\n",
      "Num timesteps: 49673\n",
      "Best mean reward: 45.32 - Last mean reward per episode: 45.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.4     |\n",
      "|    ep_rew_mean      | 43.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2232     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19162    |\n",
      "|    total_timesteps  | 49676    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0048   |\n",
      "|    n_updates        | 12393    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.1     |\n",
      "|    ep_rew_mean      | 45.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2236     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19264    |\n",
      "|    total_timesteps  | 49984    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0122   |\n",
      "|    n_updates        | 12470    |\n",
      "----------------------------------\n",
      "Num timesteps: 50033\n",
      "Best mean reward: 45.32 - Last mean reward per episode: 45.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.8     |\n",
      "|    ep_rew_mean      | 44.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2240     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19328    |\n",
      "|    total_timesteps  | 50166    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.042    |\n",
      "|    n_updates        | 12516    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.1     |\n",
      "|    ep_rew_mean      | 44.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2244     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19375    |\n",
      "|    total_timesteps  | 50305    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.132    |\n",
      "|    n_updates        | 12551    |\n",
      "----------------------------------\n",
      "Num timesteps: 50393\n",
      "Best mean reward: 45.65 - Last mean reward per episode: 45.99\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.4     |\n",
      "|    ep_rew_mean      | 45.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2248     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19438    |\n",
      "|    total_timesteps  | 50535    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0217   |\n",
      "|    n_updates        | 12608    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.2     |\n",
      "|    ep_rew_mean      | 46.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2252     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19496    |\n",
      "|    total_timesteps  | 50732    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 12657    |\n",
      "----------------------------------\n",
      "Num timesteps: 50753\n",
      "Best mean reward: 45.99 - Last mean reward per episode: 46.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.6     |\n",
      "|    ep_rew_mean      | 46.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2256     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19559    |\n",
      "|    total_timesteps  | 50936    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00969  |\n",
      "|    n_updates        | 12708    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.2     |\n",
      "|    ep_rew_mean      | 46.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2260     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19612    |\n",
      "|    total_timesteps  | 51097    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0116   |\n",
      "|    n_updates        | 12749    |\n",
      "----------------------------------\n",
      "Num timesteps: 51113\n",
      "Best mean reward: 46.26 - Last mean reward per episode: 46.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.6     |\n",
      "|    ep_rew_mean      | 45.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2264     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19664    |\n",
      "|    total_timesteps  | 51269    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0529   |\n",
      "|    n_updates        | 12792    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.9     |\n",
      "|    ep_rew_mean      | 45.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2268     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19716    |\n",
      "|    total_timesteps  | 51446    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 12836    |\n",
      "----------------------------------\n",
      "Num timesteps: 51473\n",
      "Best mean reward: 46.73 - Last mean reward per episode: 46.87\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.5     |\n",
      "|    ep_rew_mean      | 46.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2272     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19783    |\n",
      "|    total_timesteps  | 51672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00665  |\n",
      "|    n_updates        | 12892    |\n",
      "----------------------------------\n",
      "Num timesteps: 51833\n",
      "Best mean reward: 46.87 - Last mean reward per episode: 47.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2276     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19849    |\n",
      "|    total_timesteps  | 51903    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00542  |\n",
      "|    n_updates        | 12950    |\n",
      "----------------------------------\n",
      "Num timesteps: 52193\n",
      "Best mean reward: 47.30 - Last mean reward per episode: 47.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.7     |\n",
      "|    ep_rew_mean      | 46.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2280     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19939    |\n",
      "|    total_timesteps  | 52200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0197   |\n",
      "|    n_updates        | 13024    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.4     |\n",
      "|    ep_rew_mean      | 46.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2284     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 19998    |\n",
      "|    total_timesteps  | 52393    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.141    |\n",
      "|    n_updates        | 13073    |\n",
      "----------------------------------\n",
      "Num timesteps: 52553\n",
      "Best mean reward: 47.30 - Last mean reward per episode: 47.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.8     |\n",
      "|    ep_rew_mean      | 46.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2288     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20053    |\n",
      "|    total_timesteps  | 52600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0156   |\n",
      "|    n_updates        | 13124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.8     |\n",
      "|    ep_rew_mean      | 46.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2292     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20115    |\n",
      "|    total_timesteps  | 52795    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0407   |\n",
      "|    n_updates        | 13173    |\n",
      "----------------------------------\n",
      "Num timesteps: 52913\n",
      "Best mean reward: 47.33 - Last mean reward per episode: 47.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.6     |\n",
      "|    ep_rew_mean      | 47.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2296     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20196    |\n",
      "|    total_timesteps  | 53077    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00796  |\n",
      "|    n_updates        | 13244    |\n",
      "----------------------------------\n",
      "Num timesteps: 53273\n",
      "Best mean reward: 47.41 - Last mean reward per episode: 47.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.7     |\n",
      "|    ep_rew_mean      | 48.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2300     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20288    |\n",
      "|    total_timesteps  | 53373    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.108    |\n",
      "|    n_updates        | 13318    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.7     |\n",
      "|    ep_rew_mean      | 49.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2304     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20360    |\n",
      "|    total_timesteps  | 53610    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00488  |\n",
      "|    n_updates        | 13377    |\n",
      "----------------------------------\n",
      "Num timesteps: 53633\n",
      "Best mean reward: 47.41 - Last mean reward per episode: 47.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.1     |\n",
      "|    ep_rew_mean      | 51.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2308     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20420    |\n",
      "|    total_timesteps  | 53814    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.108    |\n",
      "|    n_updates        | 13428    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.6     |\n",
      "|    ep_rew_mean      | 50.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2312     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20456    |\n",
      "|    total_timesteps  | 53930    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00815  |\n",
      "|    n_updates        | 13457    |\n",
      "----------------------------------\n",
      "Num timesteps: 53993\n",
      "Best mean reward: 47.60 - Last mean reward per episode: 47.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.4     |\n",
      "|    ep_rew_mean      | 51.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2316     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20535    |\n",
      "|    total_timesteps  | 54171    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0555   |\n",
      "|    n_updates        | 13517    |\n",
      "----------------------------------\n",
      "Num timesteps: 54353\n",
      "Best mean reward: 47.67 - Last mean reward per episode: 47.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.4     |\n",
      "|    ep_rew_mean      | 53.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2320     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20627    |\n",
      "|    total_timesteps  | 54480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00384  |\n",
      "|    n_updates        | 13594    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.9     |\n",
      "|    ep_rew_mean      | 53.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2324     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20702    |\n",
      "|    total_timesteps  | 54705    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0526   |\n",
      "|    n_updates        | 13651    |\n",
      "----------------------------------\n",
      "Num timesteps: 54713\n",
      "Best mean reward: 47.90 - Last mean reward per episode: 47.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.7     |\n",
      "|    ep_rew_mean      | 53.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2328     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20752    |\n",
      "|    total_timesteps  | 54852    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0988   |\n",
      "|    n_updates        | 13687    |\n",
      "----------------------------------\n",
      "Num timesteps: 55073\n",
      "Best mean reward: 47.96 - Last mean reward per episode: 48.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.2     |\n",
      "|    ep_rew_mean      | 54.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2332     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20826    |\n",
      "|    total_timesteps  | 55100    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 13749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.5     |\n",
      "|    ep_rew_mean      | 53.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2336     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20893    |\n",
      "|    total_timesteps  | 55339    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00968  |\n",
      "|    n_updates        | 13809    |\n",
      "----------------------------------\n",
      "Num timesteps: 55433\n",
      "Best mean reward: 48.10 - Last mean reward per episode: 48.27\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.8     |\n",
      "|    ep_rew_mean      | 52.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2340     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 20928    |\n",
      "|    total_timesteps  | 55443    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0241   |\n",
      "|    n_updates        | 13835    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.3     |\n",
      "|    ep_rew_mean      | 54.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2344     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21013    |\n",
      "|    total_timesteps  | 55731    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0594   |\n",
      "|    n_updates        | 13907    |\n",
      "----------------------------------\n",
      "Num timesteps: 55793\n",
      "Best mean reward: 48.27 - Last mean reward per episode: 48.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.9     |\n",
      "|    ep_rew_mean      | 53.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2348     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21073    |\n",
      "|    total_timesteps  | 55920    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.132    |\n",
      "|    n_updates        | 13954    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.2     |\n",
      "|    ep_rew_mean      | 53.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2352     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21121    |\n",
      "|    total_timesteps  | 56050    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0819   |\n",
      "|    n_updates        | 13987    |\n",
      "----------------------------------\n",
      "Num timesteps: 56153\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 48.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.9     |\n",
      "|    ep_rew_mean      | 52.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2356     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21171    |\n",
      "|    total_timesteps  | 56224    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00692  |\n",
      "|    n_updates        | 14030    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.6     |\n",
      "|    ep_rew_mean      | 52.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2360     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21218    |\n",
      "|    total_timesteps  | 56355    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00896  |\n",
      "|    n_updates        | 14063    |\n",
      "----------------------------------\n",
      "Num timesteps: 56513\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53       |\n",
      "|    ep_rew_mean      | 53       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2364     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21281    |\n",
      "|    total_timesteps  | 56564    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0167   |\n",
      "|    n_updates        | 14115    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.6     |\n",
      "|    ep_rew_mean      | 52.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2368     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21324    |\n",
      "|    total_timesteps  | 56707    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0203   |\n",
      "|    n_updates        | 14151    |\n",
      "----------------------------------\n",
      "Num timesteps: 56873\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.6     |\n",
      "|    ep_rew_mean      | 52.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2372     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21393    |\n",
      "|    total_timesteps  | 56933    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00427  |\n",
      "|    n_updates        | 14208    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52       |\n",
      "|    ep_rew_mean      | 52       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2376     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21456    |\n",
      "|    total_timesteps  | 57105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00487  |\n",
      "|    n_updates        | 14251    |\n",
      "----------------------------------\n",
      "Num timesteps: 57233\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.7     |\n",
      "|    ep_rew_mean      | 50.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2380     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21510    |\n",
      "|    total_timesteps  | 57274    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0171   |\n",
      "|    n_updates        | 14293    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.8     |\n",
      "|    ep_rew_mean      | 50.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2384     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21576    |\n",
      "|    total_timesteps  | 57472    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0791   |\n",
      "|    n_updates        | 14342    |\n",
      "----------------------------------\n",
      "Num timesteps: 57593\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.99\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.7     |\n",
      "|    ep_rew_mean      | 50.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2388     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21636    |\n",
      "|    total_timesteps  | 57674    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00832  |\n",
      "|    n_updates        | 14393    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.2     |\n",
      "|    ep_rew_mean      | 51.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2392     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21708    |\n",
      "|    total_timesteps  | 57914    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0335   |\n",
      "|    n_updates        | 14453    |\n",
      "----------------------------------\n",
      "Num timesteps: 57953\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.1     |\n",
      "|    ep_rew_mean      | 50.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2396     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21761    |\n",
      "|    total_timesteps  | 58091    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.149    |\n",
      "|    n_updates        | 14497    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.2     |\n",
      "|    ep_rew_mean      | 48.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2400     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21798    |\n",
      "|    total_timesteps  | 58194    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00192  |\n",
      "|    n_updates        | 14523    |\n",
      "----------------------------------\n",
      "Num timesteps: 58313\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.8     |\n",
      "|    ep_rew_mean      | 47.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2404     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21856    |\n",
      "|    total_timesteps  | 58386    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0446   |\n",
      "|    n_updates        | 14571    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.1     |\n",
      "|    ep_rew_mean      | 47.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2408     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21908    |\n",
      "|    total_timesteps  | 58529    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00914  |\n",
      "|    n_updates        | 14607    |\n",
      "----------------------------------\n",
      "Num timesteps: 58673\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.5     |\n",
      "|    ep_rew_mean      | 47.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2412     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 21957    |\n",
      "|    total_timesteps  | 58683    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00262  |\n",
      "|    n_updates        | 14645    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.1     |\n",
      "|    ep_rew_mean      | 47.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2416     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22018    |\n",
      "|    total_timesteps  | 58879    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 14694    |\n",
      "----------------------------------\n",
      "Num timesteps: 59033\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46       |\n",
      "|    ep_rew_mean      | 46       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2420     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22084    |\n",
      "|    total_timesteps  | 59076    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00712  |\n",
      "|    n_updates        | 14743    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2424     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22157    |\n",
      "|    total_timesteps  | 59315    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.153    |\n",
      "|    n_updates        | 14803    |\n",
      "----------------------------------\n",
      "Num timesteps: 59393\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.6     |\n",
      "|    ep_rew_mean      | 46.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2428     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22224    |\n",
      "|    total_timesteps  | 59509    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0802   |\n",
      "|    n_updates        | 14852    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.9     |\n",
      "|    ep_rew_mean      | 45.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2432     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22277    |\n",
      "|    total_timesteps  | 59693    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0141   |\n",
      "|    n_updates        | 14898    |\n",
      "----------------------------------\n",
      "Num timesteps: 59753\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.8     |\n",
      "|    ep_rew_mean      | 45.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2436     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22344    |\n",
      "|    total_timesteps  | 59919    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00719  |\n",
      "|    n_updates        | 14954    |\n",
      "----------------------------------\n",
      "Num timesteps: 60113\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.9     |\n",
      "|    ep_rew_mean      | 46.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2440     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22413    |\n",
      "|    total_timesteps  | 60133    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00809  |\n",
      "|    n_updates        | 15008    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.2     |\n",
      "|    ep_rew_mean      | 46.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2444     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22481    |\n",
      "|    total_timesteps  | 60354    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00513  |\n",
      "|    n_updates        | 15063    |\n",
      "----------------------------------\n",
      "Num timesteps: 60473\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 48.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.2     |\n",
      "|    ep_rew_mean      | 46.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2448     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22547    |\n",
      "|    total_timesteps  | 60541    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.013    |\n",
      "|    n_updates        | 15110    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.2     |\n",
      "|    ep_rew_mean      | 46.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2452     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22594    |\n",
      "|    total_timesteps  | 60671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 15142    |\n",
      "----------------------------------\n",
      "Num timesteps: 60833\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2456     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22655    |\n",
      "|    total_timesteps  | 60834    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00506  |\n",
      "|    n_updates        | 15183    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.6     |\n",
      "|    ep_rew_mean      | 46.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2460     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22718    |\n",
      "|    total_timesteps  | 61015    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.107    |\n",
      "|    n_updates        | 15228    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.2     |\n",
      "|    ep_rew_mean      | 46.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2464     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22776    |\n",
      "|    total_timesteps  | 61182    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0221   |\n",
      "|    n_updates        | 15270    |\n",
      "----------------------------------\n",
      "Num timesteps: 61193\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2468     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22867    |\n",
      "|    total_timesteps  | 61448    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00995  |\n",
      "|    n_updates        | 15336    |\n",
      "----------------------------------\n",
      "Num timesteps: 61553\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47       |\n",
      "|    ep_rew_mean      | 47       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2472     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22934    |\n",
      "|    total_timesteps  | 61637    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 15384    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.7     |\n",
      "|    ep_rew_mean      | 46.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2476     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 22987    |\n",
      "|    total_timesteps  | 61775    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0302   |\n",
      "|    n_updates        | 15418    |\n",
      "----------------------------------\n",
      "Num timesteps: 61913\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.7     |\n",
      "|    ep_rew_mean      | 47.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2480     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23077    |\n",
      "|    total_timesteps  | 62041    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0754   |\n",
      "|    n_updates        | 15485    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.2     |\n",
      "|    ep_rew_mean      | 47.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2484     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23135    |\n",
      "|    total_timesteps  | 62197    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0936   |\n",
      "|    n_updates        | 15524    |\n",
      "----------------------------------\n",
      "Num timesteps: 62273\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2488     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23207    |\n",
      "|    total_timesteps  | 62415    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00554  |\n",
      "|    n_updates        | 15578    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.3     |\n",
      "|    ep_rew_mean      | 46.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2492     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23258    |\n",
      "|    total_timesteps  | 62542    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.106    |\n",
      "|    n_updates        | 15610    |\n",
      "----------------------------------\n",
      "Num timesteps: 62633\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.6     |\n",
      "|    ep_rew_mean      | 46.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2496     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23331    |\n",
      "|    total_timesteps  | 62753    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 15663    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.6     |\n",
      "|    ep_rew_mean      | 47.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2500     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23398    |\n",
      "|    total_timesteps  | 62952    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 15712    |\n",
      "----------------------------------\n",
      "Num timesteps: 62993\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.1     |\n",
      "|    ep_rew_mean      | 47.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2504     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23453    |\n",
      "|    total_timesteps  | 63099    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0775   |\n",
      "|    n_updates        | 15749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.2     |\n",
      "|    ep_rew_mean      | 47.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2508     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23504    |\n",
      "|    total_timesteps  | 63248    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 15786    |\n",
      "----------------------------------\n",
      "Num timesteps: 63353\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47       |\n",
      "|    ep_rew_mean      | 47       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2512     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23558    |\n",
      "|    total_timesteps  | 63380    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 15819    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.8     |\n",
      "|    ep_rew_mean      | 46.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2516     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23626    |\n",
      "|    total_timesteps  | 63562    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00388  |\n",
      "|    n_updates        | 15865    |\n",
      "----------------------------------\n",
      "Num timesteps: 63713\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.4     |\n",
      "|    ep_rew_mean      | 46.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2520     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23680    |\n",
      "|    total_timesteps  | 63715    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00849  |\n",
      "|    n_updates        | 15903    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.4     |\n",
      "|    ep_rew_mean      | 45.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2524     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23730    |\n",
      "|    total_timesteps  | 63850    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0572   |\n",
      "|    n_updates        | 15937    |\n",
      "----------------------------------\n",
      "Num timesteps: 64073\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.6     |\n",
      "|    ep_rew_mean      | 45.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2528     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23801    |\n",
      "|    total_timesteps  | 64074    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.123    |\n",
      "|    n_updates        | 15993    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.1     |\n",
      "|    ep_rew_mean      | 45.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2532     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23850    |\n",
      "|    total_timesteps  | 64207    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00683  |\n",
      "|    n_updates        | 16026    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.2     |\n",
      "|    ep_rew_mean      | 44.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2536     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23901    |\n",
      "|    total_timesteps  | 64339    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0387   |\n",
      "|    n_updates        | 16059    |\n",
      "----------------------------------\n",
      "Num timesteps: 64433\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.3     |\n",
      "|    ep_rew_mean      | 44.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2540     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 23976    |\n",
      "|    total_timesteps  | 64564    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 16115    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2544     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24030    |\n",
      "|    total_timesteps  | 64712    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 16152    |\n",
      "----------------------------------\n",
      "Num timesteps: 64793\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.6     |\n",
      "|    ep_rew_mean      | 42.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2548     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24066    |\n",
      "|    total_timesteps  | 64799    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00417  |\n",
      "|    n_updates        | 16174    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.2     |\n",
      "|    ep_rew_mean      | 42.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2552     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24105    |\n",
      "|    total_timesteps  | 64895    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.162    |\n",
      "|    n_updates        | 16198    |\n",
      "----------------------------------\n",
      "Num timesteps: 65153\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.2     |\n",
      "|    ep_rew_mean      | 43.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2556     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24202    |\n",
      "|    total_timesteps  | 65154    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0862   |\n",
      "|    n_updates        | 16263    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.8     |\n",
      "|    ep_rew_mean      | 43.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2560     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24286    |\n",
      "|    total_timesteps  | 65392    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00763  |\n",
      "|    n_updates        | 16322    |\n",
      "----------------------------------\n",
      "Num timesteps: 65513\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.87\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.7     |\n",
      "|    ep_rew_mean      | 43.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2564     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24341    |\n",
      "|    total_timesteps  | 65548    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0294   |\n",
      "|    n_updates        | 16361    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.6     |\n",
      "|    ep_rew_mean      | 42.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2568     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24397    |\n",
      "|    total_timesteps  | 65710    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00588  |\n",
      "|    n_updates        | 16402    |\n",
      "----------------------------------\n",
      "Num timesteps: 65873\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.28\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.8     |\n",
      "|    ep_rew_mean      | 42.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2572     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24475    |\n",
      "|    total_timesteps  | 65918    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.126    |\n",
      "|    n_updates        | 16454    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.7     |\n",
      "|    ep_rew_mean      | 42.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2576     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24526    |\n",
      "|    total_timesteps  | 66045    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0127   |\n",
      "|    n_updates        | 16486    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.5     |\n",
      "|    ep_rew_mean      | 41.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2580     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24583    |\n",
      "|    total_timesteps  | 66188    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 16521    |\n",
      "----------------------------------\n",
      "Num timesteps: 66233\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.4     |\n",
      "|    ep_rew_mean      | 41.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2584     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24640    |\n",
      "|    total_timesteps  | 66337    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.093    |\n",
      "|    n_updates        | 16559    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.3     |\n",
      "|    ep_rew_mean      | 41.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2588     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24708    |\n",
      "|    total_timesteps  | 66543    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.069    |\n",
      "|    n_updates        | 16610    |\n",
      "----------------------------------\n",
      "Num timesteps: 66593\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.8     |\n",
      "|    ep_rew_mean      | 41.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2592     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24769    |\n",
      "|    total_timesteps  | 66719    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00507  |\n",
      "|    n_updates        | 16654    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41       |\n",
      "|    ep_rew_mean      | 41       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2596     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24825    |\n",
      "|    total_timesteps  | 66857    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.107    |\n",
      "|    n_updates        | 16689    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | 40       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2600     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24862    |\n",
      "|    total_timesteps  | 66950    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0113   |\n",
      "|    n_updates        | 16712    |\n",
      "----------------------------------\n",
      "Num timesteps: 66953\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | 40       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2604     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24919    |\n",
      "|    total_timesteps  | 67097    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00691  |\n",
      "|    n_updates        | 16749    |\n",
      "----------------------------------\n",
      "Num timesteps: 67313\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.7     |\n",
      "|    ep_rew_mean      | 40.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2608     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 24994    |\n",
      "|    total_timesteps  | 67314    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0296   |\n",
      "|    n_updates        | 16803    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.6     |\n",
      "|    ep_rew_mean      | 40.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2612     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25043    |\n",
      "|    total_timesteps  | 67440    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0497   |\n",
      "|    n_updates        | 16834    |\n",
      "----------------------------------\n",
      "Num timesteps: 67673\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.6     |\n",
      "|    ep_rew_mean      | 41.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2616     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25141    |\n",
      "|    total_timesteps  | 67723    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00915  |\n",
      "|    n_updates        | 16905    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.9     |\n",
      "|    ep_rew_mean      | 41.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2620     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25209    |\n",
      "|    total_timesteps  | 67901    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0698   |\n",
      "|    n_updates        | 16950    |\n",
      "----------------------------------\n",
      "Num timesteps: 68033\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.6     |\n",
      "|    ep_rew_mean      | 42.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2624     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25283    |\n",
      "|    total_timesteps  | 68107    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00301  |\n",
      "|    n_updates        | 17001    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.5     |\n",
      "|    ep_rew_mean      | 42.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2628     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25366    |\n",
      "|    total_timesteps  | 68326    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.289    |\n",
      "|    n_updates        | 17056    |\n",
      "----------------------------------\n",
      "Num timesteps: 68393\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.5     |\n",
      "|    ep_rew_mean      | 43.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2632     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25448    |\n",
      "|    total_timesteps  | 68556    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00462  |\n",
      "|    n_updates        | 17113    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.9     |\n",
      "|    ep_rew_mean      | 43.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2636     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25508    |\n",
      "|    total_timesteps  | 68730    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0104   |\n",
      "|    n_updates        | 17157    |\n",
      "----------------------------------\n",
      "Num timesteps: 68753\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.5     |\n",
      "|    ep_rew_mean      | 43.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2640     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25574    |\n",
      "|    total_timesteps  | 68909    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 17202    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.9     |\n",
      "|    ep_rew_mean      | 43.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2644     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25643    |\n",
      "|    total_timesteps  | 69100    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00849  |\n",
      "|    n_updates        | 17249    |\n",
      "----------------------------------\n",
      "Num timesteps: 69113\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.8     |\n",
      "|    ep_rew_mean      | 44.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2648     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25703    |\n",
      "|    total_timesteps  | 69280    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0808   |\n",
      "|    n_updates        | 17294    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.4     |\n",
      "|    ep_rew_mean      | 45.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2652     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25758    |\n",
      "|    total_timesteps  | 69431    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0064   |\n",
      "|    n_updates        | 17332    |\n",
      "----------------------------------\n",
      "Num timesteps: 69473\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.3     |\n",
      "|    ep_rew_mean      | 44.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2656     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25814    |\n",
      "|    total_timesteps  | 69581    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00363  |\n",
      "|    n_updates        | 17370    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2660     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25881    |\n",
      "|    total_timesteps  | 69757    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00871  |\n",
      "|    n_updates        | 17414    |\n",
      "----------------------------------\n",
      "Num timesteps: 69833\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 45.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.8     |\n",
      "|    ep_rew_mean      | 43.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2664     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25942    |\n",
      "|    total_timesteps  | 69923    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0331   |\n",
      "|    n_updates        | 17455    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2668     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 25995    |\n",
      "|    total_timesteps  | 70073    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.048    |\n",
      "|    n_updates        | 17493    |\n",
      "----------------------------------\n",
      "Num timesteps: 70193\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 45.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.2     |\n",
      "|    ep_rew_mean      | 43.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2672     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26054    |\n",
      "|    total_timesteps  | 70241    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 17535    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.8     |\n",
      "|    ep_rew_mean      | 43.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2676     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26122    |\n",
      "|    total_timesteps  | 70427    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0288   |\n",
      "|    n_updates        | 17581    |\n",
      "----------------------------------\n",
      "Num timesteps: 70553\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.1     |\n",
      "|    ep_rew_mean      | 44.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2680     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26181    |\n",
      "|    total_timesteps  | 70594    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0176   |\n",
      "|    n_updates        | 17623    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.1     |\n",
      "|    ep_rew_mean      | 44.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2684     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26238    |\n",
      "|    total_timesteps  | 70748    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0265   |\n",
      "|    n_updates        | 17661    |\n",
      "----------------------------------\n",
      "Num timesteps: 70913\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.8     |\n",
      "|    ep_rew_mean      | 43.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2688     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26300    |\n",
      "|    total_timesteps  | 70921    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0193   |\n",
      "|    n_updates        | 17705    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2692     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26356    |\n",
      "|    total_timesteps  | 71075    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0187   |\n",
      "|    n_updates        | 17743    |\n",
      "----------------------------------\n",
      "Num timesteps: 71273\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.7     |\n",
      "|    ep_rew_mean      | 44.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2696     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26441    |\n",
      "|    total_timesteps  | 71331    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.319    |\n",
      "|    n_updates        | 17807    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.7     |\n",
      "|    ep_rew_mean      | 45.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2700     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26505    |\n",
      "|    total_timesteps  | 71520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.218    |\n",
      "|    n_updates        | 17854    |\n",
      "----------------------------------\n",
      "Num timesteps: 71633\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.5     |\n",
      "|    ep_rew_mean      | 46.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2704     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26580    |\n",
      "|    total_timesteps  | 71743    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00935  |\n",
      "|    n_updates        | 17910    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.8     |\n",
      "|    ep_rew_mean      | 45.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2708     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26636    |\n",
      "|    total_timesteps  | 71895    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 17948    |\n",
      "----------------------------------\n",
      "Num timesteps: 71993\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.7     |\n",
      "|    ep_rew_mean      | 46.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2712     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26711    |\n",
      "|    total_timesteps  | 72106    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0602   |\n",
      "|    n_updates        | 18001    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.7     |\n",
      "|    ep_rew_mean      | 44.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2716     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26746    |\n",
      "|    total_timesteps  | 72197    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00939  |\n",
      "|    n_updates        | 18024    |\n",
      "----------------------------------\n",
      "Num timesteps: 72353\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.9     |\n",
      "|    ep_rew_mean      | 45.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2720     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26839    |\n",
      "|    total_timesteps  | 72492    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.201    |\n",
      "|    n_updates        | 18097    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.9     |\n",
      "|    ep_rew_mean      | 44.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2724     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26880    |\n",
      "|    total_timesteps  | 72593    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 18123    |\n",
      "----------------------------------\n",
      "Num timesteps: 72713\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2728     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 26928    |\n",
      "|    total_timesteps  | 72727    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.16     |\n",
      "|    n_updates        | 18156    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2732     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27001    |\n",
      "|    total_timesteps  | 72951    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00929  |\n",
      "|    n_updates        | 18212    |\n",
      "----------------------------------\n",
      "Num timesteps: 73073\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2736     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27052    |\n",
      "|    total_timesteps  | 73092    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0947   |\n",
      "|    n_updates        | 18247    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2740     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27127    |\n",
      "|    total_timesteps  | 73272    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 18292    |\n",
      "----------------------------------\n",
      "Num timesteps: 73433\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.7     |\n",
      "|    ep_rew_mean      | 43.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2744     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27201    |\n",
      "|    total_timesteps  | 73474    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.175    |\n",
      "|    n_updates        | 18343    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.8     |\n",
      "|    ep_rew_mean      | 43.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2748     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27268    |\n",
      "|    total_timesteps  | 73661    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00874  |\n",
      "|    n_updates        | 18390    |\n",
      "----------------------------------\n",
      "Num timesteps: 73793\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.27\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.1     |\n",
      "|    ep_rew_mean      | 44.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2752     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27331    |\n",
      "|    total_timesteps  | 73841    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0918   |\n",
      "|    n_updates        | 18435    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.5     |\n",
      "|    ep_rew_mean      | 44.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2756     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27395    |\n",
      "|    total_timesteps  | 74036    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0458   |\n",
      "|    n_updates        | 18483    |\n",
      "----------------------------------\n",
      "Num timesteps: 74153\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45       |\n",
      "|    ep_rew_mean      | 45       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2760     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27473    |\n",
      "|    total_timesteps  | 74262    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0332   |\n",
      "|    n_updates        | 18540    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.3     |\n",
      "|    ep_rew_mean      | 45.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2764     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27538    |\n",
      "|    total_timesteps  | 74454    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.338    |\n",
      "|    n_updates        | 18588    |\n",
      "----------------------------------\n",
      "Num timesteps: 74513\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.7     |\n",
      "|    ep_rew_mean      | 45.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2768     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27607    |\n",
      "|    total_timesteps  | 74647    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0189   |\n",
      "|    n_updates        | 18636    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.6     |\n",
      "|    ep_rew_mean      | 45.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2772     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27668    |\n",
      "|    total_timesteps  | 74804    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00943  |\n",
      "|    n_updates        | 18675    |\n",
      "----------------------------------\n",
      "Num timesteps: 74873\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2776     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27744    |\n",
      "|    total_timesteps  | 75035    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0083   |\n",
      "|    n_updates        | 18733    |\n",
      "----------------------------------\n",
      "Num timesteps: 75233\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 45.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.5     |\n",
      "|    ep_rew_mean      | 46.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2780     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27811    |\n",
      "|    total_timesteps  | 75241    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0364   |\n",
      "|    n_updates        | 18785    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.2     |\n",
      "|    ep_rew_mean      | 47.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2784     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27882    |\n",
      "|    total_timesteps  | 75470    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.132    |\n",
      "|    n_updates        | 18842    |\n",
      "----------------------------------\n",
      "Num timesteps: 75593\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.3     |\n",
      "|    ep_rew_mean      | 47.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2788     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 27950    |\n",
      "|    total_timesteps  | 75652    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00504  |\n",
      "|    n_updates        | 18887    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.2     |\n",
      "|    ep_rew_mean      | 47.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2792     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28004    |\n",
      "|    total_timesteps  | 75792    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00342  |\n",
      "|    n_updates        | 18922    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.8     |\n",
      "|    ep_rew_mean      | 45.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2796     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28053    |\n",
      "|    total_timesteps  | 75908    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.156    |\n",
      "|    n_updates        | 18951    |\n",
      "----------------------------------\n",
      "Num timesteps: 75953\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.5     |\n",
      "|    ep_rew_mean      | 45.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2800     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28102    |\n",
      "|    total_timesteps  | 76066    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 18991    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.2     |\n",
      "|    ep_rew_mean      | 45.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2804     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28165    |\n",
      "|    total_timesteps  | 76260    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0144   |\n",
      "|    n_updates        | 19039    |\n",
      "----------------------------------\n",
      "Num timesteps: 76313\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2808     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28247    |\n",
      "|    total_timesteps  | 76509    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00486  |\n",
      "|    n_updates        | 19102    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.7     |\n",
      "|    ep_rew_mean      | 45.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2812     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28306    |\n",
      "|    total_timesteps  | 76672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0144   |\n",
      "|    n_updates        | 19142    |\n",
      "----------------------------------\n",
      "Num timesteps: 76673\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.7     |\n",
      "|    ep_rew_mean      | 46.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2816     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28370    |\n",
      "|    total_timesteps  | 76863    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0281   |\n",
      "|    n_updates        | 19190    |\n",
      "----------------------------------\n",
      "Num timesteps: 77033\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46       |\n",
      "|    ep_rew_mean      | 46       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2820     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28446    |\n",
      "|    total_timesteps  | 77088    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00841  |\n",
      "|    n_updates        | 19246    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2824     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28531    |\n",
      "|    total_timesteps  | 77328    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.161    |\n",
      "|    n_updates        | 19306    |\n",
      "----------------------------------\n",
      "Num timesteps: 77393\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2828     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28579    |\n",
      "|    total_timesteps  | 77466    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.208    |\n",
      "|    n_updates        | 19341    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.1     |\n",
      "|    ep_rew_mean      | 47.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2832     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28636    |\n",
      "|    total_timesteps  | 77660    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.349    |\n",
      "|    n_updates        | 19389    |\n",
      "----------------------------------\n",
      "Num timesteps: 77753\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.1     |\n",
      "|    ep_rew_mean      | 47.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2836     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28695    |\n",
      "|    total_timesteps  | 77807    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0489   |\n",
      "|    n_updates        | 19426    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.6     |\n",
      "|    ep_rew_mean      | 46.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2840     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28738    |\n",
      "|    total_timesteps  | 77933    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.208    |\n",
      "|    n_updates        | 19458    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.2     |\n",
      "|    ep_rew_mean      | 46.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2844     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28797    |\n",
      "|    total_timesteps  | 78095    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0198   |\n",
      "|    n_updates        | 19498    |\n",
      "----------------------------------\n",
      "Num timesteps: 78113\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.5     |\n",
      "|    ep_rew_mean      | 45.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2848     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28842    |\n",
      "|    total_timesteps  | 78212    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 19527    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.1     |\n",
      "|    ep_rew_mean      | 45.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2852     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28893    |\n",
      "|    total_timesteps  | 78347    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0223   |\n",
      "|    n_updates        | 19561    |\n",
      "----------------------------------\n",
      "Num timesteps: 78473\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.7     |\n",
      "|    ep_rew_mean      | 44.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2856     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 28949    |\n",
      "|    total_timesteps  | 78504    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.161    |\n",
      "|    n_updates        | 19600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2860     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29003    |\n",
      "|    total_timesteps  | 78659    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00483  |\n",
      "|    n_updates        | 19639    |\n",
      "----------------------------------\n",
      "Num timesteps: 78833\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.9     |\n",
      "|    ep_rew_mean      | 43.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2864     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29069    |\n",
      "|    total_timesteps  | 78847    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00935  |\n",
      "|    n_updates        | 19686    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.8     |\n",
      "|    ep_rew_mean      | 43.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2868     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29133    |\n",
      "|    total_timesteps  | 79031    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0946   |\n",
      "|    n_updates        | 19732    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.3     |\n",
      "|    ep_rew_mean      | 43.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2872     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29173    |\n",
      "|    total_timesteps  | 79138    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.153    |\n",
      "|    n_updates        | 19759    |\n",
      "----------------------------------\n",
      "Num timesteps: 79193\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.5     |\n",
      "|    ep_rew_mean      | 43.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2876     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29253    |\n",
      "|    total_timesteps  | 79381    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.206    |\n",
      "|    n_updates        | 19820    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.7     |\n",
      "|    ep_rew_mean      | 42.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2880     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29302    |\n",
      "|    total_timesteps  | 79512    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0156   |\n",
      "|    n_updates        | 19852    |\n",
      "----------------------------------\n",
      "Num timesteps: 79553\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.2     |\n",
      "|    ep_rew_mean      | 43.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2884     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29395    |\n",
      "|    total_timesteps  | 79792    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00332  |\n",
      "|    n_updates        | 19922    |\n",
      "----------------------------------\n",
      "Num timesteps: 79913\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.1     |\n",
      "|    ep_rew_mean      | 43.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2888     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29457    |\n",
      "|    total_timesteps  | 79965    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0179   |\n",
      "|    n_updates        | 19966    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.1     |\n",
      "|    ep_rew_mean      | 43.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2892     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29510    |\n",
      "|    total_timesteps  | 80101    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0652   |\n",
      "|    n_updates        | 20000    |\n",
      "----------------------------------\n",
      "Num timesteps: 80273\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.7     |\n",
      "|    ep_rew_mean      | 43.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2896     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29569    |\n",
      "|    total_timesteps  | 80277    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0779   |\n",
      "|    n_updates        | 20044    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.3     |\n",
      "|    ep_rew_mean      | 44.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2900     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29644    |\n",
      "|    total_timesteps  | 80498    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00508  |\n",
      "|    n_updates        | 20099    |\n",
      "----------------------------------\n",
      "Num timesteps: 80633\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.5     |\n",
      "|    ep_rew_mean      | 44.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2904     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29722    |\n",
      "|    total_timesteps  | 80713    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.073    |\n",
      "|    n_updates        | 20153    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.5     |\n",
      "|    ep_rew_mean      | 43.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2908     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29773    |\n",
      "|    total_timesteps  | 80854    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00909  |\n",
      "|    n_updates        | 20188    |\n",
      "----------------------------------\n",
      "Num timesteps: 80993\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2912     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29850    |\n",
      "|    total_timesteps  | 81074    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0196   |\n",
      "|    n_updates        | 20243    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2916     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29903    |\n",
      "|    total_timesteps  | 81223    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0223   |\n",
      "|    n_updates        | 20280    |\n",
      "----------------------------------\n",
      "Num timesteps: 81353\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.9     |\n",
      "|    ep_rew_mean      | 42.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2920     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 29961    |\n",
      "|    total_timesteps  | 81375    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0595   |\n",
      "|    n_updates        | 20318    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.5     |\n",
      "|    ep_rew_mean      | 41.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2924     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30002    |\n",
      "|    total_timesteps  | 81477    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0493   |\n",
      "|    n_updates        | 20344    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.4     |\n",
      "|    ep_rew_mean      | 41.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2928     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30051    |\n",
      "|    total_timesteps  | 81606    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0169   |\n",
      "|    n_updates        | 20376    |\n",
      "----------------------------------\n",
      "Num timesteps: 81713\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.4     |\n",
      "|    ep_rew_mean      | 42.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2932     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30147    |\n",
      "|    total_timesteps  | 81900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0177   |\n",
      "|    n_updates        | 20449    |\n",
      "----------------------------------\n",
      "Num timesteps: 82073\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.7     |\n",
      "|    ep_rew_mean      | 42.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2936     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30211    |\n",
      "|    total_timesteps  | 82074    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00779  |\n",
      "|    n_updates        | 20493    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2940     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30295    |\n",
      "|    total_timesteps  | 82295    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.157    |\n",
      "|    n_updates        | 20548    |\n",
      "----------------------------------\n",
      "Num timesteps: 82433\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.4     |\n",
      "|    ep_rew_mean      | 43.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2944     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30347    |\n",
      "|    total_timesteps  | 82437    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 20584    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2948     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30398    |\n",
      "|    total_timesteps  | 82575    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 20618    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.8     |\n",
      "|    ep_rew_mean      | 43.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2952     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30449    |\n",
      "|    total_timesteps  | 82726    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.509    |\n",
      "|    n_updates        | 20656    |\n",
      "----------------------------------\n",
      "Num timesteps: 82793\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2956     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30509    |\n",
      "|    total_timesteps  | 82901    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0699   |\n",
      "|    n_updates        | 20700    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.7     |\n",
      "|    ep_rew_mean      | 44.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2960     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30586    |\n",
      "|    total_timesteps  | 83129    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0141   |\n",
      "|    n_updates        | 20757    |\n",
      "----------------------------------\n",
      "Num timesteps: 83153\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2964     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30620    |\n",
      "|    total_timesteps  | 83206    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0331   |\n",
      "|    n_updates        | 20776    |\n",
      "----------------------------------\n",
      "Num timesteps: 83513\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.8     |\n",
      "|    ep_rew_mean      | 44.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2968     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30715    |\n",
      "|    total_timesteps  | 83515    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0253   |\n",
      "|    n_updates        | 20853    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.9     |\n",
      "|    ep_rew_mean      | 44.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2972     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30757    |\n",
      "|    total_timesteps  | 83628    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0463   |\n",
      "|    n_updates        | 20881    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.4     |\n",
      "|    ep_rew_mean      | 44.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2976     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30821    |\n",
      "|    total_timesteps  | 83817    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.181    |\n",
      "|    n_updates        | 20929    |\n",
      "----------------------------------\n",
      "Num timesteps: 83873\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.6     |\n",
      "|    ep_rew_mean      | 44.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2980     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30875    |\n",
      "|    total_timesteps  | 83969    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0968   |\n",
      "|    n_updates        | 20967    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.5     |\n",
      "|    ep_rew_mean      | 43.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2984     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 30937    |\n",
      "|    total_timesteps  | 84139    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0928   |\n",
      "|    n_updates        | 21009    |\n",
      "----------------------------------\n",
      "Num timesteps: 84233\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.2     |\n",
      "|    ep_rew_mean      | 44.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2988     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31019    |\n",
      "|    total_timesteps  | 84390    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 21072    |\n",
      "----------------------------------\n",
      "Num timesteps: 84593\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.1     |\n",
      "|    ep_rew_mean      | 45.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2992     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31097    |\n",
      "|    total_timesteps  | 84607    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 21126    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.8     |\n",
      "|    ep_rew_mean      | 45.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2996     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31185    |\n",
      "|    total_timesteps  | 84855    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 21188    |\n",
      "----------------------------------\n",
      "Num timesteps: 84953\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.6     |\n",
      "|    ep_rew_mean      | 44.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3000     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31221    |\n",
      "|    total_timesteps  | 84955    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0163   |\n",
      "|    n_updates        | 21213    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.8     |\n",
      "|    ep_rew_mean      | 43.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3004     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31268    |\n",
      "|    total_timesteps  | 85088    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 21246    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.1     |\n",
      "|    ep_rew_mean      | 44.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3008     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31330    |\n",
      "|    total_timesteps  | 85266    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.134    |\n",
      "|    n_updates        | 21291    |\n",
      "----------------------------------\n",
      "Num timesteps: 85313\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.3     |\n",
      "|    ep_rew_mean      | 43.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3012     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31380    |\n",
      "|    total_timesteps  | 85401    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.128    |\n",
      "|    n_updates        | 21325    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.7     |\n",
      "|    ep_rew_mean      | 43.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3016     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31450    |\n",
      "|    total_timesteps  | 85595    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.207    |\n",
      "|    n_updates        | 21373    |\n",
      "----------------------------------\n",
      "Num timesteps: 85673\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3020     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31503    |\n",
      "|    total_timesteps  | 85737    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0516   |\n",
      "|    n_updates        | 21409    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.3     |\n",
      "|    ep_rew_mean      | 45.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3024     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31593    |\n",
      "|    total_timesteps  | 86010    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0643   |\n",
      "|    n_updates        | 21477    |\n",
      "----------------------------------\n",
      "Num timesteps: 86033\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.9     |\n",
      "|    ep_rew_mean      | 44.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3028     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31629    |\n",
      "|    total_timesteps  | 86094    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.007    |\n",
      "|    n_updates        | 21498    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.6     |\n",
      "|    ep_rew_mean      | 42.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3032     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31659    |\n",
      "|    total_timesteps  | 86164    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0217   |\n",
      "|    n_updates        | 21515    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.4     |\n",
      "|    ep_rew_mean      | 42.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3036     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31714    |\n",
      "|    total_timesteps  | 86314    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.305    |\n",
      "|    n_updates        | 21553    |\n",
      "----------------------------------\n",
      "Num timesteps: 86393\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.8     |\n",
      "|    ep_rew_mean      | 41.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3040     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31774    |\n",
      "|    total_timesteps  | 86475    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.232    |\n",
      "|    n_updates        | 21593    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.6     |\n",
      "|    ep_rew_mean      | 42.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3044     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31847    |\n",
      "|    total_timesteps  | 86698    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 21649    |\n",
      "----------------------------------\n",
      "Num timesteps: 86753\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.9     |\n",
      "|    ep_rew_mean      | 42.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3048     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31906    |\n",
      "|    total_timesteps  | 86861    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00342  |\n",
      "|    n_updates        | 21690    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.3     |\n",
      "|    ep_rew_mean      | 43.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3052     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 31974    |\n",
      "|    total_timesteps  | 87054    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 21738    |\n",
      "----------------------------------\n",
      "Num timesteps: 87113\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.1     |\n",
      "|    ep_rew_mean      | 43.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3056     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32037    |\n",
      "|    total_timesteps  | 87216    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0306   |\n",
      "|    n_updates        | 21778    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.2     |\n",
      "|    ep_rew_mean      | 43.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3060     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32124    |\n",
      "|    total_timesteps  | 87451    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00558  |\n",
      "|    n_updates        | 21837    |\n",
      "----------------------------------\n",
      "Num timesteps: 87473\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.4     |\n",
      "|    ep_rew_mean      | 44.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3064     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32192    |\n",
      "|    total_timesteps  | 87641    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0456   |\n",
      "|    n_updates        | 21885    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43       |\n",
      "|    ep_rew_mean      | 43       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3068     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32253    |\n",
      "|    total_timesteps  | 87815    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.128    |\n",
      "|    n_updates        | 21928    |\n",
      "----------------------------------\n",
      "Num timesteps: 87833\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.9     |\n",
      "|    ep_rew_mean      | 42.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3072     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32297    |\n",
      "|    total_timesteps  | 87920    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0829   |\n",
      "|    n_updates        | 21954    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.3     |\n",
      "|    ep_rew_mean      | 43.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3076     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32377    |\n",
      "|    total_timesteps  | 88150    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00267  |\n",
      "|    n_updates        | 22012    |\n",
      "----------------------------------\n",
      "Num timesteps: 88193\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.2     |\n",
      "|    ep_rew_mean      | 44.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3080     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32461    |\n",
      "|    total_timesteps  | 88389    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.167    |\n",
      "|    n_updates        | 22072    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3084     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32513    |\n",
      "|    total_timesteps  | 88537    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00451  |\n",
      "|    n_updates        | 22109    |\n",
      "----------------------------------\n",
      "Num timesteps: 88553\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.5     |\n",
      "|    ep_rew_mean      | 43.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3088     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32583    |\n",
      "|    total_timesteps  | 88738    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00777  |\n",
      "|    n_updates        | 22159    |\n",
      "----------------------------------\n",
      "Num timesteps: 88913\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.6     |\n",
      "|    ep_rew_mean      | 43.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3092     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32667    |\n",
      "|    total_timesteps  | 88967    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.388    |\n",
      "|    n_updates        | 22216    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43       |\n",
      "|    ep_rew_mean      | 43       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3096     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32734    |\n",
      "|    total_timesteps  | 89153    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 22263    |\n",
      "----------------------------------\n",
      "Num timesteps: 89273\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.5     |\n",
      "|    ep_rew_mean      | 43.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3100     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32795    |\n",
      "|    total_timesteps  | 89306    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.351    |\n",
      "|    n_updates        | 22301    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3104     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32861    |\n",
      "|    total_timesteps  | 89485    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0461   |\n",
      "|    n_updates        | 22346    |\n",
      "----------------------------------\n",
      "Num timesteps: 89633\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3108     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32925    |\n",
      "|    total_timesteps  | 89671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 22392    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.6     |\n",
      "|    ep_rew_mean      | 44.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3112     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 32994    |\n",
      "|    total_timesteps  | 89865    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00738  |\n",
      "|    n_updates        | 22441    |\n",
      "----------------------------------\n",
      "Num timesteps: 89993\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3116     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33044    |\n",
      "|    total_timesteps  | 89994    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.017    |\n",
      "|    n_updates        | 22473    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.8     |\n",
      "|    ep_rew_mean      | 43.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3120     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33096    |\n",
      "|    total_timesteps  | 90119    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0106   |\n",
      "|    n_updates        | 22504    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.3     |\n",
      "|    ep_rew_mean      | 43.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3124     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33177    |\n",
      "|    total_timesteps  | 90342    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00807  |\n",
      "|    n_updates        | 22560    |\n",
      "----------------------------------\n",
      "Num timesteps: 90353\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.3     |\n",
      "|    ep_rew_mean      | 44.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3128     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33247    |\n",
      "|    total_timesteps  | 90525    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00446  |\n",
      "|    n_updates        | 22606    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.1     |\n",
      "|    ep_rew_mean      | 45.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3132     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33304    |\n",
      "|    total_timesteps  | 90676    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00652  |\n",
      "|    n_updates        | 22643    |\n",
      "----------------------------------\n",
      "Num timesteps: 90713\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.1     |\n",
      "|    ep_rew_mean      | 45.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3136     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33364    |\n",
      "|    total_timesteps  | 90828    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0056   |\n",
      "|    n_updates        | 22681    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.9     |\n",
      "|    ep_rew_mean      | 45.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3140     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33447    |\n",
      "|    total_timesteps  | 91062    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00322  |\n",
      "|    n_updates        | 22740    |\n",
      "----------------------------------\n",
      "Num timesteps: 91073\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45       |\n",
      "|    ep_rew_mean      | 45       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3144     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33500    |\n",
      "|    total_timesteps  | 91203    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00796  |\n",
      "|    n_updates        | 22775    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.3     |\n",
      "|    ep_rew_mean      | 45.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3148     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33573    |\n",
      "|    total_timesteps  | 91395    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00772  |\n",
      "|    n_updates        | 22823    |\n",
      "----------------------------------\n",
      "Num timesteps: 91433\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.7     |\n",
      "|    ep_rew_mean      | 44.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3152     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33624    |\n",
      "|    total_timesteps  | 91527    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00709  |\n",
      "|    n_updates        | 22856    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.2     |\n",
      "|    ep_rew_mean      | 45.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3156     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33703    |\n",
      "|    total_timesteps  | 91738    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0547   |\n",
      "|    n_updates        | 22909    |\n",
      "----------------------------------\n",
      "Num timesteps: 91793\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.8     |\n",
      "|    ep_rew_mean      | 44.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3160     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33775    |\n",
      "|    total_timesteps  | 91934    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00725  |\n",
      "|    n_updates        | 22958    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3164     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33818    |\n",
      "|    total_timesteps  | 92039    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 22984    |\n",
      "----------------------------------\n",
      "Num timesteps: 92153\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.5     |\n",
      "|    ep_rew_mean      | 43.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3168     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33866    |\n",
      "|    total_timesteps  | 92161    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0185   |\n",
      "|    n_updates        | 23015    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.9     |\n",
      "|    ep_rew_mean      | 43.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3172     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33925    |\n",
      "|    total_timesteps  | 92306    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0441   |\n",
      "|    n_updates        | 23051    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43       |\n",
      "|    ep_rew_mean      | 43       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3176     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 33978    |\n",
      "|    total_timesteps  | 92448    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0209   |\n",
      "|    n_updates        | 23086    |\n",
      "----------------------------------\n",
      "Num timesteps: 92513\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.1     |\n",
      "|    ep_rew_mean      | 42.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3180     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34035    |\n",
      "|    total_timesteps  | 92598    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 23124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.8     |\n",
      "|    ep_rew_mean      | 41.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3184     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34085    |\n",
      "|    total_timesteps  | 92716    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0717   |\n",
      "|    n_updates        | 23153    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.2     |\n",
      "|    ep_rew_mean      | 41.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3188     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34144    |\n",
      "|    total_timesteps  | 92860    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0147   |\n",
      "|    n_updates        | 23189    |\n",
      "----------------------------------\n",
      "Num timesteps: 92873\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.2     |\n",
      "|    ep_rew_mean      | 40.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3192     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34198    |\n",
      "|    total_timesteps  | 92988    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.153    |\n",
      "|    n_updates        | 23221    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.6     |\n",
      "|    ep_rew_mean      | 40.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3196     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34277    |\n",
      "|    total_timesteps  | 93215    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00814  |\n",
      "|    n_updates        | 23278    |\n",
      "----------------------------------\n",
      "Num timesteps: 93233\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.9     |\n",
      "|    ep_rew_mean      | 40.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3200     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34344    |\n",
      "|    total_timesteps  | 93398    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0443   |\n",
      "|    n_updates        | 23324    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.8     |\n",
      "|    ep_rew_mean      | 40.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3204     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34408    |\n",
      "|    total_timesteps  | 93565    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.215    |\n",
      "|    n_updates        | 23366    |\n",
      "----------------------------------\n",
      "Num timesteps: 93593\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.8     |\n",
      "|    ep_rew_mean      | 40.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3208     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34471    |\n",
      "|    total_timesteps  | 93749    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00982  |\n",
      "|    n_updates        | 23412    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.4     |\n",
      "|    ep_rew_mean      | 40.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3212     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34531    |\n",
      "|    total_timesteps  | 93902    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0094   |\n",
      "|    n_updates        | 23450    |\n",
      "----------------------------------\n",
      "Num timesteps: 93953\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.8     |\n",
      "|    ep_rew_mean      | 40.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3216     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34595    |\n",
      "|    total_timesteps  | 94071    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00653  |\n",
      "|    n_updates        | 23492    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.4     |\n",
      "|    ep_rew_mean      | 41.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3220     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34668    |\n",
      "|    total_timesteps  | 94262    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.231    |\n",
      "|    n_updates        | 23540    |\n",
      "----------------------------------\n",
      "Num timesteps: 94313\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.5     |\n",
      "|    ep_rew_mean      | 40.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3224     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34718    |\n",
      "|    total_timesteps  | 94391    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00642  |\n",
      "|    n_updates        | 23572    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.3     |\n",
      "|    ep_rew_mean      | 40.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3228     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34785    |\n",
      "|    total_timesteps  | 94555    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0969   |\n",
      "|    n_updates        | 23613    |\n",
      "----------------------------------\n",
      "Num timesteps: 94673\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.8     |\n",
      "|    ep_rew_mean      | 40.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3232     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34854    |\n",
      "|    total_timesteps  | 94753    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0493   |\n",
      "|    n_updates        | 23663    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.3     |\n",
      "|    ep_rew_mean      | 41.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3236     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34931    |\n",
      "|    total_timesteps  | 94961    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0362   |\n",
      "|    n_updates        | 23715    |\n",
      "----------------------------------\n",
      "Num timesteps: 95033\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.2     |\n",
      "|    ep_rew_mean      | 40.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3240     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 34978    |\n",
      "|    total_timesteps  | 95082    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.287    |\n",
      "|    n_updates        | 23745    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.3     |\n",
      "|    ep_rew_mean      | 40.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3244     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35043    |\n",
      "|    total_timesteps  | 95236    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0544   |\n",
      "|    n_updates        | 23783    |\n",
      "----------------------------------\n",
      "Num timesteps: 95393\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.3     |\n",
      "|    ep_rew_mean      | 40.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3248     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35111    |\n",
      "|    total_timesteps  | 95426    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0204   |\n",
      "|    n_updates        | 23831    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.3     |\n",
      "|    ep_rew_mean      | 41.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3252     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35197    |\n",
      "|    total_timesteps  | 95656    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0125   |\n",
      "|    n_updates        | 23888    |\n",
      "----------------------------------\n",
      "Num timesteps: 95753\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41       |\n",
      "|    ep_rew_mean      | 41       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3256     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35264    |\n",
      "|    total_timesteps  | 95840    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.26     |\n",
      "|    n_updates        | 23934    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.2     |\n",
      "|    ep_rew_mean      | 41.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3260     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35340    |\n",
      "|    total_timesteps  | 96052    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0089   |\n",
      "|    n_updates        | 23987    |\n",
      "----------------------------------\n",
      "Num timesteps: 96113\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.9     |\n",
      "|    ep_rew_mean      | 41.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3264     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35407    |\n",
      "|    total_timesteps  | 96231    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0417   |\n",
      "|    n_updates        | 24032    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.3     |\n",
      "|    ep_rew_mean      | 42.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3268     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35468    |\n",
      "|    total_timesteps  | 96393    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00191  |\n",
      "|    n_updates        | 24073    |\n",
      "----------------------------------\n",
      "Num timesteps: 96473\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.5     |\n",
      "|    ep_rew_mean      | 42.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3272     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35530    |\n",
      "|    total_timesteps  | 96554    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0272   |\n",
      "|    n_updates        | 24113    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.9     |\n",
      "|    ep_rew_mean      | 42.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3276     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35597    |\n",
      "|    total_timesteps  | 96742    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0343   |\n",
      "|    n_updates        | 24160    |\n",
      "----------------------------------\n",
      "Num timesteps: 96833\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.7     |\n",
      "|    ep_rew_mean      | 42.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3280     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35646    |\n",
      "|    total_timesteps  | 96864    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.219    |\n",
      "|    n_updates        | 24190    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.4     |\n",
      "|    ep_rew_mean      | 43.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3284     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35715    |\n",
      "|    total_timesteps  | 97051    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0266   |\n",
      "|    n_updates        | 24237    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.2     |\n",
      "|    ep_rew_mean      | 43.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3288     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35762    |\n",
      "|    total_timesteps  | 97180    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 24269    |\n",
      "----------------------------------\n",
      "Num timesteps: 97193\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.5     |\n",
      "|    ep_rew_mean      | 43.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3292     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35822    |\n",
      "|    total_timesteps  | 97333    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.204    |\n",
      "|    n_updates        | 24308    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.6     |\n",
      "|    ep_rew_mean      | 42.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3296     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35882    |\n",
      "|    total_timesteps  | 97475    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0658   |\n",
      "|    n_updates        | 24343    |\n",
      "----------------------------------\n",
      "Num timesteps: 97553\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.7     |\n",
      "|    ep_rew_mean      | 42.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3300     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 35955    |\n",
      "|    total_timesteps  | 97664    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0394   |\n",
      "|    n_updates        | 24390    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.5     |\n",
      "|    ep_rew_mean      | 42.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3304     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36009    |\n",
      "|    total_timesteps  | 97811    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.178    |\n",
      "|    n_updates        | 24427    |\n",
      "----------------------------------\n",
      "Num timesteps: 97913\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.2     |\n",
      "|    ep_rew_mean      | 42.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3308     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36070    |\n",
      "|    total_timesteps  | 97965    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.191    |\n",
      "|    n_updates        | 24466    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.3     |\n",
      "|    ep_rew_mean      | 42.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3312     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36139    |\n",
      "|    total_timesteps  | 98134    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00673  |\n",
      "|    n_updates        | 24508    |\n",
      "----------------------------------\n",
      "Num timesteps: 98273\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43       |\n",
      "|    ep_rew_mean      | 43       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3316     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36228    |\n",
      "|    total_timesteps  | 98368    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 24566    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.8     |\n",
      "|    ep_rew_mean      | 42.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3320     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36293    |\n",
      "|    total_timesteps  | 98541    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.277    |\n",
      "|    n_updates        | 24610    |\n",
      "----------------------------------\n",
      "Num timesteps: 98633\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.1     |\n",
      "|    ep_rew_mean      | 43.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3324     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36360    |\n",
      "|    total_timesteps  | 98703    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0139   |\n",
      "|    n_updates        | 24650    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.7     |\n",
      "|    ep_rew_mean      | 42.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3328     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36406    |\n",
      "|    total_timesteps  | 98829    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.194    |\n",
      "|    n_updates        | 24682    |\n",
      "----------------------------------\n",
      "Num timesteps: 98993\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.8     |\n",
      "|    ep_rew_mean      | 42.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3332     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36485    |\n",
      "|    total_timesteps  | 99036    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 24733    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.3     |\n",
      "|    ep_rew_mean      | 42.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3336     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36543    |\n",
      "|    total_timesteps  | 99195    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0244   |\n",
      "|    n_updates        | 24773    |\n",
      "----------------------------------\n",
      "Num timesteps: 99353\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.78\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.9     |\n",
      "|    ep_rew_mean      | 42.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3340     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36610    |\n",
      "|    total_timesteps  | 99374    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0674   |\n",
      "|    n_updates        | 24818    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.1     |\n",
      "|    ep_rew_mean      | 43.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3344     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36674    |\n",
      "|    total_timesteps  | 99547    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.398    |\n",
      "|    n_updates        | 24861    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.2     |\n",
      "|    ep_rew_mean      | 42.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3348     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36715    |\n",
      "|    total_timesteps  | 99644    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00519  |\n",
      "|    n_updates        | 24885    |\n",
      "----------------------------------\n",
      "Num timesteps: 99713\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.27\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.1     |\n",
      "|    ep_rew_mean      | 41.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3352     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36761    |\n",
      "|    total_timesteps  | 99762    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00901  |\n",
      "|    n_updates        | 24915    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.6     |\n",
      "|    ep_rew_mean      | 41.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3356     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36846    |\n",
      "|    total_timesteps  | 99998    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.415    |\n",
      "|    n_updates        | 24974    |\n",
      "----------------------------------\n",
      "Num timesteps: 100073\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.9     |\n",
      "|    ep_rew_mean      | 40.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3360     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36892    |\n",
      "|    total_timesteps  | 100145   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0882   |\n",
      "|    n_updates        | 25011    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.2     |\n",
      "|    ep_rew_mean      | 41.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3364     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 36948    |\n",
      "|    total_timesteps  | 100353   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.392    |\n",
      "|    n_updates        | 25063    |\n",
      "----------------------------------\n",
      "Num timesteps: 100433\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.5     |\n",
      "|    ep_rew_mean      | 41.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3368     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37006    |\n",
      "|    total_timesteps  | 100546   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.459    |\n",
      "|    n_updates        | 25111    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.3     |\n",
      "|    ep_rew_mean      | 42.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3372     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37078    |\n",
      "|    total_timesteps  | 100784   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.268    |\n",
      "|    n_updates        | 25170    |\n",
      "----------------------------------\n",
      "Num timesteps: 100793\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.6     |\n",
      "|    ep_rew_mean      | 42.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3376     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37138    |\n",
      "|    total_timesteps  | 100999   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0168   |\n",
      "|    n_updates        | 25224    |\n",
      "----------------------------------\n",
      "Num timesteps: 101153\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 42.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 43.1     |\n",
      "|    ep_rew_mean      | 43.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3380     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37191    |\n",
      "|    total_timesteps  | 101171   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 25267    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.5     |\n",
      "|    ep_rew_mean      | 44.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3384     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37287    |\n",
      "|    total_timesteps  | 101499   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0664   |\n",
      "|    n_updates        | 25349    |\n",
      "----------------------------------\n",
      "Num timesteps: 101513\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.1     |\n",
      "|    ep_rew_mean      | 45.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3388     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37345    |\n",
      "|    total_timesteps  | 101695   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.522    |\n",
      "|    n_updates        | 25398    |\n",
      "----------------------------------\n",
      "Num timesteps: 101873\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 43.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3392     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37416    |\n",
      "|    total_timesteps  | 101942   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00521  |\n",
      "|    n_updates        | 25460    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.9     |\n",
      "|    ep_rew_mean      | 46.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3396     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37483    |\n",
      "|    total_timesteps  | 102165   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.341    |\n",
      "|    n_updates        | 25516    |\n",
      "----------------------------------\n",
      "Num timesteps: 102233\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.5     |\n",
      "|    ep_rew_mean      | 46.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3400     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37523    |\n",
      "|    total_timesteps  | 102310   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 25552    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.5     |\n",
      "|    ep_rew_mean      | 47.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3404     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37589    |\n",
      "|    total_timesteps  | 102562   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00651  |\n",
      "|    n_updates        | 25615    |\n",
      "----------------------------------\n",
      "Num timesteps: 102593\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.2     |\n",
      "|    ep_rew_mean      | 48.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3408     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37657    |\n",
      "|    total_timesteps  | 102784   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.373    |\n",
      "|    n_updates        | 25670    |\n",
      "----------------------------------\n",
      "Num timesteps: 102953\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.4     |\n",
      "|    ep_rew_mean      | 48.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3412     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37712    |\n",
      "|    total_timesteps  | 102975   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.215    |\n",
      "|    n_updates        | 25718    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.4     |\n",
      "|    ep_rew_mean      | 48.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3416     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37780    |\n",
      "|    total_timesteps  | 103212   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.142    |\n",
      "|    n_updates        | 25777    |\n",
      "----------------------------------\n",
      "Num timesteps: 103313\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.2     |\n",
      "|    ep_rew_mean      | 49.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3420     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37854    |\n",
      "|    total_timesteps  | 103466   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.156    |\n",
      "|    n_updates        | 25841    |\n",
      "----------------------------------\n",
      "Num timesteps: 103673\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 44.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.2     |\n",
      "|    ep_rew_mean      | 50.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3424     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 37933    |\n",
      "|    total_timesteps  | 103728   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.473    |\n",
      "|    n_updates        | 25906    |\n",
      "----------------------------------\n",
      "Num timesteps: 104033\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 45.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.2     |\n",
      "|    ep_rew_mean      | 53.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3428     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38048    |\n",
      "|    total_timesteps  | 104149   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.28     |\n",
      "|    n_updates        | 26012    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.9     |\n",
      "|    ep_rew_mean      | 52.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3432     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38103    |\n",
      "|    total_timesteps  | 104325   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.502    |\n",
      "|    n_updates        | 26056    |\n",
      "----------------------------------\n",
      "Num timesteps: 104393\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 45.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.6     |\n",
      "|    ep_rew_mean      | 53.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3436     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38165    |\n",
      "|    total_timesteps  | 104556   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0189   |\n",
      "|    n_updates        | 26113    |\n",
      "----------------------------------\n",
      "Num timesteps: 104753\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 45.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.9     |\n",
      "|    ep_rew_mean      | 54.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3440     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38255    |\n",
      "|    total_timesteps  | 104862   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.281    |\n",
      "|    n_updates        | 26190    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55.1     |\n",
      "|    ep_rew_mean      | 55.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3444     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38315    |\n",
      "|    total_timesteps  | 105058   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0428   |\n",
      "|    n_updates        | 26239    |\n",
      "----------------------------------\n",
      "Num timesteps: 105113\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 45.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.6     |\n",
      "|    ep_rew_mean      | 56.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3448     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38385    |\n",
      "|    total_timesteps  | 105301   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.228    |\n",
      "|    n_updates        | 26300    |\n",
      "----------------------------------\n",
      "Num timesteps: 105473\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.5     |\n",
      "|    ep_rew_mean      | 57.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3452     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38444    |\n",
      "|    total_timesteps  | 105511   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0133   |\n",
      "|    n_updates        | 26352    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.4     |\n",
      "|    ep_rew_mean      | 56.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3456     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38485    |\n",
      "|    total_timesteps  | 105637   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.301    |\n",
      "|    n_updates        | 26384    |\n",
      "----------------------------------\n",
      "Num timesteps: 105833\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.7     |\n",
      "|    ep_rew_mean      | 57.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3460     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38571    |\n",
      "|    total_timesteps  | 105913   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0469   |\n",
      "|    n_updates        | 26453    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.4     |\n",
      "|    ep_rew_mean      | 57.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3464     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38627    |\n",
      "|    total_timesteps  | 106096   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0343   |\n",
      "|    n_updates        | 26498    |\n",
      "----------------------------------\n",
      "Num timesteps: 106193\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.7     |\n",
      "|    ep_rew_mean      | 57.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3468     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38686    |\n",
      "|    total_timesteps  | 106314   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0139   |\n",
      "|    n_updates        | 26553    |\n",
      "----------------------------------\n",
      "Num timesteps: 106553\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.8     |\n",
      "|    ep_rew_mean      | 57.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3472     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38762    |\n",
      "|    total_timesteps  | 106567   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0213   |\n",
      "|    n_updates        | 26616    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 58.2     |\n",
      "|    ep_rew_mean      | 58.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3476     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38837    |\n",
      "|    total_timesteps  | 106822   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0112   |\n",
      "|    n_updates        | 26680    |\n",
      "----------------------------------\n",
      "Num timesteps: 106913\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 58.4     |\n",
      "|    ep_rew_mean      | 58.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3480     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38890    |\n",
      "|    total_timesteps  | 107015   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0286   |\n",
      "|    n_updates        | 26728    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.1     |\n",
      "|    ep_rew_mean      | 57.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3484     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 38946    |\n",
      "|    total_timesteps  | 107207   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0928   |\n",
      "|    n_updates        | 26776    |\n",
      "----------------------------------\n",
      "Num timesteps: 107273\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 46.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.2     |\n",
      "|    ep_rew_mean      | 57.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3488     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39009    |\n",
      "|    total_timesteps  | 107416   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0342   |\n",
      "|    n_updates        | 26828    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.7     |\n",
      "|    ep_rew_mean      | 56.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3492     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39067    |\n",
      "|    total_timesteps  | 107608   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.232    |\n",
      "|    n_updates        | 26876    |\n",
      "----------------------------------\n",
      "Num timesteps: 107633\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.7     |\n",
      "|    ep_rew_mean      | 56.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3496     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39133    |\n",
      "|    total_timesteps  | 107834   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0119   |\n",
      "|    n_updates        | 26933    |\n",
      "----------------------------------\n",
      "Num timesteps: 107993\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.4     |\n",
      "|    ep_rew_mean      | 57.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3500     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39195    |\n",
      "|    total_timesteps  | 108048   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.259    |\n",
      "|    n_updates        | 26986    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.2     |\n",
      "|    ep_rew_mean      | 57.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3504     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39269    |\n",
      "|    total_timesteps  | 108284   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.092    |\n",
      "|    n_updates        | 27045    |\n",
      "----------------------------------\n",
      "Num timesteps: 108353\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.8     |\n",
      "|    ep_rew_mean      | 56.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3508     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39324    |\n",
      "|    total_timesteps  | 108463   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0142   |\n",
      "|    n_updates        | 27090    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.1     |\n",
      "|    ep_rew_mean      | 57.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3512     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39391    |\n",
      "|    total_timesteps  | 108690   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0133   |\n",
      "|    n_updates        | 27147    |\n",
      "----------------------------------\n",
      "Num timesteps: 108713\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.7     |\n",
      "|    ep_rew_mean      | 57.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3516     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39472    |\n",
      "|    total_timesteps  | 108980   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.215    |\n",
      "|    n_updates        | 27219    |\n",
      "----------------------------------\n",
      "Num timesteps: 109073\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 47.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.9     |\n",
      "|    ep_rew_mean      | 56.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3520     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39524    |\n",
      "|    total_timesteps  | 109160   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.024    |\n",
      "|    n_updates        | 27264    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57       |\n",
      "|    ep_rew_mean      | 57       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3524     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39603    |\n",
      "|    total_timesteps  | 109427   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0295   |\n",
      "|    n_updates        | 27331    |\n",
      "----------------------------------\n",
      "Num timesteps: 109433\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 48.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55.8     |\n",
      "|    ep_rew_mean      | 55.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3528     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39683    |\n",
      "|    total_timesteps  | 109725   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 27406    |\n",
      "----------------------------------\n",
      "Num timesteps: 109793\n",
      "Best mean reward: 48.59 - Last mean reward per episode: 48.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55.5     |\n",
      "|    ep_rew_mean      | 55.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3532     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39728    |\n",
      "|    total_timesteps  | 109873   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0113   |\n",
      "|    n_updates        | 27443    |\n",
      "----------------------------------\n",
      "Num timesteps: 110153\n",
      "Best mean reward: 48.71 - Last mean reward per episode: 49.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.2     |\n",
      "|    ep_rew_mean      | 56.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3536     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39820    |\n",
      "|    total_timesteps  | 110178   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.357    |\n",
      "|    n_updates        | 27519    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.1     |\n",
      "|    ep_rew_mean      | 56.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3540     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39916    |\n",
      "|    total_timesteps  | 110477   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0721   |\n",
      "|    n_updates        | 27594    |\n",
      "----------------------------------\n",
      "Num timesteps: 110513\n",
      "Best mean reward: 49.18 - Last mean reward per episode: 49.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.1     |\n",
      "|    ep_rew_mean      | 56.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3544     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 39975    |\n",
      "|    total_timesteps  | 110672   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.556    |\n",
      "|    n_updates        | 27642    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55.3     |\n",
      "|    ep_rew_mean      | 55.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3548     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40028    |\n",
      "|    total_timesteps  | 110835   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0083   |\n",
      "|    n_updates        | 27683    |\n",
      "----------------------------------\n",
      "Num timesteps: 110873\n",
      "Best mean reward: 49.66 - Last mean reward per episode: 49.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55.6     |\n",
      "|    ep_rew_mean      | 55.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3552     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40099    |\n",
      "|    total_timesteps  | 111070   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.603    |\n",
      "|    n_updates        | 27742    |\n",
      "----------------------------------\n",
      "Num timesteps: 111233\n",
      "Best mean reward: 49.93 - Last mean reward per episode: 50.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.6     |\n",
      "|    ep_rew_mean      | 56.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3556     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40171    |\n",
      "|    total_timesteps  | 111299   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.454    |\n",
      "|    n_updates        | 27799    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.5     |\n",
      "|    ep_rew_mean      | 56.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3560     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40253    |\n",
      "|    total_timesteps  | 111568   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0266   |\n",
      "|    n_updates        | 27866    |\n",
      "----------------------------------\n",
      "Num timesteps: 111593\n",
      "Best mean reward: 50.09 - Last mean reward per episode: 50.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.6     |\n",
      "|    ep_rew_mean      | 57.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3564     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40344    |\n",
      "|    total_timesteps  | 111861   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0953   |\n",
      "|    n_updates        | 27940    |\n",
      "----------------------------------\n",
      "Num timesteps: 111953\n",
      "Best mean reward: 50.47 - Last mean reward per episode: 50.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 57.9     |\n",
      "|    ep_rew_mean      | 57.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3568     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40422    |\n",
      "|    total_timesteps  | 112102   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.476    |\n",
      "|    n_updates        | 28000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.6     |\n",
      "|    ep_rew_mean      | 56.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3572     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40466    |\n",
      "|    total_timesteps  | 112231   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.278    |\n",
      "|    n_updates        | 28032    |\n",
      "----------------------------------\n",
      "Num timesteps: 112313\n",
      "Best mean reward: 50.74 - Last mean reward per episode: 50.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.5     |\n",
      "|    ep_rew_mean      | 56.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3576     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40543    |\n",
      "|    total_timesteps  | 112471   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.126    |\n",
      "|    n_updates        | 28092    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.4     |\n",
      "|    ep_rew_mean      | 56.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3580     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40601    |\n",
      "|    total_timesteps  | 112658   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0551   |\n",
      "|    n_updates        | 28139    |\n",
      "----------------------------------\n",
      "Num timesteps: 112673\n",
      "Best mean reward: 50.97 - Last mean reward per episode: 51.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.8     |\n",
      "|    ep_rew_mean      | 56.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3584     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40676    |\n",
      "|    total_timesteps  | 112883   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0152   |\n",
      "|    n_updates        | 28195    |\n",
      "----------------------------------\n",
      "Num timesteps: 113033\n",
      "Best mean reward: 51.10 - Last mean reward per episode: 51.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.9     |\n",
      "|    ep_rew_mean      | 56.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3588     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40745    |\n",
      "|    total_timesteps  | 113101   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.182    |\n",
      "|    n_updates        | 28250    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.7     |\n",
      "|    ep_rew_mean      | 56.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3592     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40807    |\n",
      "|    total_timesteps  | 113280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 28294    |\n",
      "----------------------------------\n",
      "Num timesteps: 113393\n",
      "Best mean reward: 51.48 - Last mean reward per episode: 51.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.3     |\n",
      "|    ep_rew_mean      | 56.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3596     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40869    |\n",
      "|    total_timesteps  | 113468   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0403   |\n",
      "|    n_updates        | 28341    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.7     |\n",
      "|    ep_rew_mean      | 56.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3600     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40946    |\n",
      "|    total_timesteps  | 113718   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.319    |\n",
      "|    n_updates        | 28404    |\n",
      "----------------------------------\n",
      "Num timesteps: 113753\n",
      "Best mean reward: 51.48 - Last mean reward per episode: 51.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55.5     |\n",
      "|    ep_rew_mean      | 55.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3604     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 40985    |\n",
      "|    total_timesteps  | 113829   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0081   |\n",
      "|    n_updates        | 28432    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55.6     |\n",
      "|    ep_rew_mean      | 55.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3608     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41042    |\n",
      "|    total_timesteps  | 114021   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0457   |\n",
      "|    n_updates        | 28480    |\n",
      "----------------------------------\n",
      "Num timesteps: 114113\n",
      "Best mean reward: 51.77 - Last mean reward per episode: 51.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55.8     |\n",
      "|    ep_rew_mean      | 55.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3612     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41121    |\n",
      "|    total_timesteps  | 114266   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.037    |\n",
      "|    n_updates        | 28541    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.8     |\n",
      "|    ep_rew_mean      | 54.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3616     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41178    |\n",
      "|    total_timesteps  | 114463   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0284   |\n",
      "|    n_updates        | 28590    |\n",
      "----------------------------------\n",
      "Num timesteps: 114473\n",
      "Best mean reward: 51.77 - Last mean reward per episode: 51.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.2     |\n",
      "|    ep_rew_mean      | 56.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3620     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41266    |\n",
      "|    total_timesteps  | 114782   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0193   |\n",
      "|    n_updates        | 28670    |\n",
      "----------------------------------\n",
      "Num timesteps: 114833\n",
      "Best mean reward: 51.77 - Last mean reward per episode: 51.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55.8     |\n",
      "|    ep_rew_mean      | 55.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3624     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41336    |\n",
      "|    total_timesteps  | 115002   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0573   |\n",
      "|    n_updates        | 28725    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.6     |\n",
      "|    ep_rew_mean      | 54.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3628     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41397    |\n",
      "|    total_timesteps  | 115181   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0159   |\n",
      "|    n_updates        | 28770    |\n",
      "----------------------------------\n",
      "Num timesteps: 115193\n",
      "Best mean reward: 51.91 - Last mean reward per episode: 52.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.8     |\n",
      "|    ep_rew_mean      | 54.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3632     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41455    |\n",
      "|    total_timesteps  | 115356   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0535   |\n",
      "|    n_updates        | 28813    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.6     |\n",
      "|    ep_rew_mean      | 53.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3636     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41510    |\n",
      "|    total_timesteps  | 115537   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0149   |\n",
      "|    n_updates        | 28859    |\n",
      "----------------------------------\n",
      "Num timesteps: 115553\n",
      "Best mean reward: 52.19 - Last mean reward per episode: 52.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.6     |\n",
      "|    ep_rew_mean      | 52.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3640     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41571    |\n",
      "|    total_timesteps  | 115741   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0222   |\n",
      "|    n_updates        | 28910    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.1     |\n",
      "|    ep_rew_mean      | 52.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3644     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41622    |\n",
      "|    total_timesteps  | 115882   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0596   |\n",
      "|    n_updates        | 28945    |\n",
      "----------------------------------\n",
      "Num timesteps: 115913\n",
      "Best mean reward: 52.21 - Last mean reward per episode: 52.27\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.7     |\n",
      "|    ep_rew_mean      | 51.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3648     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41665    |\n",
      "|    total_timesteps  | 116006   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.309    |\n",
      "|    n_updates        | 28976    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.2     |\n",
      "|    ep_rew_mean      | 51.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3652     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41723    |\n",
      "|    total_timesteps  | 116192   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0885   |\n",
      "|    n_updates        | 29022    |\n",
      "----------------------------------\n",
      "Num timesteps: 116273\n",
      "Best mean reward: 52.27 - Last mean reward per episode: 52.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.8     |\n",
      "|    ep_rew_mean      | 51.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3656     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41806    |\n",
      "|    total_timesteps  | 116482   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.277    |\n",
      "|    n_updates        | 29095    |\n",
      "----------------------------------\n",
      "Num timesteps: 116633\n",
      "Best mean reward: 52.39 - Last mean reward per episode: 52.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.7     |\n",
      "|    ep_rew_mean      | 50.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3660     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41855    |\n",
      "|    total_timesteps  | 116638   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.271    |\n",
      "|    n_updates        | 29134    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.5     |\n",
      "|    ep_rew_mean      | 49.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3664     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41909    |\n",
      "|    total_timesteps  | 116808   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.182    |\n",
      "|    n_updates        | 29176    |\n",
      "----------------------------------\n",
      "Num timesteps: 116993\n",
      "Best mean reward: 52.79 - Last mean reward per episode: 52.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.7     |\n",
      "|    ep_rew_mean      | 49.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3668     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 41988    |\n",
      "|    total_timesteps  | 117069   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.366    |\n",
      "|    n_updates        | 29242    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.7     |\n",
      "|    ep_rew_mean      | 49.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3672     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42035    |\n",
      "|    total_timesteps  | 117201   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.453    |\n",
      "|    n_updates        | 29275    |\n",
      "----------------------------------\n",
      "Num timesteps: 117353\n",
      "Best mean reward: 52.89 - Last mean reward per episode: 52.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.7     |\n",
      "|    ep_rew_mean      | 49.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3676     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42108    |\n",
      "|    total_timesteps  | 117442   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.268    |\n",
      "|    n_updates        | 29335    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.9     |\n",
      "|    ep_rew_mean      | 49.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3680     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42176    |\n",
      "|    total_timesteps  | 117648   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.23     |\n",
      "|    n_updates        | 29386    |\n",
      "----------------------------------\n",
      "Num timesteps: 117713\n",
      "Best mean reward: 52.95 - Last mean reward per episode: 53.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.5     |\n",
      "|    ep_rew_mean      | 49.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3684     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42235    |\n",
      "|    total_timesteps  | 117831   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0784   |\n",
      "|    n_updates        | 29432    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.4     |\n",
      "|    ep_rew_mean      | 49.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3688     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42297    |\n",
      "|    total_timesteps  | 118040   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.366    |\n",
      "|    n_updates        | 29484    |\n",
      "----------------------------------\n",
      "Num timesteps: 118073\n",
      "Best mean reward: 53.08 - Last mean reward per episode: 53.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.8     |\n",
      "|    ep_rew_mean      | 49.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3692     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42366    |\n",
      "|    total_timesteps  | 118257   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 29539    |\n",
      "----------------------------------\n",
      "Num timesteps: 118433\n",
      "Best mean reward: 53.36 - Last mean reward per episode: 53.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.8     |\n",
      "|    ep_rew_mean      | 50.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3696     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42461    |\n",
      "|    total_timesteps  | 118551   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0547   |\n",
      "|    n_updates        | 29612    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.7     |\n",
      "|    ep_rew_mean      | 50.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3700     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42531    |\n",
      "|    total_timesteps  | 118788   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.437    |\n",
      "|    n_updates        | 29671    |\n",
      "----------------------------------\n",
      "Num timesteps: 118793\n",
      "Best mean reward: 53.48 - Last mean reward per episode: 53.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.3     |\n",
      "|    ep_rew_mean      | 52.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3704     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42616    |\n",
      "|    total_timesteps  | 119063   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.124    |\n",
      "|    n_updates        | 29740    |\n",
      "----------------------------------\n",
      "Num timesteps: 119153\n",
      "Best mean reward: 53.93 - Last mean reward per episode: 54.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.6     |\n",
      "|    ep_rew_mean      | 52.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3708     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42681    |\n",
      "|    total_timesteps  | 119283   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0631   |\n",
      "|    n_updates        | 29795    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.5     |\n",
      "|    ep_rew_mean      | 51.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3712     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42721    |\n",
      "|    total_timesteps  | 119418   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0339   |\n",
      "|    n_updates        | 29829    |\n",
      "----------------------------------\n",
      "Num timesteps: 119513\n",
      "Best mean reward: 54.24 - Last mean reward per episode: 54.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.3     |\n",
      "|    ep_rew_mean      | 51.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3716     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42776    |\n",
      "|    total_timesteps  | 119595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 29873    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50       |\n",
      "|    ep_rew_mean      | 50       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3720     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42836    |\n",
      "|    total_timesteps  | 119785   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.692    |\n",
      "|    n_updates        | 29921    |\n",
      "----------------------------------\n",
      "Num timesteps: 119873\n",
      "Best mean reward: 54.40 - Last mean reward per episode: 54.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.8     |\n",
      "|    ep_rew_mean      | 49.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3724     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42898    |\n",
      "|    total_timesteps  | 119980   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0354   |\n",
      "|    n_updates        | 29969    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.5     |\n",
      "|    ep_rew_mean      | 50.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3728     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 42983    |\n",
      "|    total_timesteps  | 120231   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.646    |\n",
      "|    n_updates        | 30032    |\n",
      "----------------------------------\n",
      "Num timesteps: 120233\n",
      "Best mean reward: 54.51 - Last mean reward per episode: 54.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.3     |\n",
      "|    ep_rew_mean      | 51.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3732     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43067    |\n",
      "|    total_timesteps  | 120483   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.266    |\n",
      "|    n_updates        | 30095    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.3     |\n",
      "|    ep_rew_mean      | 50.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3736     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43101    |\n",
      "|    total_timesteps  | 120570   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00897  |\n",
      "|    n_updates        | 30117    |\n",
      "----------------------------------\n",
      "Num timesteps: 120593\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 54.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.3     |\n",
      "|    ep_rew_mean      | 49.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3740     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43141    |\n",
      "|    total_timesteps  | 120669   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00641  |\n",
      "|    n_updates        | 30142    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49       |\n",
      "|    ep_rew_mean      | 49       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3744     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43182    |\n",
      "|    total_timesteps  | 120778   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.543    |\n",
      "|    n_updates        | 30169    |\n",
      "----------------------------------\n",
      "Num timesteps: 120953\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 53.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.8     |\n",
      "|    ep_rew_mean      | 49.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3748     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43253    |\n",
      "|    total_timesteps  | 120983   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0235   |\n",
      "|    n_updates        | 30220    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.8     |\n",
      "|    ep_rew_mean      | 49.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3752     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43322    |\n",
      "|    total_timesteps  | 121171   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0084   |\n",
      "|    n_updates        | 30267    |\n",
      "----------------------------------\n",
      "Num timesteps: 121313\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 53.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49       |\n",
      "|    ep_rew_mean      | 49       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3756     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43396    |\n",
      "|    total_timesteps  | 121379   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.367    |\n",
      "|    n_updates        | 30319    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.7     |\n",
      "|    ep_rew_mean      | 49.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3760     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43473    |\n",
      "|    total_timesteps  | 121612   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.505    |\n",
      "|    n_updates        | 30377    |\n",
      "----------------------------------\n",
      "Num timesteps: 121673\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 53.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.1     |\n",
      "|    ep_rew_mean      | 49.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3764     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43515    |\n",
      "|    total_timesteps  | 121720   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.29     |\n",
      "|    n_updates        | 30404    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3768     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43550    |\n",
      "|    total_timesteps  | 121807   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00974  |\n",
      "|    n_updates        | 30426    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48       |\n",
      "|    ep_rew_mean      | 48       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3772     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43618    |\n",
      "|    total_timesteps  | 122005   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0734   |\n",
      "|    n_updates        | 30476    |\n",
      "----------------------------------\n",
      "Num timesteps: 122033\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 52.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.9     |\n",
      "|    ep_rew_mean      | 47.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3776     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43694    |\n",
      "|    total_timesteps  | 122227   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 30531    |\n",
      "----------------------------------\n",
      "Num timesteps: 122393\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 52.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.7     |\n",
      "|    ep_rew_mean      | 47.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3780     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43758    |\n",
      "|    total_timesteps  | 122419   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0151   |\n",
      "|    n_updates        | 30579    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.8     |\n",
      "|    ep_rew_mean      | 47.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3784     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43821    |\n",
      "|    total_timesteps  | 122615   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 30628    |\n",
      "----------------------------------\n",
      "Num timesteps: 122753\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 52.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.7     |\n",
      "|    ep_rew_mean      | 47.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3788     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43893    |\n",
      "|    total_timesteps  | 122814   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.031    |\n",
      "|    n_updates        | 30678    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.5     |\n",
      "|    ep_rew_mean      | 47.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3792     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 43966    |\n",
      "|    total_timesteps  | 123012   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0196   |\n",
      "|    n_updates        | 30727    |\n",
      "----------------------------------\n",
      "Num timesteps: 123113\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.4     |\n",
      "|    ep_rew_mean      | 46.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3796     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44029    |\n",
      "|    total_timesteps  | 123186   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.107    |\n",
      "|    n_updates        | 30771    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.9     |\n",
      "|    ep_rew_mean      | 44.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3800     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44064    |\n",
      "|    total_timesteps  | 123277   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.228    |\n",
      "|    n_updates        | 30794    |\n",
      "----------------------------------\n",
      "Num timesteps: 123473\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.1     |\n",
      "|    ep_rew_mean      | 44.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3804     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44129    |\n",
      "|    total_timesteps  | 123476   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.249    |\n",
      "|    n_updates        | 30843    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.8     |\n",
      "|    ep_rew_mean      | 44.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3808     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44227    |\n",
      "|    total_timesteps  | 123765   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.228    |\n",
      "|    n_updates        | 30916    |\n",
      "----------------------------------\n",
      "Num timesteps: 123833\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.5     |\n",
      "|    ep_rew_mean      | 45.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3812     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44296    |\n",
      "|    total_timesteps  | 123967   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0142   |\n",
      "|    n_updates        | 30966    |\n",
      "----------------------------------\n",
      "Num timesteps: 124193\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3816     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44375    |\n",
      "|    total_timesteps  | 124204   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.319    |\n",
      "|    n_updates        | 31025    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47       |\n",
      "|    ep_rew_mean      | 47       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3820     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44472    |\n",
      "|    total_timesteps  | 124480   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0562   |\n",
      "|    n_updates        | 31094    |\n",
      "----------------------------------\n",
      "Num timesteps: 124553\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.7     |\n",
      "|    ep_rew_mean      | 46.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3824     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44533    |\n",
      "|    total_timesteps  | 124653   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0137   |\n",
      "|    n_updates        | 31138    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.1     |\n",
      "|    ep_rew_mean      | 45.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3828     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44570    |\n",
      "|    total_timesteps  | 124746   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0617   |\n",
      "|    n_updates        | 31161    |\n",
      "----------------------------------\n",
      "Num timesteps: 124913\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.3     |\n",
      "|    ep_rew_mean      | 44.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3832     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44635    |\n",
      "|    total_timesteps  | 124916   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0282   |\n",
      "|    n_updates        | 31203    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.5     |\n",
      "|    ep_rew_mean      | 45.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3836     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44711    |\n",
      "|    total_timesteps  | 125120   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.535    |\n",
      "|    n_updates        | 31254    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.6     |\n",
      "|    ep_rew_mean      | 45.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3840     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44751    |\n",
      "|    total_timesteps  | 125229   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.444    |\n",
      "|    n_updates        | 31282    |\n",
      "----------------------------------\n",
      "Num timesteps: 125273\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.6     |\n",
      "|    ep_rew_mean      | 46.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3844     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44824    |\n",
      "|    total_timesteps  | 125441   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0136   |\n",
      "|    n_updates        | 31335    |\n",
      "----------------------------------\n",
      "Num timesteps: 125633\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.2     |\n",
      "|    ep_rew_mean      | 47.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3848     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44906    |\n",
      "|    total_timesteps  | 125699   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.125    |\n",
      "|    n_updates        | 31399    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.7     |\n",
      "|    ep_rew_mean      | 47.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3852     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 44990    |\n",
      "|    total_timesteps  | 125937   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0235   |\n",
      "|    n_updates        | 31459    |\n",
      "----------------------------------\n",
      "Num timesteps: 125993\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.3     |\n",
      "|    ep_rew_mean      | 47.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3856     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45059    |\n",
      "|    total_timesteps  | 126113   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0472   |\n",
      "|    n_updates        | 31503    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.6     |\n",
      "|    ep_rew_mean      | 46.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3860     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45117    |\n",
      "|    total_timesteps  | 126273   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.398    |\n",
      "|    n_updates        | 31543    |\n",
      "----------------------------------\n",
      "Num timesteps: 126353\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.6     |\n",
      "|    ep_rew_mean      | 47.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3864     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45188    |\n",
      "|    total_timesteps  | 126477   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0251   |\n",
      "|    n_updates        | 31594    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.4     |\n",
      "|    ep_rew_mean      | 48.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3868     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45250    |\n",
      "|    total_timesteps  | 126648   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.331    |\n",
      "|    n_updates        | 31636    |\n",
      "----------------------------------\n",
      "Num timesteps: 126713\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.9     |\n",
      "|    ep_rew_mean      | 47.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3872     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45301    |\n",
      "|    total_timesteps  | 126794   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.457    |\n",
      "|    n_updates        | 31673    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.6     |\n",
      "|    ep_rew_mean      | 47.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3876     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45369    |\n",
      "|    total_timesteps  | 126992   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0253   |\n",
      "|    n_updates        | 31722    |\n",
      "----------------------------------\n",
      "Num timesteps: 127073\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.5     |\n",
      "|    ep_rew_mean      | 47.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3880     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45425    |\n",
      "|    total_timesteps  | 127170   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0852   |\n",
      "|    n_updates        | 31767    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.5     |\n",
      "|    ep_rew_mean      | 47.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3884     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45488    |\n",
      "|    total_timesteps  | 127361   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.361    |\n",
      "|    n_updates        | 31815    |\n",
      "----------------------------------\n",
      "Num timesteps: 127433\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.2     |\n",
      "|    ep_rew_mean      | 47.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3888     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45544    |\n",
      "|    total_timesteps  | 127533   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0404   |\n",
      "|    n_updates        | 31858    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.2     |\n",
      "|    ep_rew_mean      | 47.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3892     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45617    |\n",
      "|    total_timesteps  | 127735   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.271    |\n",
      "|    n_updates        | 31908    |\n",
      "----------------------------------\n",
      "Num timesteps: 127793\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.3     |\n",
      "|    ep_rew_mean      | 47.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3896     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45685    |\n",
      "|    total_timesteps  | 127914   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0105   |\n",
      "|    n_updates        | 31953    |\n",
      "----------------------------------\n",
      "Num timesteps: 128153\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.5     |\n",
      "|    ep_rew_mean      | 49.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3900     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45786    |\n",
      "|    total_timesteps  | 128224   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 32030    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.1     |\n",
      "|    ep_rew_mean      | 50.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3904     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45876    |\n",
      "|    total_timesteps  | 128490   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.32     |\n",
      "|    n_updates        | 32097    |\n",
      "----------------------------------\n",
      "Num timesteps: 128513\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.6     |\n",
      "|    ep_rew_mean      | 49.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3908     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 45953    |\n",
      "|    total_timesteps  | 128730   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.745    |\n",
      "|    n_updates        | 32157    |\n",
      "----------------------------------\n",
      "Num timesteps: 128873\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.9     |\n",
      "|    ep_rew_mean      | 49.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3912     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46026    |\n",
      "|    total_timesteps  | 128961   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0195   |\n",
      "|    n_updates        | 32215    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.9     |\n",
      "|    ep_rew_mean      | 49.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3916     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46102    |\n",
      "|    total_timesteps  | 129189   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0406   |\n",
      "|    n_updates        | 32272    |\n",
      "----------------------------------\n",
      "Num timesteps: 129233\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.7     |\n",
      "|    ep_rew_mean      | 49.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3920     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46190    |\n",
      "|    total_timesteps  | 129451   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.296    |\n",
      "|    n_updates        | 32337    |\n",
      "----------------------------------\n",
      "Num timesteps: 129593\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.5     |\n",
      "|    ep_rew_mean      | 50.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3924     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46278    |\n",
      "|    total_timesteps  | 129700   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 32399    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.3     |\n",
      "|    ep_rew_mean      | 51.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3928     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46340    |\n",
      "|    total_timesteps  | 129873   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.37     |\n",
      "|    n_updates        | 32443    |\n",
      "----------------------------------\n",
      "Num timesteps: 129953\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.8     |\n",
      "|    ep_rew_mean      | 51.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3932     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46415    |\n",
      "|    total_timesteps  | 130092   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0584   |\n",
      "|    n_updates        | 32497    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.6     |\n",
      "|    ep_rew_mean      | 51.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3936     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46472    |\n",
      "|    total_timesteps  | 130278   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0141   |\n",
      "|    n_updates        | 32544    |\n",
      "----------------------------------\n",
      "Num timesteps: 130313\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.3     |\n",
      "|    ep_rew_mean      | 52.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3940     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46532    |\n",
      "|    total_timesteps  | 130460   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0107   |\n",
      "|    n_updates        | 32589    |\n",
      "----------------------------------\n",
      "Num timesteps: 130673\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.5     |\n",
      "|    ep_rew_mean      | 52.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3944     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46605    |\n",
      "|    total_timesteps  | 130692   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 32647    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.4     |\n",
      "|    ep_rew_mean      | 52.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3948     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46680    |\n",
      "|    total_timesteps  | 130935   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 32708    |\n",
      "----------------------------------\n",
      "Num timesteps: 131033\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.2     |\n",
      "|    ep_rew_mean      | 52.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3952     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46747    |\n",
      "|    total_timesteps  | 131161   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 32765    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.4     |\n",
      "|    ep_rew_mean      | 52.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3956     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46814    |\n",
      "|    total_timesteps  | 131354   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00937  |\n",
      "|    n_updates        | 32813    |\n",
      "----------------------------------\n",
      "Num timesteps: 131393\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.1     |\n",
      "|    ep_rew_mean      | 52.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3960     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46855    |\n",
      "|    total_timesteps  | 131479   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0378   |\n",
      "|    n_updates        | 32844    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.4     |\n",
      "|    ep_rew_mean      | 52.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3964     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46927    |\n",
      "|    total_timesteps  | 131719   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0597   |\n",
      "|    n_updates        | 32904    |\n",
      "----------------------------------\n",
      "Num timesteps: 131753\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.6     |\n",
      "|    ep_rew_mean      | 52.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3968     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 46989    |\n",
      "|    total_timesteps  | 131908   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 32951    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.1     |\n",
      "|    ep_rew_mean      | 53.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3972     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47054    |\n",
      "|    total_timesteps  | 132106   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.334    |\n",
      "|    n_updates        | 33001    |\n",
      "----------------------------------\n",
      "Num timesteps: 132113\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.4     |\n",
      "|    ep_rew_mean      | 52.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3976     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47093    |\n",
      "|    total_timesteps  | 132230   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00778  |\n",
      "|    n_updates        | 33032    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.3     |\n",
      "|    ep_rew_mean      | 52.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3980     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47153    |\n",
      "|    total_timesteps  | 132402   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0364   |\n",
      "|    n_updates        | 33075    |\n",
      "----------------------------------\n",
      "Num timesteps: 132473\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.7     |\n",
      "|    ep_rew_mean      | 52.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3984     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47224    |\n",
      "|    total_timesteps  | 132635   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0666   |\n",
      "|    n_updates        | 33133    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53       |\n",
      "|    ep_rew_mean      | 53       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3988     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47283    |\n",
      "|    total_timesteps  | 132829   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0185   |\n",
      "|    n_updates        | 33182    |\n",
      "----------------------------------\n",
      "Num timesteps: 132833\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.8     |\n",
      "|    ep_rew_mean      | 52.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3992     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47338    |\n",
      "|    total_timesteps  | 133014   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0209   |\n",
      "|    n_updates        | 33228    |\n",
      "----------------------------------\n",
      "Num timesteps: 133193\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.14\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.4     |\n",
      "|    ep_rew_mean      | 53.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3996     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47409    |\n",
      "|    total_timesteps  | 133254   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0499   |\n",
      "|    n_updates        | 33288    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52       |\n",
      "|    ep_rew_mean      | 52       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4000     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47464    |\n",
      "|    total_timesteps  | 133424   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.317    |\n",
      "|    n_updates        | 33330    |\n",
      "----------------------------------\n",
      "Num timesteps: 133553\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.3     |\n",
      "|    ep_rew_mean      | 51.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4004     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47528    |\n",
      "|    total_timesteps  | 133618   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.198    |\n",
      "|    n_updates        | 33379    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.9     |\n",
      "|    ep_rew_mean      | 50.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4008     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47589    |\n",
      "|    total_timesteps  | 133818   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0267   |\n",
      "|    n_updates        | 33429    |\n",
      "----------------------------------\n",
      "Num timesteps: 133913\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.2     |\n",
      "|    ep_rew_mean      | 50.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4012     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47639    |\n",
      "|    total_timesteps  | 133985   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0154   |\n",
      "|    n_updates        | 33471    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.7     |\n",
      "|    ep_rew_mean      | 49.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4016     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47699    |\n",
      "|    total_timesteps  | 134162   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0741   |\n",
      "|    n_updates        | 33515    |\n",
      "----------------------------------\n",
      "Num timesteps: 134273\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.23\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.7     |\n",
      "|    ep_rew_mean      | 49.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4020     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47783    |\n",
      "|    total_timesteps  | 134418   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.043    |\n",
      "|    n_updates        | 33579    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.3     |\n",
      "|    ep_rew_mean      | 49.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4024     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47849    |\n",
      "|    total_timesteps  | 134631   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0237   |\n",
      "|    n_updates        | 33632    |\n",
      "----------------------------------\n",
      "Num timesteps: 134633\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50       |\n",
      "|    ep_rew_mean      | 50       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4028     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 47927    |\n",
      "|    total_timesteps  | 134871   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0193   |\n",
      "|    n_updates        | 33692    |\n",
      "----------------------------------\n",
      "Num timesteps: 134993\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.2     |\n",
      "|    ep_rew_mean      | 50.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4032     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48000    |\n",
      "|    total_timesteps  | 135110   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.072    |\n",
      "|    n_updates        | 33752    |\n",
      "----------------------------------\n",
      "Num timesteps: 135353\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.8     |\n",
      "|    ep_rew_mean      | 50.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4036     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48080    |\n",
      "|    total_timesteps  | 135356   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.351    |\n",
      "|    n_updates        | 33813    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.3     |\n",
      "|    ep_rew_mean      | 50.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4040     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48122    |\n",
      "|    total_timesteps  | 135487   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0257   |\n",
      "|    n_updates        | 33846    |\n",
      "----------------------------------\n",
      "Num timesteps: 135713\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.2     |\n",
      "|    ep_rew_mean      | 50.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4044     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48199    |\n",
      "|    total_timesteps  | 135715   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.053    |\n",
      "|    n_updates        | 33903    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.6     |\n",
      "|    ep_rew_mean      | 49.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4048     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48262    |\n",
      "|    total_timesteps  | 135899   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.4      |\n",
      "|    n_updates        | 33949    |\n",
      "----------------------------------\n",
      "Num timesteps: 136073\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.53\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.4     |\n",
      "|    ep_rew_mean      | 49.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4052     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48323    |\n",
      "|    total_timesteps  | 136105   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0203   |\n",
      "|    n_updates        | 34001    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.2     |\n",
      "|    ep_rew_mean      | 50.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4056     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48410    |\n",
      "|    total_timesteps  | 136372   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.306    |\n",
      "|    n_updates        | 34067    |\n",
      "----------------------------------\n",
      "Num timesteps: 136433\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.2     |\n",
      "|    ep_rew_mean      | 50.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4060     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48452    |\n",
      "|    total_timesteps  | 136497   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.69     |\n",
      "|    n_updates        | 34099    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.8     |\n",
      "|    ep_rew_mean      | 48.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4064     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48491    |\n",
      "|    total_timesteps  | 136596   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.309    |\n",
      "|    n_updates        | 34123    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.5     |\n",
      "|    ep_rew_mean      | 48.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4068     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48552    |\n",
      "|    total_timesteps  | 136760   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 34164    |\n",
      "----------------------------------\n",
      "Num timesteps: 136793\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.3     |\n",
      "|    ep_rew_mean      | 49.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4072     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48633    |\n",
      "|    total_timesteps  | 137034   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.385    |\n",
      "|    n_updates        | 34233    |\n",
      "----------------------------------\n",
      "Num timesteps: 137153\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.9     |\n",
      "|    ep_rew_mean      | 49.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4076     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48688    |\n",
      "|    total_timesteps  | 137216   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.203    |\n",
      "|    n_updates        | 34278    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51       |\n",
      "|    ep_rew_mean      | 51       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4080     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48780    |\n",
      "|    total_timesteps  | 137505   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0549   |\n",
      "|    n_updates        | 34351    |\n",
      "----------------------------------\n",
      "Num timesteps: 137513\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.3     |\n",
      "|    ep_rew_mean      | 51.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4084     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48856    |\n",
      "|    total_timesteps  | 137765   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0775   |\n",
      "|    n_updates        | 34416    |\n",
      "----------------------------------\n",
      "Num timesteps: 137873\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.4     |\n",
      "|    ep_rew_mean      | 51.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4088     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48922    |\n",
      "|    total_timesteps  | 137972   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0158   |\n",
      "|    n_updates        | 34467    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.6     |\n",
      "|    ep_rew_mean      | 51.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4092     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 48985    |\n",
      "|    total_timesteps  | 138171   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.436    |\n",
      "|    n_updates        | 34517    |\n",
      "----------------------------------\n",
      "Num timesteps: 138233\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.6     |\n",
      "|    ep_rew_mean      | 51.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4096     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49061    |\n",
      "|    total_timesteps  | 138417   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0143   |\n",
      "|    n_updates        | 34579    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.6     |\n",
      "|    ep_rew_mean      | 51.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4100     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49114    |\n",
      "|    total_timesteps  | 138584   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0851   |\n",
      "|    n_updates        | 34620    |\n",
      "----------------------------------\n",
      "Num timesteps: 138593\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.9     |\n",
      "|    ep_rew_mean      | 51.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4104     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49184    |\n",
      "|    total_timesteps  | 138803   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0124   |\n",
      "|    n_updates        | 34675    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.9     |\n",
      "|    ep_rew_mean      | 50.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4108     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49222    |\n",
      "|    total_timesteps  | 138907   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00825  |\n",
      "|    n_updates        | 34701    |\n",
      "----------------------------------\n",
      "Num timesteps: 138953\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.4     |\n",
      "|    ep_rew_mean      | 51.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4112     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49287    |\n",
      "|    total_timesteps  | 139123   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00931  |\n",
      "|    n_updates        | 34755    |\n",
      "----------------------------------\n",
      "Num timesteps: 139313\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.7     |\n",
      "|    ep_rew_mean      | 51.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4116     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49354    |\n",
      "|    total_timesteps  | 139333   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0181   |\n",
      "|    n_updates        | 34808    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.5     |\n",
      "|    ep_rew_mean      | 51.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4120     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49427    |\n",
      "|    total_timesteps  | 139567   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.336    |\n",
      "|    n_updates        | 34866    |\n",
      "----------------------------------\n",
      "Num timesteps: 139673\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.87\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.4     |\n",
      "|    ep_rew_mean      | 52.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4124     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49521    |\n",
      "|    total_timesteps  | 139869   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0258   |\n",
      "|    n_updates        | 34942    |\n",
      "----------------------------------\n",
      "Num timesteps: 140033\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.6     |\n",
      "|    ep_rew_mean      | 51.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4128     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49573    |\n",
      "|    total_timesteps  | 140034   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 34983    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.2     |\n",
      "|    ep_rew_mean      | 51.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4132     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49637    |\n",
      "|    total_timesteps  | 140233   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0284   |\n",
      "|    n_updates        | 35033    |\n",
      "----------------------------------\n",
      "Num timesteps: 140393\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.4     |\n",
      "|    ep_rew_mean      | 50.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4136     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49694    |\n",
      "|    total_timesteps  | 140393   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.086    |\n",
      "|    n_updates        | 35073    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.8     |\n",
      "|    ep_rew_mean      | 50.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4140     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49748    |\n",
      "|    total_timesteps  | 140565   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00692  |\n",
      "|    n_updates        | 35116    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.2     |\n",
      "|    ep_rew_mean      | 50.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4144     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49797    |\n",
      "|    total_timesteps  | 140731   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.342    |\n",
      "|    n_updates        | 35157    |\n",
      "----------------------------------\n",
      "Num timesteps: 140753\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.5     |\n",
      "|    ep_rew_mean      | 51.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4148     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49888    |\n",
      "|    total_timesteps  | 141048   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 35236    |\n",
      "----------------------------------\n",
      "Num timesteps: 141113\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.2     |\n",
      "|    ep_rew_mean      | 51.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4152     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49947    |\n",
      "|    total_timesteps  | 141229   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0717   |\n",
      "|    n_updates        | 35282    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50       |\n",
      "|    ep_rew_mean      | 50       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4156     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 49989    |\n",
      "|    total_timesteps  | 141375   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0185   |\n",
      "|    n_updates        | 35318    |\n",
      "----------------------------------\n",
      "Num timesteps: 141473\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.7     |\n",
      "|    ep_rew_mean      | 50.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4160     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50054    |\n",
      "|    total_timesteps  | 141567   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.139    |\n",
      "|    n_updates        | 35366    |\n",
      "----------------------------------\n",
      "Num timesteps: 141833\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.7     |\n",
      "|    ep_rew_mean      | 52.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4164     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50149    |\n",
      "|    total_timesteps  | 141869   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.028    |\n",
      "|    n_updates        | 35442    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.5     |\n",
      "|    ep_rew_mean      | 52.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4168     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50193    |\n",
      "|    total_timesteps  | 142010   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0508   |\n",
      "|    n_updates        | 35477    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.1     |\n",
      "|    ep_rew_mean      | 51.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4172     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50236    |\n",
      "|    total_timesteps  | 142143   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0235   |\n",
      "|    n_updates        | 35510    |\n",
      "----------------------------------\n",
      "Num timesteps: 142193\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.8     |\n",
      "|    ep_rew_mean      | 50.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4176     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50285    |\n",
      "|    total_timesteps  | 142300   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0238   |\n",
      "|    n_updates        | 35549    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.1     |\n",
      "|    ep_rew_mean      | 50.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4180     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50352    |\n",
      "|    total_timesteps  | 142520   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0399   |\n",
      "|    n_updates        | 35604    |\n",
      "----------------------------------\n",
      "Num timesteps: 142553\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.8     |\n",
      "|    ep_rew_mean      | 49.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4184     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50426    |\n",
      "|    total_timesteps  | 142744   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 35660    |\n",
      "----------------------------------\n",
      "Num timesteps: 142913\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.7     |\n",
      "|    ep_rew_mean      | 49.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4188     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50482    |\n",
      "|    total_timesteps  | 142942   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0122   |\n",
      "|    n_updates        | 35710    |\n",
      "----------------------------------\n",
      "Num timesteps: 143273\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.1     |\n",
      "|    ep_rew_mean      | 51.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4192     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50580    |\n",
      "|    total_timesteps  | 143277   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.167    |\n",
      "|    n_updates        | 35794    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52       |\n",
      "|    ep_rew_mean      | 52       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4196     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50678    |\n",
      "|    total_timesteps  | 143618   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0262   |\n",
      "|    n_updates        | 35879    |\n",
      "----------------------------------\n",
      "Num timesteps: 143633\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.5     |\n",
      "|    ep_rew_mean      | 52.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4200     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50748    |\n",
      "|    total_timesteps  | 143830   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0216   |\n",
      "|    n_updates        | 35932    |\n",
      "----------------------------------\n",
      "Num timesteps: 143993\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.2     |\n",
      "|    ep_rew_mean      | 52.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4204     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50817    |\n",
      "|    total_timesteps  | 144024   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 35980    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.2     |\n",
      "|    ep_rew_mean      | 53.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4208     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50879    |\n",
      "|    total_timesteps  | 144231   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0107   |\n",
      "|    n_updates        | 36032    |\n",
      "----------------------------------\n",
      "Num timesteps: 144353\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.28\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.1     |\n",
      "|    ep_rew_mean      | 53.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4212     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 50941    |\n",
      "|    total_timesteps  | 144434   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00946  |\n",
      "|    n_updates        | 36083    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.3     |\n",
      "|    ep_rew_mean      | 53.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4216     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51013    |\n",
      "|    total_timesteps  | 144667   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.511    |\n",
      "|    n_updates        | 36141    |\n",
      "----------------------------------\n",
      "Num timesteps: 144713\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.9     |\n",
      "|    ep_rew_mean      | 52.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4220     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51081    |\n",
      "|    total_timesteps  | 144856   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.337    |\n",
      "|    n_updates        | 36188    |\n",
      "----------------------------------\n",
      "Num timesteps: 145073\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52       |\n",
      "|    ep_rew_mean      | 52       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4224     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51151    |\n",
      "|    total_timesteps  | 145074   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.436    |\n",
      "|    n_updates        | 36243    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52       |\n",
      "|    ep_rew_mean      | 52       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4228     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51201    |\n",
      "|    total_timesteps  | 145233   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0169   |\n",
      "|    n_updates        | 36283    |\n",
      "----------------------------------\n",
      "Num timesteps: 145433\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52       |\n",
      "|    ep_rew_mean      | 52       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4232     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51271    |\n",
      "|    total_timesteps  | 145434   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0359   |\n",
      "|    n_updates        | 36333    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.8     |\n",
      "|    ep_rew_mean      | 52.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4236     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51346    |\n",
      "|    total_timesteps  | 145669   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0135   |\n",
      "|    n_updates        | 36392    |\n",
      "----------------------------------\n",
      "Num timesteps: 145793\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 52.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.5     |\n",
      "|    ep_rew_mean      | 53.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4240     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51423    |\n",
      "|    total_timesteps  | 145912   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00987  |\n",
      "|    n_updates        | 36452    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.2     |\n",
      "|    ep_rew_mean      | 54.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4244     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51498    |\n",
      "|    total_timesteps  | 146151   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.328    |\n",
      "|    n_updates        | 36512    |\n",
      "----------------------------------\n",
      "Num timesteps: 146153\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 52.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52       |\n",
      "|    ep_rew_mean      | 52       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4248     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51537    |\n",
      "|    total_timesteps  | 146252   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.12     |\n",
      "|    n_updates        | 36537    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.4     |\n",
      "|    ep_rew_mean      | 52.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4252     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51608    |\n",
      "|    total_timesteps  | 146465   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0104   |\n",
      "|    n_updates        | 36591    |\n",
      "----------------------------------\n",
      "Num timesteps: 146513\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 52.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.3     |\n",
      "|    ep_rew_mean      | 53.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4256     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51681    |\n",
      "|    total_timesteps  | 146702   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0197   |\n",
      "|    n_updates        | 36650    |\n",
      "----------------------------------\n",
      "Num timesteps: 146873\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 52.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.4     |\n",
      "|    ep_rew_mean      | 53.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4260     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51749    |\n",
      "|    total_timesteps  | 146910   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00801  |\n",
      "|    n_updates        | 36702    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.6     |\n",
      "|    ep_rew_mean      | 51.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4264     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51788    |\n",
      "|    total_timesteps  | 147026   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0224   |\n",
      "|    n_updates        | 36731    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52       |\n",
      "|    ep_rew_mean      | 52       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4268     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51854    |\n",
      "|    total_timesteps  | 147212   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 36777    |\n",
      "----------------------------------\n",
      "Num timesteps: 147233\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.7     |\n",
      "|    ep_rew_mean      | 52.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4272     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51919    |\n",
      "|    total_timesteps  | 147414   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.258    |\n",
      "|    n_updates        | 36828    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.6     |\n",
      "|    ep_rew_mean      | 52.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4276     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 51969    |\n",
      "|    total_timesteps  | 147558   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0596   |\n",
      "|    n_updates        | 36864    |\n",
      "----------------------------------\n",
      "Num timesteps: 147593\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.7     |\n",
      "|    ep_rew_mean      | 52.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4280     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52041    |\n",
      "|    total_timesteps  | 147792   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.149    |\n",
      "|    n_updates        | 36922    |\n",
      "----------------------------------\n",
      "Num timesteps: 147953\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.8     |\n",
      "|    ep_rew_mean      | 52.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4284     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52112    |\n",
      "|    total_timesteps  | 148028   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.654    |\n",
      "|    n_updates        | 36981    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.8     |\n",
      "|    ep_rew_mean      | 52.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4288     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52176    |\n",
      "|    total_timesteps  | 148223   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.255    |\n",
      "|    n_updates        | 37030    |\n",
      "----------------------------------\n",
      "Num timesteps: 148313\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51       |\n",
      "|    ep_rew_mean      | 51       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4292     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52226    |\n",
      "|    total_timesteps  | 148372   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0122   |\n",
      "|    n_updates        | 37067    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.1     |\n",
      "|    ep_rew_mean      | 49.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4296     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52278    |\n",
      "|    total_timesteps  | 148533   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.48     |\n",
      "|    n_updates        | 37108    |\n",
      "----------------------------------\n",
      "Num timesteps: 148673\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.8     |\n",
      "|    ep_rew_mean      | 48.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4300     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52336    |\n",
      "|    total_timesteps  | 148706   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.106    |\n",
      "|    n_updates        | 37151    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.5     |\n",
      "|    ep_rew_mean      | 48.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4304     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52395    |\n",
      "|    total_timesteps  | 148879   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0297   |\n",
      "|    n_updates        | 37194    |\n",
      "----------------------------------\n",
      "Num timesteps: 149033\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.4     |\n",
      "|    ep_rew_mean      | 48.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4308     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52449    |\n",
      "|    total_timesteps  | 149071   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00365  |\n",
      "|    n_updates        | 37242    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.8     |\n",
      "|    ep_rew_mean      | 47.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4312     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52496    |\n",
      "|    total_timesteps  | 149213   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.389    |\n",
      "|    n_updates        | 37278    |\n",
      "----------------------------------\n",
      "Num timesteps: 149393\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.28\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4316     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52557    |\n",
      "|    total_timesteps  | 149402   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 37325    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.9     |\n",
      "|    ep_rew_mean      | 46.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4320     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52602    |\n",
      "|    total_timesteps  | 149550   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00457  |\n",
      "|    n_updates        | 37362    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.7     |\n",
      "|    ep_rew_mean      | 46.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4324     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52662    |\n",
      "|    total_timesteps  | 149744   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.376    |\n",
      "|    n_updates        | 37410    |\n",
      "----------------------------------\n",
      "Num timesteps: 149753\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.7     |\n",
      "|    ep_rew_mean      | 47.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4328     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52738    |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.344    |\n",
      "|    n_updates        | 37474    |\n",
      "----------------------------------\n",
      "Num timesteps: 150113\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48       |\n",
      "|    ep_rew_mean      | 48       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4332     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52815    |\n",
      "|    total_timesteps  | 150236   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.016    |\n",
      "|    n_updates        | 37533    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.1     |\n",
      "|    ep_rew_mean      | 47.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4336     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52865    |\n",
      "|    total_timesteps  | 150377   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0296   |\n",
      "|    n_updates        | 37569    |\n",
      "----------------------------------\n",
      "Num timesteps: 150473\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.2     |\n",
      "|    ep_rew_mean      | 47.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4340     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 52942    |\n",
      "|    total_timesteps  | 150630   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0576   |\n",
      "|    n_updates        | 37632    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.7     |\n",
      "|    ep_rew_mean      | 46.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4344     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53008    |\n",
      "|    total_timesteps  | 150821   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0112   |\n",
      "|    n_updates        | 37680    |\n",
      "----------------------------------\n",
      "Num timesteps: 150833\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.9     |\n",
      "|    ep_rew_mean      | 47.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4348     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53076    |\n",
      "|    total_timesteps  | 151043   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.449    |\n",
      "|    n_updates        | 37735    |\n",
      "----------------------------------\n",
      "Num timesteps: 151193\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.6     |\n",
      "|    ep_rew_mean      | 48.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4352     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53163    |\n",
      "|    total_timesteps  | 151322   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 37805    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48       |\n",
      "|    ep_rew_mean      | 48       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4356     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53222    |\n",
      "|    total_timesteps  | 151507   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0371   |\n",
      "|    n_updates        | 37851    |\n",
      "----------------------------------\n",
      "Num timesteps: 151553\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.4     |\n",
      "|    ep_rew_mean      | 48.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4360     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53294    |\n",
      "|    total_timesteps  | 151745   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.281    |\n",
      "|    n_updates        | 37911    |\n",
      "----------------------------------\n",
      "Num timesteps: 151913\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.6     |\n",
      "|    ep_rew_mean      | 49.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4364     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53378    |\n",
      "|    total_timesteps  | 151989   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.461    |\n",
      "|    n_updates        | 37972    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50       |\n",
      "|    ep_rew_mean      | 50       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4368     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53446    |\n",
      "|    total_timesteps  | 152216   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0497   |\n",
      "|    n_updates        | 38028    |\n",
      "----------------------------------\n",
      "Num timesteps: 152273\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.5     |\n",
      "|    ep_rew_mean      | 50.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4372     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53522    |\n",
      "|    total_timesteps  | 152466   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0189   |\n",
      "|    n_updates        | 38091    |\n",
      "----------------------------------\n",
      "Num timesteps: 152633\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.6     |\n",
      "|    ep_rew_mean      | 51.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4376     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53598    |\n",
      "|    total_timesteps  | 152717   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.281    |\n",
      "|    n_updates        | 38154    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.1     |\n",
      "|    ep_rew_mean      | 51.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4380     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53658    |\n",
      "|    total_timesteps  | 152906   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00633  |\n",
      "|    n_updates        | 38201    |\n",
      "----------------------------------\n",
      "Num timesteps: 152993\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.6     |\n",
      "|    ep_rew_mean      | 50.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4384     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53712    |\n",
      "|    total_timesteps  | 153085   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0472   |\n",
      "|    n_updates        | 38246    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.2     |\n",
      "|    ep_rew_mean      | 51.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4388     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53793    |\n",
      "|    total_timesteps  | 153343   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.203    |\n",
      "|    n_updates        | 38310    |\n",
      "----------------------------------\n",
      "Num timesteps: 153353\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52       |\n",
      "|    ep_rew_mean      | 52       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4392     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53866    |\n",
      "|    total_timesteps  | 153570   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.362    |\n",
      "|    n_updates        | 38367    |\n",
      "----------------------------------\n",
      "Num timesteps: 153713\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.2     |\n",
      "|    ep_rew_mean      | 52.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4396     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53929    |\n",
      "|    total_timesteps  | 153756   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0309   |\n",
      "|    n_updates        | 38413    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.5     |\n",
      "|    ep_rew_mean      | 52.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4400     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 53997    |\n",
      "|    total_timesteps  | 153956   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0386   |\n",
      "|    n_updates        | 38463    |\n",
      "----------------------------------\n",
      "Num timesteps: 154073\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.27\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.7     |\n",
      "|    ep_rew_mean      | 52.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4404     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54062    |\n",
      "|    total_timesteps  | 154148   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00925  |\n",
      "|    n_updates        | 38511    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.5     |\n",
      "|    ep_rew_mean      | 53.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4408     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54141    |\n",
      "|    total_timesteps  | 154422   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.43     |\n",
      "|    n_updates        | 38580    |\n",
      "----------------------------------\n",
      "Num timesteps: 154433\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.6     |\n",
      "|    ep_rew_mean      | 53.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4412     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54191    |\n",
      "|    total_timesteps  | 154569   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.303    |\n",
      "|    n_updates        | 38617    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.6     |\n",
      "|    ep_rew_mean      | 53.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4416     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54257    |\n",
      "|    total_timesteps  | 154766   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0475   |\n",
      "|    n_updates        | 38666    |\n",
      "----------------------------------\n",
      "Num timesteps: 154793\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.3     |\n",
      "|    ep_rew_mean      | 54.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4420     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54326    |\n",
      "|    total_timesteps  | 154979   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.428    |\n",
      "|    n_updates        | 38719    |\n",
      "----------------------------------\n",
      "Num timesteps: 155153\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.8     |\n",
      "|    ep_rew_mean      | 54.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4424     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54401    |\n",
      "|    total_timesteps  | 155221   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.362    |\n",
      "|    n_updates        | 38780    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55       |\n",
      "|    ep_rew_mean      | 55       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4428     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54483    |\n",
      "|    total_timesteps  | 155502   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0144   |\n",
      "|    n_updates        | 38850    |\n",
      "----------------------------------\n",
      "Num timesteps: 155513\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 52.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.1     |\n",
      "|    ep_rew_mean      | 54.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4432     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54531    |\n",
      "|    total_timesteps  | 155647   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0357   |\n",
      "|    n_updates        | 38886    |\n",
      "----------------------------------\n",
      "Num timesteps: 155873\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.87\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 55.3     |\n",
      "|    ep_rew_mean      | 55.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4436     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54612    |\n",
      "|    total_timesteps  | 155904   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00463  |\n",
      "|    n_updates        | 38950    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.1     |\n",
      "|    ep_rew_mean      | 54.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4440     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54654    |\n",
      "|    total_timesteps  | 156041   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0284   |\n",
      "|    n_updates        | 38985    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53.7     |\n",
      "|    ep_rew_mean      | 53.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4444     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54702    |\n",
      "|    total_timesteps  | 156187   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0393   |\n",
      "|    n_updates        | 39021    |\n",
      "----------------------------------\n",
      "Num timesteps: 156233\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54       |\n",
      "|    ep_rew_mean      | 54       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4448     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54788    |\n",
      "|    total_timesteps  | 156438   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0203   |\n",
      "|    n_updates        | 39084    |\n",
      "----------------------------------\n",
      "Num timesteps: 156593\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.8     |\n",
      "|    ep_rew_mean      | 52.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4452     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54843    |\n",
      "|    total_timesteps  | 156603   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.252    |\n",
      "|    n_updates        | 39125    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 52.7     |\n",
      "|    ep_rew_mean      | 52.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4456     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54902    |\n",
      "|    total_timesteps  | 156781   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 39170    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.9     |\n",
      "|    ep_rew_mean      | 51.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4460     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 54954    |\n",
      "|    total_timesteps  | 156935   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0189   |\n",
      "|    n_updates        | 39208    |\n",
      "----------------------------------\n",
      "Num timesteps: 156953\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.3     |\n",
      "|    ep_rew_mean      | 51.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4464     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55015    |\n",
      "|    total_timesteps  | 157121   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.539    |\n",
      "|    n_updates        | 39255    |\n",
      "----------------------------------\n",
      "Num timesteps: 157313\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 51.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51       |\n",
      "|    ep_rew_mean      | 51       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4468     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55076    |\n",
      "|    total_timesteps  | 157313   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 39303    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.4     |\n",
      "|    ep_rew_mean      | 50.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4472     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55144    |\n",
      "|    total_timesteps  | 157508   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.649    |\n",
      "|    n_updates        | 39351    |\n",
      "----------------------------------\n",
      "Num timesteps: 157673\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.8     |\n",
      "|    ep_rew_mean      | 49.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4476     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55206    |\n",
      "|    total_timesteps  | 157692   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.252    |\n",
      "|    n_updates        | 39397    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.5     |\n",
      "|    ep_rew_mean      | 49.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4480     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55256    |\n",
      "|    total_timesteps  | 157854   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0136   |\n",
      "|    n_updates        | 39438    |\n",
      "----------------------------------\n",
      "Num timesteps: 158033\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.9     |\n",
      "|    ep_rew_mean      | 49.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4484     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55323    |\n",
      "|    total_timesteps  | 158077   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0234   |\n",
      "|    n_updates        | 39494    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.9     |\n",
      "|    ep_rew_mean      | 48.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4488     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55372    |\n",
      "|    total_timesteps  | 158231   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.428    |\n",
      "|    n_updates        | 39532    |\n",
      "----------------------------------\n",
      "Num timesteps: 158393\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.4     |\n",
      "|    ep_rew_mean      | 48.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4492     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55433    |\n",
      "|    total_timesteps  | 158407   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 39576    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4496     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55470    |\n",
      "|    total_timesteps  | 158492   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.428    |\n",
      "|    n_updates        | 39597    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.1     |\n",
      "|    ep_rew_mean      | 47.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4500     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55531    |\n",
      "|    total_timesteps  | 158662   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0311   |\n",
      "|    n_updates        | 39640    |\n",
      "----------------------------------\n",
      "Num timesteps: 158753\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.8     |\n",
      "|    ep_rew_mean      | 47.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4504     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55610    |\n",
      "|    total_timesteps  | 158926   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00812  |\n",
      "|    n_updates        | 39706    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.9     |\n",
      "|    ep_rew_mean      | 46.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4508     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55673    |\n",
      "|    total_timesteps  | 159108   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0236   |\n",
      "|    n_updates        | 39751    |\n",
      "----------------------------------\n",
      "Num timesteps: 159113\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.1     |\n",
      "|    ep_rew_mean      | 47.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4512     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55729    |\n",
      "|    total_timesteps  | 159281   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.434    |\n",
      "|    n_updates        | 39795    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.9     |\n",
      "|    ep_rew_mean      | 46.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4516     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55790    |\n",
      "|    total_timesteps  | 159454   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0137   |\n",
      "|    n_updates        | 39838    |\n",
      "----------------------------------\n",
      "Num timesteps: 159473\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.3     |\n",
      "|    ep_rew_mean      | 47.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4520     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55864    |\n",
      "|    total_timesteps  | 159707   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.015    |\n",
      "|    n_updates        | 39901    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4524     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55904    |\n",
      "|    total_timesteps  | 159829   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0232   |\n",
      "|    n_updates        | 39932    |\n",
      "----------------------------------\n",
      "Num timesteps: 159833\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.8     |\n",
      "|    ep_rew_mean      | 45.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4528     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 55984    |\n",
      "|    total_timesteps  | 160081   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0105   |\n",
      "|    n_updates        | 39995    |\n",
      "----------------------------------\n",
      "Num timesteps: 160193\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.7     |\n",
      "|    ep_rew_mean      | 45.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4532     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56034    |\n",
      "|    total_timesteps  | 160218   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0264   |\n",
      "|    n_updates        | 40029    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.2     |\n",
      "|    ep_rew_mean      | 45.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4536     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56103    |\n",
      "|    total_timesteps  | 160428   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.152    |\n",
      "|    n_updates        | 40081    |\n",
      "----------------------------------\n",
      "Num timesteps: 160553\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.5     |\n",
      "|    ep_rew_mean      | 45.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4540     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56160    |\n",
      "|    total_timesteps  | 160587   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0893   |\n",
      "|    n_updates        | 40121    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4544     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56233    |\n",
      "|    total_timesteps  | 160797   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.403    |\n",
      "|    n_updates        | 40174    |\n",
      "----------------------------------\n",
      "Num timesteps: 160913\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 50.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4548     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56319    |\n",
      "|    total_timesteps  | 161050   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.022    |\n",
      "|    n_updates        | 40237    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.4     |\n",
      "|    ep_rew_mean      | 46.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4552     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56385    |\n",
      "|    total_timesteps  | 161239   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.672    |\n",
      "|    n_updates        | 40284    |\n",
      "----------------------------------\n",
      "Num timesteps: 161273\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46       |\n",
      "|    ep_rew_mean      | 46       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4556     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56437    |\n",
      "|    total_timesteps  | 161385   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.685    |\n",
      "|    n_updates        | 40321    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.5     |\n",
      "|    ep_rew_mean      | 46.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4560     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56505    |\n",
      "|    total_timesteps  | 161583   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0313   |\n",
      "|    n_updates        | 40370    |\n",
      "----------------------------------\n",
      "Num timesteps: 161633\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.1     |\n",
      "|    ep_rew_mean      | 47.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4564     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56589    |\n",
      "|    total_timesteps  | 161830   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 40432    |\n",
      "----------------------------------\n",
      "Num timesteps: 161993\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 49.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.8     |\n",
      "|    ep_rew_mean      | 46.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4568     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56649    |\n",
      "|    total_timesteps  | 161995   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0593   |\n",
      "|    n_updates        | 40473    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.9     |\n",
      "|    ep_rew_mean      | 45.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4572     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56689    |\n",
      "|    total_timesteps  | 162097   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.443    |\n",
      "|    n_updates        | 40499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46       |\n",
      "|    ep_rew_mean      | 46       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4576     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56755    |\n",
      "|    total_timesteps  | 162293   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00542  |\n",
      "|    n_updates        | 40548    |\n",
      "----------------------------------\n",
      "Num timesteps: 162353\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.5     |\n",
      "|    ep_rew_mean      | 46.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4580     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56832    |\n",
      "|    total_timesteps  | 162508   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.085    |\n",
      "|    n_updates        | 40601    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.9     |\n",
      "|    ep_rew_mean      | 45.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4584     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56885    |\n",
      "|    total_timesteps  | 162662   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.205    |\n",
      "|    n_updates        | 40640    |\n",
      "----------------------------------\n",
      "Num timesteps: 162713\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4588     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 56951    |\n",
      "|    total_timesteps  | 162844   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0106   |\n",
      "|    n_updates        | 40685    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.7     |\n",
      "|    ep_rew_mean      | 45.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4592     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57001    |\n",
      "|    total_timesteps  | 162979   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0481   |\n",
      "|    n_updates        | 40719    |\n",
      "----------------------------------\n",
      "Num timesteps: 163073\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.7     |\n",
      "|    ep_rew_mean      | 46.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4596     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57064    |\n",
      "|    total_timesteps  | 163164   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.487    |\n",
      "|    n_updates        | 40765    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.8     |\n",
      "|    ep_rew_mean      | 46.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4600     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57125    |\n",
      "|    total_timesteps  | 163343   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0225   |\n",
      "|    n_updates        | 40810    |\n",
      "----------------------------------\n",
      "Num timesteps: 163433\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.9     |\n",
      "|    ep_rew_mean      | 45.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4604     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57191    |\n",
      "|    total_timesteps  | 163518   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00861  |\n",
      "|    n_updates        | 40854    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46       |\n",
      "|    ep_rew_mean      | 46       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4608     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57256    |\n",
      "|    total_timesteps  | 163707   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0188   |\n",
      "|    n_updates        | 40901    |\n",
      "----------------------------------\n",
      "Num timesteps: 163793\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.8     |\n",
      "|    ep_rew_mean      | 46.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4612     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57346    |\n",
      "|    total_timesteps  | 163964   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.41     |\n",
      "|    n_updates        | 40965    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.7     |\n",
      "|    ep_rew_mean      | 46.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4616     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57401    |\n",
      "|    total_timesteps  | 164122   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.216    |\n",
      "|    n_updates        | 41005    |\n",
      "----------------------------------\n",
      "Num timesteps: 164153\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.4     |\n",
      "|    ep_rew_mean      | 45.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4620     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57451    |\n",
      "|    total_timesteps  | 164249   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.442    |\n",
      "|    n_updates        | 41037    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.3     |\n",
      "|    ep_rew_mean      | 46.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4624     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57519    |\n",
      "|    total_timesteps  | 164458   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.202    |\n",
      "|    n_updates        | 41089    |\n",
      "----------------------------------\n",
      "Num timesteps: 164513\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.23\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.3     |\n",
      "|    ep_rew_mean      | 45.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4628     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57571    |\n",
      "|    total_timesteps  | 164610   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00918  |\n",
      "|    n_updates        | 41127    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.7     |\n",
      "|    ep_rew_mean      | 45.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4632     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57638    |\n",
      "|    total_timesteps  | 164789   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.487    |\n",
      "|    n_updates        | 41172    |\n",
      "----------------------------------\n",
      "Num timesteps: 164873\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.8     |\n",
      "|    ep_rew_mean      | 44.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4636     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57683    |\n",
      "|    total_timesteps  | 164912   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00785  |\n",
      "|    n_updates        | 41202    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.1     |\n",
      "|    ep_rew_mean      | 45.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4640     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57745    |\n",
      "|    total_timesteps  | 165102   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.163    |\n",
      "|    n_updates        | 41250    |\n",
      "----------------------------------\n",
      "Num timesteps: 165233\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 47.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.4     |\n",
      "|    ep_rew_mean      | 44.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4644     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57799    |\n",
      "|    total_timesteps  | 165235   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0709   |\n",
      "|    n_updates        | 41283    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44       |\n",
      "|    ep_rew_mean      | 44       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4648     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57866    |\n",
      "|    total_timesteps  | 165447   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 41336    |\n",
      "----------------------------------\n",
      "Num timesteps: 165593\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 47.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.5     |\n",
      "|    ep_rew_mean      | 44.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4652     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 57950    |\n",
      "|    total_timesteps  | 165690   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.424    |\n",
      "|    n_updates        | 41397    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.6     |\n",
      "|    ep_rew_mean      | 44.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4656     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58007    |\n",
      "|    total_timesteps  | 165849   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00624  |\n",
      "|    n_updates        | 41437    |\n",
      "----------------------------------\n",
      "Num timesteps: 165953\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 47.99\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.7     |\n",
      "|    ep_rew_mean      | 44.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4660     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58074    |\n",
      "|    total_timesteps  | 166052   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.69     |\n",
      "|    n_updates        | 41487    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 44.4     |\n",
      "|    ep_rew_mean      | 44.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4664     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58150    |\n",
      "|    total_timesteps  | 166273   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0159   |\n",
      "|    n_updates        | 41543    |\n",
      "----------------------------------\n",
      "Num timesteps: 166313\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45       |\n",
      "|    ep_rew_mean      | 45       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4668     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58230    |\n",
      "|    total_timesteps  | 166492   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 41597    |\n",
      "----------------------------------\n",
      "Num timesteps: 166673\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46       |\n",
      "|    ep_rew_mean      | 46       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4672     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58299    |\n",
      "|    total_timesteps  | 166694   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0287   |\n",
      "|    n_updates        | 41648    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.3     |\n",
      "|    ep_rew_mean      | 45.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4676     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58350    |\n",
      "|    total_timesteps  | 166823   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 41680    |\n",
      "----------------------------------\n",
      "Num timesteps: 167033\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.1     |\n",
      "|    ep_rew_mean      | 46.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4680     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58449    |\n",
      "|    total_timesteps  | 167123   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 41755    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.1     |\n",
      "|    ep_rew_mean      | 47.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4684     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58539    |\n",
      "|    total_timesteps  | 167373   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.454    |\n",
      "|    n_updates        | 41818    |\n",
      "----------------------------------\n",
      "Num timesteps: 167393\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 46.6     |\n",
      "|    ep_rew_mean      | 46.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4688     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58586    |\n",
      "|    total_timesteps  | 167508   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.428    |\n",
      "|    n_updates        | 41851    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4692     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58656    |\n",
      "|    total_timesteps  | 167721   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.411    |\n",
      "|    n_updates        | 41905    |\n",
      "----------------------------------\n",
      "Num timesteps: 167753\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.4     |\n",
      "|    ep_rew_mean      | 47.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4696     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58723    |\n",
      "|    total_timesteps  | 167905   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00673  |\n",
      "|    n_updates        | 41951    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.5     |\n",
      "|    ep_rew_mean      | 47.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4700     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58782    |\n",
      "|    total_timesteps  | 168091   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.473    |\n",
      "|    n_updates        | 41997    |\n",
      "----------------------------------\n",
      "Num timesteps: 168113\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.3     |\n",
      "|    ep_rew_mean      | 48.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4704     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58862    |\n",
      "|    total_timesteps  | 168345   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.41     |\n",
      "|    n_updates        | 42061    |\n",
      "----------------------------------\n",
      "Num timesteps: 168473\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.6     |\n",
      "|    ep_rew_mean      | 48.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4708     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 58935    |\n",
      "|    total_timesteps  | 168568   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0426   |\n",
      "|    n_updates        | 42116    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.1     |\n",
      "|    ep_rew_mean      | 48.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4712     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 59005    |\n",
      "|    total_timesteps  | 168774   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0412   |\n",
      "|    n_updates        | 42168    |\n",
      "----------------------------------\n",
      "Num timesteps: 168833\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.4     |\n",
      "|    ep_rew_mean      | 48.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4716     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 59073    |\n",
      "|    total_timesteps  | 168965   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.441    |\n",
      "|    n_updates        | 42216    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.9     |\n",
      "|    ep_rew_mean      | 48.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4720     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 59127    |\n",
      "|    total_timesteps  | 169140   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0134   |\n",
      "|    n_updates        | 42259    |\n",
      "----------------------------------\n",
      "Num timesteps: 169193\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 48.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.7     |\n",
      "|    ep_rew_mean      | 48.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4724     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 59188    |\n",
      "|    total_timesteps  | 169325   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0206   |\n",
      "|    n_updates        | 42306    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.8     |\n",
      "|    ep_rew_mean      | 48.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4728     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 59244    |\n",
      "|    total_timesteps  | 169489   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.405    |\n",
      "|    n_updates        | 42347    |\n",
      "----------------------------------\n",
      "Num timesteps: 169553\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 47.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.8     |\n",
      "|    ep_rew_mean      | 48.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4732     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 59305    |\n",
      "|    total_timesteps  | 169671   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 42392    |\n",
      "----------------------------------\n",
      "Num timesteps: 169913\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 47.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50       |\n",
      "|    ep_rew_mean      | 50       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4736     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 59391    |\n",
      "|    total_timesteps  | 169914   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.382    |\n",
      "|    n_updates        | 42453    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.1     |\n",
      "|    ep_rew_mean      | 50.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4740     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 59459    |\n",
      "|    total_timesteps  | 170113   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.704    |\n",
      "|    n_updates        | 42503    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.1     |\n",
      "|    ep_rew_mean      | 50.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4744     |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 59505    |\n",
      "|    total_timesteps  | 170245   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.499    |\n",
      "|    n_updates        | 42536    |\n",
      "----------------------------------\n",
      "Num timesteps: 170273\n",
      "Best mean reward: 54.68 - Last mean reward per episode: 47.67\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m360000\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#100k decent model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m driver\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:269\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    262\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:311\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    308\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 311\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:543\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    540\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[0;32m    542\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    546\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 54\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[2], line 36\u001b[0m, in \u001b[0;36mChromeDinoRL.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     33\u001b[0m     pydirectinput\u001b[38;5;241m.\u001b[39mpress(action_map[action])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# update state each timestep\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m game_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame_over\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     37\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserve_env()\n\u001b[0;32m     38\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# stay alive\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 92\u001b[0m, in \u001b[0;36mChromeDinoRL.game_over\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m opening \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mmorphologyEx(thresh, cv2\u001b[38;5;241m.\u001b[39mMORPH_OPEN, kernel, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# erosion + dilation \u001b[39;00m\n\u001b[0;32m     90\u001b[0m invert \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m-\u001b[39m opening\n\u001b[1;32m---> 92\u001b[0m OCR \u001b[38;5;241m=\u001b[39m \u001b[43mpytesseract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_over\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m words \u001b[38;5;241m=\u001b[39m OCR\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m check_words):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\pytesseract\\pytesseract.py:423\u001b[0m, in \u001b[0;36mimage_to_string\u001b[1;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    421\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[1;32m--> 423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBYTES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDICT\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRING\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\pytesseract\\pytesseract.py:426\u001b[0m, in \u001b[0;36mimage_to_string.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    421\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    424\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[0;32m    425\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs)},\n\u001b[1;32m--> 426\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    427\u001b[0m }[output_type]()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\pytesseract\\pytesseract.py:288\u001b[0m, in \u001b[0;36mrun_and_get_output\u001b[1;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save(image) \u001b[38;5;28;01mas\u001b[39;00m (temp_name, input_filename):\n\u001b[0;32m    278\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_filename\u001b[39m\u001b[38;5;124m'\u001b[39m: input_filename,\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m: temp_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[0;32m    286\u001b[0m     }\n\u001b[1;32m--> 288\u001b[0m     run_tesseract(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextsep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_file:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\pytesseract\\pytesseract.py:262\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractNotFoundError()\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timeout_manager(proc, timeout) \u001b[38;5;28;01mas\u001b[39;00m error_string:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode:\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractError(proc\u001b[38;5;241m.\u001b[39mreturncode, get_errors(error_string))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\site-packages\\pytesseract\\pytesseract.py:127\u001b[0m, in \u001b[0;36mtimeout_manager\u001b[1;34m(proc, seconds)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m seconds:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\subprocess.py:1154\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1151\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1154\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\subprocess.py:1528\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;66;03m# threads remain reading and the fds left open in case the user\u001b[39;00m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;66;03m# calls communicate again.\u001b[39;00m\n\u001b[0;32m   1527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1528\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remaining_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m   1530\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, orig_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrome-dino\\lib\\threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train DQN\n",
    "# higher timesteps = longer training, think TF epochs\n",
    "\n",
    "driver = ChromeDinoDriver()\n",
    "driver.run()\n",
    "time.sleep(1)\n",
    "\n",
    "timesteps = 360000\n",
    "model.learn(total_timesteps=timesteps, callback=callback) #100k decent model\n",
    "\n",
    "time.sleep(1)\n",
    "driver.end()\n",
    "\n",
    "plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"Chrome Dino\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19843e88-8610-4bc4-8345-0a8a75ba2416",
   "metadata": {},
   "source": [
    "## continual learning\n",
    "\n",
    "Load weights for a pre-trained model and continue training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6648e7-d682-4650-bbc6-f65aa901e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up where left off and continue training\n",
    "# reference: https://stable-baselines.readthedocs.io/en/master/guide/examples.html#continual-learning\n",
    "# reference: https://github.com/hill-a/stable-baselines/issues/599\n",
    "\n",
    "env = ChromeDinoRL()\n",
    "env = Monitor(env, LOG_DIR) # monitor progress using tensorboard logs\n",
    "\n",
    "# check environment - no output means no issues\n",
    "#env_checker.check_env(env)\n",
    "\n",
    "model_name = 'best_model_'\n",
    "log_name = ''\n",
    "model = DQN.load(os.path.join(CHECKPOINT_DIR, model_name), tensorboard_log=log_name)\n",
    "model.set_env(env)\n",
    "\n",
    "driver = ChromeDinoDriver()\n",
    "driver.run()\n",
    "time.sleep(1)\n",
    "\n",
    "timesteps = 36000\n",
    "model.learn(total_timesteps=timesteps, callback=callback)\n",
    "\n",
    "time.sleep(1)\n",
    "driver.end()\n",
    "\n",
    "plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"Chrome Dino\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb21966-3abf-4ee4-a180-e62171866725",
   "metadata": {},
   "source": [
    "## use model\n",
    "\n",
    "Load weights for a pre-trained model and use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369dbae-fdc8-4f0e-9b8b-ee4e0c9757fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "model_name = 'best_model_'\n",
    "model = DQN.load(os.path.join(MODELS_DIR, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15573a-e6ee-4366-86f1-d86ec7aef3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model in driver and see how it does\n",
    "env = ChromeDinoRL()\n",
    "driver = ChromeDinoDriver()\n",
    "driver.run()\n",
    "time.sleep(1)\n",
    "\n",
    "for ep in range(1):\n",
    "    obs = env.reset()\n",
    "    game_over = False \n",
    "    score = 0\n",
    "    \n",
    "    while not game_over:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, game_over, info = env.step(int(action))\n",
    "        score += reward\n",
    "    \n",
    "    print('Score for ep {} = {}.'.format(ep, score))\n",
    "    time.sleep(3)\n",
    "    \n",
    "driver.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
